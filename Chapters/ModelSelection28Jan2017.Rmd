#Model Selection and Inference

##Nonparametric Estimation Tools {#S:NonParTools}

###Moments

####Moment Estimators


-   $X_1, \ldots, X_n$ is a random sample (with replacement) from F(.)

-   Sometimes we say that $X_1, \ldots, X_n$ are identically and
    independently distributed ($iid$)

We will not assume a parametric form for the distribution function F()
and so proceed with a *nonparametric* analysis.

-   The $k$th (*raw*) moment is $\mathrm{E~} X^k = \mu^{\prime}_k$ .

-   It is estimated by the corresponding statistic
    $$\frac{1}{n} \sum_{i=1}^n X_i^k .$$

-   The $k$th (central) moment is $\mathrm{E~} (X-\mu)^k = \mu_k$.

-   It is estimated by
    $$\frac{1}{n} \sum_{i=1}^n \left(X_i-\bar{X}\right)^k .$$

####Empirical Distribution Function

-   Define the **empirical distribution function** to be
    $$\begin{aligned}
    F_n(x) &= \frac{\text{number of observations less than or equal to }x}{n} \\
    &= \frac{1}{n} \sum_{i=1}^n I\left(X_i \le x\right).\end{aligned}$$
    Here, the notation $I(\cdot)$ is the indicator function, it returns
    1 if the event $(\cdot)$ is true and 0 otherwise.

-   **Example -- Toy**. Consider $n=10$ observations as in Figure \@ref(fig:EDFToy)

\begin{equation*}
\begin{array}{l|cccccccccc}
    \hline
i   &1&2&3&4&5&6&7&8&9&10 \\
X_i & 10 &15 &15 &15 &20 &23 &23 &23 &23 &30\\
    \hline
    \end{array}\end{equation*}
        
        

-   $\bar{x} = 19.7$ and that the estimate of the second central moment,
    the **sample variance**, is 34.45556.

```{r EDFToy, echo=FALSE, fig.cap='Empirical Distribution Function of a Toy Example', out.width='80%', fig.asp=.75, fig.align='center'}
(xExample = c(10,rep(15,3),20,rep(23,4),30))
PercentilesxExample <- ecdf(xExample)
plot(PercentilesxExample, main="",xlab="x")
```

<h4 style="text-align: center;"><a id="displayText2.4f" href="javascript:togglecode('toggleToy','displayText2.4f');"><i><strong>R Code for Toy Example CDF</strong></i></a> </h4>
<div id="toggleToy" style="display: none">

``` 
(xExample = c(10,rep(15,3),20,rep(23,4),30))
PercentilesxExample <- ecdf(xExample)
plot(PercentilesxExample, main="",xlab="x")
```
</div>

###Quantiles


-   Special Cases

    -   The *median* is that point so that approximately half of a data
        set is below (or above) it.

    -   The first *quartile* is that number so that approximately 25% of
        the data is below it.

    -   A $100p$ *percentile* is that number so that $100 \times p$
        percent of the data is below it.
      

-   In general, for a given $0<q<1$, define the **$q$th quantile** $q_F$
    to be any number that satisfies $$\begin{aligned}
    \label{E:Quantile}
    F(q_F-) \le q \le F(q_F).\end{aligned}$$ Here, the notation $F(x-)$
    means to evaluate the function $F(\cdot)$ as a left-hand limit.

-   If $F(\cdot)$ is continuous at $q_F$, then $F(q_F-) = F(q_F)$

![Quantiles for a Continuous Distribution Function](FiguresCh4/ContinuousQuantileCase.jpg){width=".2\textwidth"}

####Quantiles

-   If F is smooth or there is a jump at $q$, the definition of the
    quantile $q_F$ is unique

-   if F is flat at $q$, then there a many definitions of $q_F$

![Three Quantile Cases](FiguresCh4/ThreeQuantileCases.jpg){width=".7\textwidth"}


####Quantiles

**Example -- Toy**. Consider $n=10$ observations:

-   The median might be defined to be any number between 20 and 23.

-   Many software packages use the average 21.5.

-   KPW defines the *smoothed empirical percentile* to be
    $$\hat{\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}$$ where $j=[(n+1)q]$
    and, $h=(n+1)q-j$, and $X_{(1)}, \ldots, X_{(n)}$ are the ordered
    values (the *order statistics*) corresponding to $X_1, \ldots, X_n$.

**Example**. Take $n=10$ and $q=0.5$. Then, $j=[(11)0.5]=[5.5]=5$ and,
$h=(11)(0.5)-5=0.5$. With this
$$\hat{\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.$$
Take $n=10$ and $q=0.2$. Then, $j=[(11)0.2]=[2.2]=2$ and,
$h=(11)(0.2)-2=0.2$. With this
$$\hat{\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.2 (15) + (0.8)(15) = 15.$$

###Density Estimators

-   When the random variable is discrete, estimate the probability mass
    function $f(x) = \Pr(X=x)$ is using
    $$f_n(x) = \frac{1}{n} \sum_{i=1}^n I(X_i = x).$$

-   Observations may be grouped in the sense that they fall into
    intervals of the form $[c_{j-1}, c_j)$, for $j=1, \ldots, k$. The
    constants $\{c_0 < c_1 < \cdots < c_k\}$ form some partition of the
    domain of F(.).

-   Then, use
    $$f_n(x) = \frac{n_j}{n \times (c_j - c_{j-1})}  \ \ \ \ \ \ c_{j-1} \le x < c_j,$$
    where $n_j$ is the number of observations ($X_i$) that fall into the
    interval $[c_{j-1}, c_j)$.

-   Another way to write this is
    $$f_n(x) = \frac{1}{n(c_j-c_{j-1})} \sum_{i=1}^n I(c_{j-1} < X_i \le c_j).$$

####Uniform Kernel Density Estimator

-   Let $b>0$, known as a bandwidth, $$\begin{aligned}
    \label{E:KDF}
     f_n(x) = \frac{1}{2nb} \sum_{i=1}^n I(x-b < X_i \le x + b).\end{aligned}$$

-   The estimator is the average over $n$ $iid$ realizations of a random
    variable with mean $$\begin{aligned}
    \mathrm{E~ } \frac{1}{2b} I(x-b < X \le x + b) &= \frac{1}{2b}\left(F(x+b)-F(x-b)\right) \\
    &= \frac{1}{2b} \left( \left\{ F(x) + b F^{\prime}(x) + b^2 C_1\right\} \right.\\
    & ~ ~ ~ -
    \left. \left\{ F(x) - b F^{\prime}(x) + b^2 C_2\right\} \right) \\
    &= F^{\prime}(x) + b \frac{C_1-C_2}{2} \rightarrow  F^{\prime}(x) = f(x),\end{aligned}$$
    as $b\rightarrow  0$. That is, $f_n(x)$ is an asymptotically
    unbiased estimator of $f(x)$.

####Kernel Density Estimator

-   More generally, define the **kernel density estimator**
    $$\begin{aligned}
    \label{E:KDF2}
     f_n(x) = \frac{1}{nb} \sum_{i=1}^n k\left(\frac{x-X_i}{b}\right).\end{aligned}$$
    where $k$ is a probability density function centered about 0.

-   Special Cases

    -   uniform kernel, $k(y) = \frac{1}{2}I(-1 < y \leq 1)$ .

    -   triangular kernel, $k(y) = (1-|y|)\times I(|y| \le 1)$

    -   Epanechnikov kernel,
        $k(y) = \frac{3}{4}(1-y^2) \times I(|y| \le 1)$, and

    -   Gaussian kernel $k(y) = \phi(y)$, where $\phi(\cdot)$ is the
        standard normal density function.

####Kernel Density Estimator of a Distribution Function

-   The kernel density estimator of a **distribution function** is
    $$\begin{aligned}
     \hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n K\left(\frac{x-X_i}{b}\right).\end{aligned}$$
    where $K$ is a probability distribution function associated with the
    kernel density $k$.

-   To illustrate, for the uniform kernel, we have
    $k(y) = \frac{1}{2}I(-1 < y \le 1)$ so $$\begin{aligned}
    K(y) =
    \begin{cases}
    0 &            y<-1\\
    \frac{y+1}{2}& -1 \le y < 1 \\
    1 & y \ge 1 \\
    \end{cases}\end{aligned}$$

##Nonparametric Estimation Tools For Model Selection

###Graphical Comparisions

####Comparing Distribution and Density Functions

-   The left-hand panel compares distribution functions, with the dots
    corresponding to the empirical distribution, the thick blue curve
    corresponding to the fitted gamma and the light purple curve
    corresponding to the fitted Pareto.

-   The right hand panel compares these three distributions summarized
    using probability density functions.

![Nonparametric Versus Fitted Parametric Distribution and Density Functions](FiguresCh4/ComparisonCDFPDF.jpg){width=".6\textwidth"}

####*PP* Plot

-   The horizontal axes gives the empirical distribution function at
    each observation.

-   In the left-hand panel, the corresponding distribution function for
    the gamma is shown in the vertical axis.

-   The right-hand panel shows the fitted Pareto distribution. Lines of
    $y=x$ are superimposed.
    
![Probability-Probability (pp) Plots.](FiguresCh4/PPPlot.jpg)

**KPW** also recommends plotting the difference $D(x) = F_n(x) - F^*(x)$
versus $x$. Here, $F^*(x)$ is the fitted model distribution function.

####*QQ* Plot

-   The horizontal axes gives the empirical quantiles at
    each observation.

-   The right-hand panels they are graphed on a logarithmic basis.

-   The vertical axis gives the quantiles from the fitted distributions;
    Gamma quantiles are in the upper panels, Pareto quantiles are in the
    lower panels.

-   The lower-right hand panel suggests that the Pareto distribution
    does a good job with large observations but provides a poorer fit
    for small observations.

![Quantile-Quantile ($qq$) Plots](FiguresCh4/QQplot.jpg)

###Statistical Comparisions

####Three Goodness of Fit Statistics

$$
\begin{matrix}
\begin{array}{ccc}
\text{Statistic} & \text{Definition} & \text{Computational Expression} \\ \hline
Kolmogorov & sup_x |F_n(x) - F(x) |  & max(D^+ - D^-) \\
 -Smirnov &&D^+ = \max_{i=1, \ldots, n} \left(\frac{i}{n} - F_i\right) \\
& &D^- = \max_{i=1, \ldots, n} \left(F_i - \frac{i-1}{n} \right) \\ \hline
Cramer&  n \int (F_n(x) - F(x))^2 dx &
\frac{1}{12n} + \sum_{i=1}^n \left(F_i - (2i-1)/n\right)^2 \\ 
 -von Mises & & \\ \hline
Anderson&  n \int \frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} dx &\\
  -Darling & &
-\frac{1}{n} \sum_{i=1}^n (2i-1) \log\left(F_i(1-F_{n+1-i})\right)^2 \\ \hline
\end{array}
\end{matrix}
$$



##Nonparametric Estimation using Modified Data

###Grouped Data

####Grouped Data

-   Observations may be grouped in the sense that they fall into
    intervals of the form $[c_{j-1}, c_j)$, for $j=1, \ldots, k$.

-   The constants $\{c_0 < c_1 < \cdots < c_k\}$ form some partition of
    the domain of F(.).

-   Define the empirical distribution function at the boundaries is
    defined in the usual way:
    $$F_n(c_j) = \frac{\text{number of observations } \le c_j}{n}$$

-   For other values of $x$, one could use the

    **Ogive:** connect values of the boundaries with a straight line.

-   For another way of smoothing, recall the kernel density estimator of
    the distribution function $$\begin{aligned}
     \hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n K\left(\frac{x-X_i}{b}\right).\end{aligned}$$

-   For densities, use
    $$f_n(x) = \frac{n_j}{n \times (c_j - c_{j-1})}  \ \ \ \ \ \ c_{j-1} \le x < c_j,$$

###Censored Data

####Censored Data

-   Censoring occurs when we observe only a limited value of
    an observation.

-   Suppose that $X$ represents a loss due to an insured event and that
    $u$ is a known censoring point.

-   If observations are censored from the **right** (or from above),
    then we observe $$Y = \min(X,u).$$

    -   In this case, $u$ may represent the upper limit of coverage for
        an insurer. The loss exceeds the amount $u$ but the insurer does
        not have in its records the amount of the actual loss.

-   If observations are censored from the **left** (or from below), then
    we observe $$Y = \max(X,u).$$

    -   Let $u$ represents the upper limit of coverage but now $Y - u$
        represents the amount that a *reinsurer* is responsible for. If
        the loss $X < u$, then $Y=0$, no loss for the reinsurer. If the
        loss $X  \ge u$, then $Y= X-u$ represents the reinsurer's
        retained claims.

####Kaplan-Meier Product Limit Estimator

-   Let $t_{1} <\cdots< t_{c}$ be distinct points at which an event of
    interest occurs, or non-censored losses, and let $s_j$ be the number
    of events at time point $t_{j}$ .

-   Further, the corresponding risk set is the number of observations
    that are active at an instant just prior to $t_{j}$ . Using
    notation, the risk set is $R_{j}=\sum_{i=1}^{n}I(x_{i}\geq t_{j})$.

-   With this notation, the **product-limit estimator** of the
    distribution function is $$\hat{F}(x)=
    \left\lbrace
    \begin{array}{llll}
    0 &
    x < t_{1} \\
    1-\prod_{j:t_{j} \leq x}\left( 1-\frac{s_j}{R_{j}}\right)  &
    x \geq t_{1} .\\
    \end{array}
    \right .$$

-   Greenwood (1926) derived the formula for the estimated variance
    $$\widehat{Var}(\hat{F}(x)) =
    (1-\hat{F}(x))^{2}
    \sum _{j:t_{j} \leq x} \dfrac{s_j}{R_{j}(R_{j}-s_j)}.$$

###Truncated Data

####Truncated Data

-   An outcome is potentially **truncated** when the availability of an
    observation depends on the outcome.

-   In insurance, it is common for observations to be truncated from the
    **left** (or below) at $d$ when the amount observed is
    $$Y = \begin{cases}
    \text{we do not observe X}  &  X < d\\
    X-d                         &   X \ge d.
    \end{cases}$$

    -   In this case, $d$ may represent the deductible associated with
        an insurance coverage. If the insured loss is less than the
        deductible, then the insurer does not observe the loss. If the
        loss exceeds the deductible, then the excess $X-d$ is the claim
        that the insurer covers.

-   Observations may also truncated from the **right** (or above) at $d$
    when the amount observed is $$Y = \begin{cases}
    X   &   X < d  \\
    \text{we do not observe X}  &  X \ge d\\
    \end{cases}$$

    -   Classic examples of truncation from the right include $X$ as a
        measure of distance of a star. When the distance exceeds a
        certain level $d$, the star is no longer observable.

####Right-Censored, Left-Truncated Empirical Distribution Functions

-   Procedure from **KPW**. Notation:

    -   For each observation $i$, let $d_i$ be the lower truncation
        limit (0 if no truncation)

    -   Let $u_i$ be the upper censoring limit (=$\infty$ if
        no censoring)

    -   The recorded value is $x_i$ in the case of no censoring, $u_i$
        if there is censoring.

    -   For notation, let $t_1 < \cdots < t_k$ be $k$ unique
        observations of $x_i$ that are uncensored.

    -   Define $s_j$ to be the number of $x_i$'s at $t_j$.

    -   Define the risk set
        $$R_j = \sum_{i=1}^n I(x_i \geq t_{j}) + \sum_{i=1}^n I(u_i \geq t_{j}) - \sum_{i=1}^n I(d_i \geq t_{j})$$

-   The product-limit estimator of the distribution function is
    $$\hat{F}(x)=
    \left\lbrace
    \begin{array}{llll}
    0 &
    x < t_{1} \\
    1- \prod_{j:t_{j} \leq x}\left( 1-\frac{s_j}{R_{j}}\right)  &
    x \geq t_{1}\\
    \end{array}
    \right .$$

-   The Nelson-Aalen estimator of the distribution function is
    $$\hat{F}(x)=
    \left\lbrace
    \begin{array}{llll}
    0 &
    x < t_{1} \\
    1- \exp \left(-\sum_{j:t_{j} \leq x}\frac{s_j}{R_j} \right) &
    x \geq t_{1}\\
    \end{array}
    \right.$$

##Topics in Parametric Estimation

###Starting Values


-   Maximum likelihood is a desirable estimation technique because

    -   It employs data efficiently (enjoys certain
        optimality properties)

    -   It can be used in a variety of data sampling schemes
        (e.g.,*iid*, grouped, censored, regression, and so forth)

-   However, maximum likelihood is a recursive estimation procedure that
    requires starting values to begin the recursion

-   Two alternative estimation techniques are:

    -   Method of moments

    -   Percentile matching

-   These are non-recursive techniques. Easy to implement and explain.
    Although less efficient than maximum likelihood, they can be
    employed to provide starting values for maximum likelihood.

####Method of Moments

-   Idea: Approximate the moments using a parametric distribution to the
    empirical (nonparametric) moments

-   **Example - Property Fund.** For the 2010 property fund, there are
    $n=1,377$ individual claims (in thousands of dollars) with
    $$\begin{aligned}
    m_1 = \frac{1}{n} \sum_{i=1}^n X_i = 26.62259 \ \ \ \
    \text{and} \ \ \ \
     m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = 136154.6 .\end{aligned}$$

-   Gamma Distribution

    -   From theory, $\mu_1 = \alpha \theta$ and
        $\mu_2^{\prime} = \alpha(\alpha+1) \theta^2$.

    -   Equating the two yields the method of moments estimators, easy
        algebra shows that $$\begin{aligned}
        \alpha = \frac{\mu_1^2}{\mu_2^{\prime}-2\mu_1^2}  \ \ \ \text{and} \ \ \  \theta = \frac{\mu_2^{\prime}-\mu_1^2}{\mu_1}.\end{aligned}$$

    -   The method of moment estimators are $$\begin{aligned}
        \hat{\alpha} &= \frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809\\
        %\text{and} \\
        \hat{\theta} &= \frac{136154.6-26.62259^2}{26.62259} = 5,087.629.\end{aligned}$$

    -   In contrast, the maximum likelihood values turn out to be
        $\hat{\alpha}_{MLE} =  0.2905959$ and
        $\hat{\theta}_{MLE} = 91.61378$

    -   Big discrepancies between the two estimation procedures,
        suggesting that the gamma model fits poorly.


-   **Example - Property Fund.** Recall the nonparametric estimates
    $$\begin{aligned}
    m_1 = \frac{1}{n} \sum_{i=1}^n X_i = 26.62259 \ \ \ \
    \text{and} \ \ \ \
     m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = 136154.6 .\end{aligned}$$

-   Pareto Distribution

    -   From theory, $\mu_1 = \theta/(\alpha -1)$ and
        $\mu_2^{\prime} = 2\theta^2/((\alpha-1)(\alpha-2) )$.

    -   Easy algebra shows $$\begin{aligned}
        \alpha = 1+ \frac{\mu_2^{\prime}}{\mu_2^{\prime}-2\mu_1^2} \ \ \ \
        \text{and} \ \ \ \ \
         \theta = (\alpha-1)\mu_1.\end{aligned}$$

    -   The method of moment estimators are $$\begin{aligned}
        \hat{\alpha} &= 1+ \frac{136154.6}{136154.6-2*26,62259^2} = 2.01052\\
        %\text{and} \\
        \hat{\theta} &= (2.01052-1) \cdot 26.62259 = 26.9027\end{aligned}$$

    -   The maximum likelihood values turn out to be
        $\hat{\alpha}_{MLE} =  0.9990936$ and
        $\hat{\theta}_{MLE} = 2.2821147$.

    -   Interesting that $\hat{\alpha}_{MLE}<1$; for the Pareto
        distribution; recall that $\alpha <1$ means that the mean
        is infinite.

    -   Indicates that the property claims data set is a long
        tail distribution.

####Percentile Matching

-   Under percentile matching, one approximates the parametric
    distribution using the empirical (nonparametric) quantiles,
    or percentiles.

-   **Example - Property Fund.** The 25th percentile (the first quartile) turns out to be 0.78853 and the 95th percentile is 50.98293 (both in thousands of dollars).

-   Pareto Distribution

    -   The Pareto distribution is particularly intuitively pleasing
        because of the closed-form solution for the quantiles.

    -   The distribution function is
        $F(x) = 1 - \left(\theta/(x+\theta )\right)^{\alpha}$.

    -   Easy algebra shows that we can express the quantile as
        $$F^{-1}(q) = \theta \left( (1-q)^{-1/\alpha} -1 \right)$$ for a
        fraction $q$, $0<q<1$.

    -   With two equations
        $$0.78853 = \theta \left( (1-.25)^{-1/\alpha} -1 \right) \ \ \ \ \text{and} \ \ \ \ 50.98293 = \theta \left( (1-.95)^{-1/\alpha} -1\right)$$
        and two unknowns, the solution is $$
        \hat{\alpha} = 0.9412076 \ \ \ \ \ \text{and} \ \ \ \
        \hat{\theta} = 2.205617 .$$

    -   A numerical routine was required for these solutions - no
        analytic solution available.

    -   Recall that the maximum likelihood values are
        $\hat{\alpha}_{MLE} =  0.9990936$ and
        $\hat{\theta}_{MLE} = 2.2821147$.

    -   The percentile matching provides a better approximation for the
        Pareto distribution than did the method of moments.

###Grouped Data

####Parametric Estimation Using Grouped Data

-   Observations may be grouped in the sense that they fall into
    intervals of the form $(c_{j-1}, c_j]$, for $j=1, \ldots, k$.

-   The constants $\{c_0 < c_1 < \cdots < c_k\}$ form some partition of
    the domain of F(.).

-   Define $n_j$ to be the number of observations that fall in the $j$th
    interval, $(c_{j-1}, c_j]$.

-   The probability of an observation $X$ falling in the $j$th interval
    is $$\Pr\left(X \in c_{j-1}, c_j]\right) = F(c_j) - F(c_{j-1}).$$


-   The probability of an observation $X$ falling in the $j$th interval
    is $$\Pr\left(X \in c_{j-1}, c_j]\right) = F(c_j) - F(c_{j-1}).$$

-   The corresponding mass function is $$\begin{aligned}
    f(x) &=
    \begin{cases}
    F(c_1) - F(c_{0}) &   \textrm{if~} x \in (c_{0}, c_1]\\
    \vdots & \vdots \\
    F(c_k) - F(c_{k-1}) &   \textrm{if~} x \in (c_{k-1}, c_k]\\
    \end{cases} \\
    &= \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{I(x \in (c_{j-1}, c_j])}\end{aligned}$$

-   The likelihood is $$\begin{aligned}
    \prod_{j=1}^n f(x_i) = \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{n_j}\end{aligned}$$

-   The log-likelihood is $$\begin{aligned}
    L(\theta) = \ln \prod_{j=1}^n f(x_i) = \sum_{j=1}^k n_j \ln \left\{F(c_j) - F(c_{j-1})\right\}\end{aligned}$$

###Parametric Estimation Using Censored Data

####Censored Data Likelihood

-   Censoring occurs when we observe only a limited value of
    an observation.

-   Suppose that $X$ represents a loss due to an insured event and that
    $u$ is a known censoring point.

-   If observations are censored from the **right** (or from above),
    then we observe we observe $Y= \min(X, u)$ and
    $\delta_u= \mathrm{I}(X \geq u)$.

-   If censoring occurs so that $\delta_u=1$, then $X \geq u$ and the
    likelihood is $\Pr(X \ge u) = 1-\mathrm{F}(u)$.

-   If censoring does not occur so that $\delta_u=0$, then $X < C_U$ and
    the likelihood is $\mathrm{f}(y)$.

-   Summarizing, we have $$\begin{aligned}
    Likelihood  &= \left\{
    \begin{array}{cl}
    \mathrm{f}(y) & \textrm{if~}\delta=0 \\
    1-\mathrm{F}(u)  &  \textrm{if~}\delta=1
    \end{array}\right. \\
    &= \left( \mathrm{f}(y)\right)^{1-\delta} \left(1-\mathrm{F}(u)\right)^{\delta} .\end{aligned}$$
    The second right-hand expression allows us to present the likelihood
    more compactly.


-   For a single observation, we have $$\begin{aligned}
    Likelihood  &= \left\{
    \begin{array}{cl}
    \mathrm{f}(y) & \textrm{if~}\delta=0 \\
    1-\mathrm{F}(u)  &  \textrm{if~}\delta=1
    \end{array}\right. \\
    &= \left( \mathrm{f}(y)\right)^{1-\delta} \left(1-\mathrm{F}(u)\right)^{\delta} .\end{aligned}$$
    

-   Consider a random sample of size $n$,
    $$\{ (y_1,\delta_1), \ldots,(y_n, \delta_n) \} $$ with potential
    censoring times $\{ u_1,  \ldots, u_n \} $.

-   The likelihood is
    $$\prod_{i=1}^n \left( \mathrm{f}(y_i)\right)^{1-\delta_i} \left(1-\mathrm{F}(u_i)\right)^{\delta_i}
    = \prod_{\delta_i=0}\mathrm{f}(y_i) \prod_{\delta_i=1} \{1-\mathrm{F}(u_i)\},$$

-   Here, the notation $\prod_{\delta_i=0}$ means take the product
    over uncensored observations, and similarly for
    $\prod_{\delta_i=1}$.

-   The log-likelihood is
    $$L(\theta) = \sum_{i=1}^n \left\{(1-\delta_i) \ln  \mathrm{f}(y_i) +  \delta_i \ln \left(1-\mathrm{F}(u_i)\right) \right\}$$

###Censored and Truncated Data


-   Let $X$ denote the outcome and let $C_L$ and $C_U$ be two constants.

  Type                       Limited Variable              Censoring Information
  -------------------- ---------------------------- ------------------------------------
  right censoring       $X_U^{\ast}= \min(X, C_U)$   $\delta_U= \mathrm{I}(X \geq C_U)$
  left censoring        $X_L^{\ast}= \max(y, C_L)$   $\delta_L= \mathrm{I}(X \leq C_L)$
  interval censoring                                
  right truncation                 $X$                   observe $X$ if $X < C_U$
  left truncation                  $X$                   observe $X$ if $X < C_L$

```{r CensoringTruncation, fig.cap='Censoring and Truncation', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
#  FIGURE 14.1A
plot.new()
par(mar=c(0,0,0,0), cex=1.5)
plot.window(xlim=c(0,14),ylim=c(0,4))

arrows(1,1,13.5,1,code=2,lwd=2,angle=25,length=0.10)

text(0.8,1.8,"No exact value under",adj=0, cex=0.8)
text(1.9,1.5,"left-censoring",adj=0, cex=0.8)
text(8.2,1.8,"No exact value under",adj=0, cex=0.8)
text(9.3,1.5,"right-censoring",adj=0, cex=0.8)

text(4.8,2.5,"No exact value under",adj=0, cex=0.8)
text(5.2,2.2,"interval-censoring",adj=0, cex=0.8)

text(0.8,3.5,"No observed value under",adj=0, cex=0.8)
text(1.9,3.2,"left-truncation",adj=0, cex=0.8)
text(8.2,3.5,"No observed value under",adj=0, cex=0.8)
text(9.3,3.2,"right-truncation",adj=0, cex=0.8)

text(1,1,labels="|",adj=0)
text(4.5,1,labels="|",adj=0)
text(8,1,labels="|",adj=0, cex=1)


text(1,  0.6,expression(0),adj=0)
text(4.5,0.6,expression(C[L]),adj=0)
text(8,  0.6,expression(C[U]),adj=0)
text(6,  0.3,expression(X),adj=0, cex=1.1)
```

####Example: Mortality Study

-   Suppose that you are conducting a two-year study of mortality of
    high-risk subjects, beginning January 1, 2010 and finishing January
    1, 2012.

-   For each subject, the beginning of the arrow represents that the the
    subject was recruited and the arrow end represents the event time.
    Thus, the arrow represents exposure time.

```{r MortalityTimeLine, fig.cap='Subjects on Test in a Mortality Study', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
plot.new()
par(mar=c(0,0,0,0), cex=1.5)
plot.window(xlim=c(0,14),ylim=c(0,10))

arrows(1.2,1.2,13.5,1.2,code=2,lwd=2,angle=25,length=0.10)
text(6,0.5,"Calendar Time",adj=0)
arrows(1.5,8,11,8,code=2,lwd=2,angle=25,length=0.10)
text(11.5,8,"A",adj=0, cex=0.8)
arrows(1.8,7,6,7,code=2,lwd=2,angle=25,length=0.10)
text(7,7,"B",adj=0, cex=0.8)
arrows(4,6,11.2,6,code=2,lwd=2,angle=25,length=0.10)
text(11.5,6,"C",adj=0, cex=0.8)
arrows(3.5,5,6.5,5,code=2,lwd=2,angle=25,length=0.10)
text(7,5,"D",adj=0, cex=0.8)
arrows(1,4,2.8,4,code=2,lwd=2,angle=25,length=0.10)
text(3.5,4,"E",adj=0, cex=0.8)
arrows(9,3,11.4,3,code=2,lwd=2,angle=25,length=0.10)
text(11.5,3,"F",adj=0, cex=0.8)

arrows(2.9,9,3,1,code=2,lwd=2,angle=0,length=0.10)
text(2.5,9.3,"1/1/2010",adj=0, cex=0.7)
arrows(5.4,9,5.5,1,code=2,lwd=2,angle=0,length=0.10)
text(4.9,9.3,"1/1/2011",adj=0, cex=0.7)
arrows(7.9,9,8,1,code=2,lwd=2,angle=0,length=0.10)
text(7.4,9.3,"1/1/2012",adj=0, cex=0.7)
```


-   Type A - **right-censored**. This subject is alive at the beginning
    and the end of the study. Because the time of death is not known by
    the end of the study, it is right-censored. Most subjects are
    Type A.

-   Type B. **Complete information** is available for a type B subject.
    The subject is alive at the beginning of the study and the death
    occurs within the observation period.

-   Type C - **right-censored** and **left-truncated**. A type C subject
    is right-censored, in that death occurs after the
    observation period. However, the subject entered after the start of
    the study and is said to have a *delayed entry time*. Because the
    subject would not have been observed had death occurred before
    entry, it is left-truncated.


-   Type D - **left-truncated**. A type D subject also has
    delayed entry. Because death occurs within the observation period,
    this subject is not right censored.

-   Type E - **left-truncated**. A type E subject is not included in the
    study because death occurs prior to the observation period.

-   Type F - **right-truncated**. Similarly, a type F subject is not
    included because the entry time occurs after the observation period.


###Parametric Estimation Using Censored and Truncated Data


-   Truncated data are handled in likelihood inference via
    conditional probabilities.

-   Adjust the likelihood contribution by dividing by the probability
    that the variable was observed.

-   Summarizing, we have the following contributions to the likelihood
    for six types of outcomes.

$$\begin{array}{lc}
\hline Outcome            & Likelihood~Contribution \\\hline
\text{exact value      }  & f(x) \\
\text{right-censoring  }  & 1-F(C_U) \\
\text{left-censoring   }  & F(C_L) \\
\text{right-truncation }  & f(x)/F(C_U) \\
\text{left-truncation    }& f(x)/(1-F(C_L)) \\
\text{interval-censoring} & F(C_U)-F(C_L) \\
\hline
\end{array}$$


-   For known outcomes and censored data, the likelihood is
    $$\prod_{E} \mathrm{f}(x_i) \prod_{R} \{1-\mathrm{F}(C_{Ui})\} \prod_{L}
    \mathrm{F}(C_{Li}) \prod_{I} (\mathrm{F}(C_{Ui})-\mathrm{F}(C_{Li})),$$
    where $\prod_{E}$ is the product over observations with *E*xact
    values, and similarly for *R*ight-, *L*eft- and
    *I*nterval-censoring.

-   For right-censored and left-truncated data, the likelihood is
    $$\prod_{E} \frac{\mathrm{f}(x_i)}{1-\mathrm{F}(C_{Li})} \prod_{R} \frac{1-\mathrm{F}(C_{Ui})}{1-\mathrm{F}(C_{Li})} ,$$

-   Similarly for other combinations.

####Special Case: Exponential Distribution

-   Consider data that are right-censored and left-truncated, with
    random variables $X_i$ that are exponentially distributed with mean
    $\theta$.

-   With these specifications, recall that
    $\mathrm{f}(x) = \theta^{-1} \exp(-x/\theta)$ and
    $\mathrm{F}(x) = 1-\exp(-x/\theta)$.

-   For this special case, the logarithmic likelihood is
    $$\begin{aligned}
     \ln Likelihood  &= \sum_{E} \left( \ln \mathrm{f}(x_i) - \ln (1-\mathrm{F}(C_{Li})) \right) -\sum_{R}\left( \ln (1-\mathrm{F}(C_{Ui}))- \ln (1-\mathrm{F}(C_{Li}))
     \right) \\
     &=  \sum_{E} (-\ln \theta -(x_i-C_{Li})/\theta ) -\sum_{R} (C_{Ui}-C_{Li})/\theta .\end{aligned}$$

-   To simplify the notation, define
    $\delta_i = \mathrm{I}(X_i \geq C_{Ui})$ to be a binary variable
    that indicates right-censoring.

-   Let $X_i^{\ast \ast} = \min(X_i, C_{Ui}) - C_{Li}$ be the amount
    that the observed variable exceeds the lower truncation limit.

-   With this, the logarithmic likelihood is
    $$\ln Likelihood =  - \sum_{i=1}^n \left((1-\delta_i) \ln \theta + \frac{x_i^{\ast \ast}}{\theta} \right).$$

-   Taking derivatives with respect to the parameter $\theta$ and
    setting it equal to zero yields the maximum likelihood estimator
    $$\begin{aligned}
    \widehat{\theta}  &= \frac{1}{n_u} \sum_{i=1}^n  x_i^{\ast \ast},\end{aligned}$$
    where $n_u = \sum_i (1-\delta_i)$ is the number of
    uncensored observations.

##Bayesian Inference

###Bayesian Model


-   In the **frequentist interpretation**, one treats the vector of
    parameters $\boldsymbol \theta$ as fixed yet unknown, whereas the
    outcomes $X$ are realizations of random variables.

-   With Bayesian statistical models, one views both the model
    parameters and the data as random variables.

    -   Use probability tools to reflect this uncertainty about the
        parameters $\boldsymbol \theta$.

-   For notation, we will think about $\boldsymbol \theta$ as a random
    vector and let $\pi(\boldsymbol \theta)$ denote the distribution of
    possible outcomes.

####Bayesian Inference Strengths

There are several advantages of the Bayesian approach.

1.  One can describe the entire distribution of parameters conditional
    on the data. This allows one, for example, to provide probability
    statements regarding the likelihood of parameters.

2.  This approach allows analysts to blend information known from other
    sources with the data in a coherent manner. This topic is developed
    in detail in the credibility chapter.

3.  The Bayesian approach provides for a unified approach for
    estimating parameters. Some non-Bayesian methods, such as least
    squares, required a approach to estimating variance components. In
    contrast, in Bayesian methods, all parameters can be treated in a
    similar fashion. Convenient for explaining results to consumers of
    the data analysis.

4.  Bayesian analysis is particularly useful for forecasting
    future responses.

####Bayesian Model

-   **Prior Distribution.** $\pi(\boldsymbol \theta)$ is called the
    *prior distribution*.

    -   Typically, it is a regular distribution and so integrates
        to one.

    -   We may be very uncertain (or have no clue) about the
        distribution of $\boldsymbol \theta$; the Bayesian machinery
        allows this situation $$\int \pi(\theta) d\theta = \infty$$ in
        which case $\pi(\cdot)$ is called an **improper prior**.

-   **Model Distribution.** The distribution of outcomes given an
    assumed value of $\boldsymbol \theta$ is known as the *model
    distribution* and denoted as
    $f(x | \boldsymbol \theta) = f_{X|\boldsymbol \theta} (x|\boldsymbol \theta )$.
    This is the (usual frequentist) mass or density function.


-   **Joint Distribution.** The distribution of outcomes and model
    parameters is, not surprisingly, known as the *joint distribution*
    and denoted as
    $f(x , \boldsymbol \theta) = f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)$.

-   **Marginal Outcome Distribution.** The distribution of outcomes can
    be expressed as
    $$f(x) =\int f(x | \boldsymbol \theta)\pi(\boldsymbol \theta) d\boldsymbol \theta.$$
    This is analogous to a frequentist mixture distribution.


-   **Posterior Distribution of Parameters.** After outcomes have been
    observed (hence the terminology posterior), one can use Bayes
    theorem to write the distribution as
    $$\pi(\boldsymbol \theta | x) =\frac{f(x , \boldsymbol \theta)}{f(x)} =\frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)}$$
    The idea is to update your knowledge of the distribution of
    $\boldsymbol \theta$ ($\pi(\boldsymbol \theta)$) with the data $x$.

    -   We can summarize the distribution using a confidence interval
        type statement.

    -   **Definition**. $[a,b]$ is said to be a $100(1-\alpha)\%$
        **credibility interval** for $\boldsymbol \theta$ if
        $$\Pr (a \le \theta \le b | \mathbf{x}) \ge 1- \alpha.$$

####Two Examples

**Exam C Question 157.** You are given:\
(i) In a portfolio of risks, each policyholder can have at most one
claim per year.\
(ii) The probability of a claim for a policyholder during a year is
$q$.\
(iii) The prior density is $$\pi(q) = q^3/0.07, \ \ \ 0.6 < q < 0.8$$ A
randomly selected policyholder has one claim in Year 1 and zero claims
in Year 2.\
For this policyholder, calculate the posterior probability that
$0.7 < q < 0.8$.

**Exam C Question 43.** You are given:\
(i) The prior distribution of the parameter $\Theta$ has probability
density function:
$$\pi(\theta) = 1/\theta^2, \ \ \ \ 1 < \theta < \infty$$ (ii) Given
$\Theta = \theta$, claim sizes follow a Pareto distribution with
parameters $\alpha=2$ and $\theta$.\
A claim of 3 is observed.\
Calculate the posterior probability that $\Theta$ exceeds 2.

###Bayesian Inference - Decision Analysis

-   In classical decision analysis, the loss function
    $l(\hat{\theta}, \theta)$ determines the penalty paid for using the
    estimate $\hat{\theta}$ instead of the true $\theta$.

-   The **Bayes estimate** is that value that minimizes the expected
    loss $\mathrm{E~}l(\hat{\theta}, \theta)$.


-   Some important special cases include: 
$$\begin{array}{ccc}  \hline
    \text{ Loss function } l(\hat{\theta}, \theta) & \text{Descriptor} & \text{Bayes Estimate}\\ \hline
    (\hat{\theta}- \theta)^2 & \text{squared error loss} & \mathrm{E}(\theta|X)  \\
    |\hat{\theta}- \theta| & \text{absolute deviation loss} & median of \pi(\theta|x)\\
    I(\hat{\theta} =\theta) & \text{zero-one loss (for discrete probabilities)}& mode of \pi(\theta|x) \\  \hline
    \end{array}$$

-   For new data $y$, the predictive distribution is
    $$f(y|x) = \int f(y|\theta) \pi(\theta|x) d\theta .$$

-   With this, the Bayesian prediction of $y$ is $$\begin{aligned}
    \mathrm{E}(y|x) &= \int y f(y|x) dy = \int y \left(\int f(y|\theta) \pi(\theta|x) d\theta \right) dy \\
    &= \int  \mathrm{E}(y|\theta) \pi(\theta|x) d\theta .\end{aligned}$$

####Posterior Distribution

How to calculate the posterior distribution?

-   **By hand** - can do this in special cases

-   **Simulation** - uses modern computational techniques. **KPW**
    (Section 12.4.4) mentions Markov Chain Monte Carlo (MCMC) simulation

-   **Normal Approximation**. Theorem 12.39 of **KPW** provides a
    justification

-   **Conjugate distributions**. Classical approach. Although this
    approach is available only for a limited number of distributions, it
    has the appeal that it provides closed-form expressions for the
    distributions, allowing for easy interpretations of results. We
    focus on this approach.

To relate the prior and posterior distributions of the parameters, we
have 
$$\begin{aligned}
\pi(\boldsymbol \theta | x)&=\frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)} \\
& \propto  f(x|\boldsymbol \theta ) \pi(\boldsymbol \theta) \\
\text{Posterior} &\text{is proportional to} \text{likelihood} \times \text{prior}
\end{aligned}$$


For **conjugate distributions**, the posterior and the prior come from
the same family of distributions.

####Special Case: Poisson Gamma Conjugate Family

-   Assume a Poisson($\lambda$) model distribution so that
    $$f(\mathbf{x} | \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}$$

-   Assume $\lambda$ follows a gamma($\alpha, \theta$) prior
    distribution so that
    $$\pi(\lambda) = \frac{\left(\lambda/\theta\right)^{\alpha} \exp(-\lambda/\theta)}{\lambda \Gamma(\alpha)}.$$

-   The posterior distribution is proportional to $$\begin{aligned}
    \pi(\lambda | \mathbf{x}) & \propto f(\mathbf{x}|\theta ) \pi(\lambda) \\
    &= C \lambda^{\sum_i x_i + \alpha -1} \exp(-\lambda (n+1/\theta))\end{aligned}$$
    where $C$ is a constant.

-   We recognize this to be a gamma distribution with new parameters
    $\alpha_{new} = \sum_i x_i + \alpha$ and
    $\theta_{new} = 1/(n + 1/\theta)$.
    
##Exercises

Here are a set of exercises that guide the viewer through some of the theoretical foundations of **Loss Data Analytics**. Each tutorial is based on one or more questions from the professional actuarial examinations â€“ typically the Society of Actuaries Exam C.

```{r }
knitr::include_url("http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-model-selection/",height = "600px")
```
