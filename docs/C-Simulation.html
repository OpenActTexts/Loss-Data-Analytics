<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Simulation and Resampling | Loss Data Analytics</title>
  <meta name="description" content="Chapter 6 Simulation and Resampling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Simulation and Resampling | Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 6 Simulation and Resampling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/openacttexts/Loss-Data-Analytics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Simulation and Resampling | Loss Data Analytics" />
  
  <meta name="twitter:description" content="Chapter 6 Simulation and Resampling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C-AggLossModels.html">
<link rel="next" href="C-PremiumFoundations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<!-- Mathjax -->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<!-- The following code is for the quizzes -->
<script src="https://surveyjs.azureedge.net/1.0.50/survey.jquery.js"></script>
<link href="https://surveyjs.azureedge.net/1.0.50/survey.css" type="text/css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.6.4/showdown.min.js"></script>  

<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the mardown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
         MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};
// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}
// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};



</script>  
<!-- This completes the code for the quizzes -->


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="C-Intro.html"><a href="C-Intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="C-Intro.html"><a href="C-Intro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics to Insurance Activities</a><ul>
<li class="chapter" data-level="1.1.1" data-path="C-Intro.html"><a href="C-Intro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="C-Intro.html"><a href="C-Intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.2</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="C-Intro.html"><a href="C-Intro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="C-Intro.html"><a href="C-Intro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="C-Intro.html"><a href="C-Intro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="C-Intro.html"><a href="C-Intro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="C-Intro.html"><a href="C-Intro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.3.1" data-path="C-Intro.html"><a href="C-Intro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="C-Intro.html"><a href="C-Intro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="C-Intro.html"><a href="C-Intro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="C-Intro.html"><a href="C-Intro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html"><i class="fa fa-check"></i><b>2</b> Frequency Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>2.1</b> Frequency Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Basic Frequency Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>2.2.1</b> Foundations</a></li>
<li class="chapter" data-level="2.2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>2.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> The (a, b, 0) Class</a></li>
<li class="chapter" data-level="2.4" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimating Frequency Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>2.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Frequency Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="2.7" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="2.8" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>2.9</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="2.9.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>2.9.1</b> TS 2.A. R Code for Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C-Severity.html"><a href="C-Severity.html"><i class="fa fa-check"></i><b>3</b> Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.1" data-path="C-Severity.html"><a href="C-Severity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Basic Distributional Quantities</a><ul>
<li class="chapter" data-level="3.1.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>3.1.1</b> Moments</a></li>
<li class="chapter" data-level="3.1.2" data-path="C-Severity.html"><a href="C-Severity.html#quantiles"><i class="fa fa-check"></i><b>3.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="C-Severity.html"><a href="C-Severity.html#moment-generating-function"><i class="fa fa-check"></i><b>3.1.3</b> Moment Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C-Severity.html"><a href="C-Severity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions for Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Loss:Gamma"><i class="fa fa-check"></i><b>3.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="C-Severity.html"><a href="C-Severity.html#pareto-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="C-Severity.html"><a href="C-Severity.html#S:LS:Weibull"><i class="fa fa-check"></i><b>3.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="C-Severity.html"><a href="C-Severity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>3.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C-Severity.html"><a href="C-Severity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Methods of Creating New Distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="C-Severity.html"><a href="C-Severity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="3.3.2" data-path="C-Severity.html"><a href="C-Severity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>3.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="3.3.3" data-path="C-Severity.html"><a href="C-Severity.html#raising-to-a-power"><i class="fa fa-check"></i><b>3.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="3.3.4" data-path="C-Severity.html"><a href="C-Severity.html#exponentiation"><i class="fa fa-check"></i><b>3.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="3.3.5" data-path="C-Severity.html"><a href="C-Severity.html#finite-mixtures"><i class="fa fa-check"></i><b>3.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="3.3.6" data-path="C-Severity.html"><a href="C-Severity.html#continuous-mixtures"><i class="fa fa-check"></i><b>3.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="C-Severity.html"><a href="C-Severity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>3.4</b> Coverage Modifications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="3.4.2" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="3.4.3" data-path="C-Severity.html"><a href="C-Severity.html#coinsurance-and-inflation"><i class="fa fa-check"></i><b>3.4.3</b> Coinsurance and Inflation</a></li>
<li class="chapter" data-level="3.4.4" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C-Severity.html"><a href="C-Severity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>3.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="3.5.2" data-path="C-Severity.html"><a href="C-Severity.html#S:Loss:MLEModified"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Likelihood Estimators using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C-Severity.html"><a href="C-Severity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>3.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html"><i class="fa fa-check"></i><b>4</b> Model Selection and Estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation"><i class="fa fa-check"></i><b>4.1.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.1.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>4.1.2</b> Tools for Model Selection and Diagnostics</a></li>
<li class="chapter" data-level="4.1.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#starting-values"><i class="fa fa-check"></i><b>4.1.3</b> Starting Values</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>4.2</b> Model Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#iterative-model-selection"><i class="fa fa-check"></i><b>4.2.1</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="4.2.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-training-dataset"><i class="fa fa-check"></i><b>4.2.2</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="4.2.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>4.2.3</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="4.2.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-cross-validation"><i class="fa fa-check"></i><b>4.2.4</b> Model Selection Based on Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>4.3</b> Estimation using Modified Data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#parametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.1</b> Parametric Estimation using Modified Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.2</b> Nonparametric Estimation using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:BayesInference"><i class="fa fa-check"></i><b>4.4</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:IntroBayes"><i class="fa fa-check"></i><b>4.4.1</b> Introduction to Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-model"><i class="fa fa-check"></i><b>4.4.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-inference"><i class="fa fa-check"></i><b>4.4.3</b> Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>4.4.4</b> Conjugate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html"><i class="fa fa-check"></i><b>5</b> Aggregate Loss Models</a><ul>
<li class="chapter" data-level="5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>5.2</b> Individual Risk Model</a></li>
<li class="chapter" data-level="5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#collective-risk-model"><i class="fa fa-check"></i><b>5.3</b> Collective Risk Model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>5.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>5.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="5.3.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#analytic-results"><i class="fa fa-check"></i><b>5.3.3</b> Analytic Results</a></li>
<li class="chapter" data-level="5.3.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#tweedie-distribution"><i class="fa fa-check"></i><b>5.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>5.4</b> Computing the Aggregate Claims Distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>5.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="5.4.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#simulation"><i class="fa fa-check"></i><b>5.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>5.5</b> Effects of Coverage Modifications</a><ul>
<li class="chapter" data-level="5.5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>5.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="5.5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>5.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="5.5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>5.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>5.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#ts-5.a.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS 5.A.1. Individual Risk Model Properties</a></li>
<li><a href="C-AggLossModels.html#ts-5.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it">TS 5.A.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li><a href="C-AggLossModels.html#ts-5.a.3.-example-5.3.8-moment-generating-function-of-aggregate-loss-s_n">TS 5.A.3. Example 5.3.8 Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="C-Simulation.html"><a href="C-Simulation.html"><i class="fa fa-check"></i><b>6</b> Simulation and Resampling</a><ul>
<li class="chapter" data-level="6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#S:SimulationFundamentals"><i class="fa fa-check"></i><b>6.1</b> Simulation Fundamentals</a><ul>
<li class="chapter" data-level="6.1.1" data-path="C-Simulation.html"><a href="C-Simulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="6.1.2" data-path="C-Simulation.html"><a href="C-Simulation.html#S:InverseTransform"><i class="fa fa-check"></i><b>6.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="6.1.3" data-path="C-Simulation.html"><a href="C-Simulation.html#simulation-precision"><i class="fa fa-check"></i><b>6.1.3</b> Simulation Precision</a></li>
<li class="chapter" data-level="6.1.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:SimulationStatInference"><i class="fa fa-check"></i><b>6.1.4</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="C-Simulation.html"><a href="C-Simulation.html#S:Bootstrap"><i class="fa fa-check"></i><b>6.2</b> Bootstrapping and Resampling</a><ul>
<li class="chapter" data-level="6.2.1" data-path="C-Simulation.html"><a href="C-Simulation.html#bootstrap-foundations"><i class="fa fa-check"></i><b>6.2.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="6.2.2" data-path="C-Simulation.html"><a href="C-Simulation.html#bootstrap-precision-bias-standard-deviation-and-mse"><i class="fa fa-check"></i><b>6.2.2</b> Bootstrap Precision: Bias, Standard Deviation, and MSE</a></li>
<li class="chapter" data-level="6.2.3" data-path="C-Simulation.html"><a href="C-Simulation.html#confidence-intervals"><i class="fa fa-check"></i><b>6.2.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.2.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:ParametricBootStrap"><i class="fa fa-check"></i><b>6.2.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="C-Simulation.html"><a href="C-Simulation.html#S:CrossValidation"><i class="fa fa-check"></i><b>6.3</b> Cross-Validation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="C-Simulation.html"><a href="C-Simulation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.3.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="6.3.2" data-path="C-Simulation.html"><a href="C-Simulation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="6.3.3" data-path="C-Simulation.html"><a href="C-Simulation.html#cross-validation-and-bootstrap"><i class="fa fa-check"></i><b>6.3.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:ImportanceSampling"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.5" data-path="C-Simulation.html"><a href="C-Simulation.html#S:MCMC"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo Markov Chain (MCMC)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="C-Simulation.html"><a href="C-Simulation.html#hastings-metropolis"><i class="fa fa-check"></i><b>6.5.1</b> Hastings Metropolis</a></li>
<li class="chapter" data-level="6.5.2" data-path="C-Simulation.html"><a href="C-Simulation.html#gibbs-sampler"><i class="fa fa-check"></i><b>6.5.2</b> Gibbs sampler</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="C-Simulation.html"><a href="C-Simulation.html#Simulation:further-reading-and-resources"><i class="fa fa-check"></i><b>6.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="6.6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#ts-6.a.-bootstrap-applications-in-predictive-modeling"><i class="fa fa-check"></i><b>6.6.1</b> TS 6.A. Bootstrap Applications in Predictive Modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html"><i class="fa fa-check"></i><b>7</b> Premium Foundations</a><ul>
<li class="chapter" data-level="7.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:IntroductionRatemaking"><i class="fa fa-check"></i><b>7.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="7.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:AggRateMaking"><i class="fa fa-check"></i><b>7.2</b> Aggregate Ratemaking Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:PurePremium"><i class="fa fa-check"></i><b>7.2.1</b> Pure Premium Method</a></li>
<li class="chapter" data-level="7.2.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:LossRatio"><i class="fa fa-check"></i><b>7.2.2</b> Loss Ratio Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:PricingPrinciples"><i class="fa fa-check"></i><b>7.3</b> Pricing Principles</a><ul>
<li class="chapter" data-level="7.3.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#premium-principles"><i class="fa fa-check"></i><b>7.3.1</b> Premium Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#properties-of-premium-principles"><i class="fa fa-check"></i><b>7.3.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:HeterogeneousRisks"><i class="fa fa-check"></i><b>7.4</b> Heterogeneous Risks</a><ul>
<li class="chapter" data-level="7.4.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:ExposureToRisk"><i class="fa fa-check"></i><b>7.4.1</b> Exposure to Risk</a></li>
<li class="chapter" data-level="7.4.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:RatingFactors"><i class="fa fa-check"></i><b>7.4.2</b> Rating Factors</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:TrendDevelopment"><i class="fa fa-check"></i><b>7.5</b> Development and Trending</a><ul>
<li class="chapter" data-level="7.5.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#exposures-and-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Exposures and Premiums</a></li>
<li class="chapter" data-level="7.5.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#losses-claims-and-payments"><i class="fa fa-check"></i><b>7.5.2</b> Losses, Claims, and Payments</a></li>
<li class="chapter" data-level="7.5.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:CompareMethods"><i class="fa fa-check"></i><b>7.5.3</b> Comparing Pure Premium and Loss Ratio Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:GiniStatistic"><i class="fa fa-check"></i><b>7.6</b> Selecting a Premium</a><ul>
<li class="chapter" data-level="7.6.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#classic-lorenz-curve"><i class="fa fa-check"></i><b>7.6.1</b> Classic Lorenz Curve</a></li>
<li class="chapter" data-level="7.6.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#performance-curve-and-a-gini-statistic"><i class="fa fa-check"></i><b>7.6.2</b> Performance Curve and a Gini Statistic</a></li>
<li class="chapter" data-level="7.6.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#out-of-sample-validation"><i class="fa fa-check"></i><b>7.6.3</b> Out-of-Sample Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>7.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#ts-7.a.-rate-regulation"><i class="fa fa-check"></i>TS 7.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C-RiskClass.html"><a href="C-RiskClass.html"><i class="fa fa-check"></i><b>8</b> Risk Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>8.2</b> Poisson Regression Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>8.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>8.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>8.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="8.2.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#exercises-3"><i class="fa fa-check"></i><b>8.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>8.3</b> Categorical Variables and Multiplicative Tariff</a><ul>
<li class="chapter" data-level="8.3.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>8.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="8.3.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>8.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="8.3.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>8.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="8.3.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>8.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>8.4</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ts-8.a.-estimating-poisson-regression-models"><i class="fa fa-check"></i>TS 8.A. Estimating Poisson Regression Models</a></li>
<li class="chapter" data-level="" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ts-8.b.-selecting-rating-factors"><i class="fa fa-check"></i>TS 8.B. Selecting Rating Factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C-Credibility.html"><a href="C-Credibility.html"><i class="fa fa-check"></i><b>9</b> Experience Rating Using Credibility Theory</a><ul>
<li class="chapter" data-level="9.1" data-path="C-Credibility.html"><a href="C-Credibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>9.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="9.2" data-path="C-Credibility.html"><a href="C-Credibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.2</b> Limited Fluctuation Credibility</a><ul>
<li class="chapter" data-level="9.2.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:frequency"><i class="fa fa-check"></i><b>9.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="9.2.2" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>9.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="9.2.3" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>9.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="9.2.4" data-path="C-Credibility.html"><a href="C-Credibility.html#partial-credibility"><i class="fa fa-check"></i><b>9.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-credibility"><i class="fa fa-check"></i><b>9.3</b> Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.3.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>9.3.1</b> Credibility Z, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-straub-credibility"><i class="fa fa-check"></i><b>9.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="9.5" data-path="C-Credibility.html"><a href="C-Credibility.html#bayesian-inference-and-buhlmann-credibility"><i class="fa fa-check"></i><b>9.5</b> Bayesian Inference and Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.5.1" data-path="C-Credibility.html"><a href="C-Credibility.html#gamma-poisson-model"><i class="fa fa-check"></i><b>9.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="9.5.2" data-path="C-Credibility.html"><a href="C-Credibility.html#beta-binomial-model"><i class="fa fa-check"></i><b>9.5.2</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="9.5.3" data-path="C-Credibility.html"><a href="C-Credibility.html#exact-credibility"><i class="fa fa-check"></i><b>9.5.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="C-Credibility.html"><a href="C-Credibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>9.6</b> Estimating Credibility Parameters</a><ul>
<li class="chapter" data-level="9.6.1" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="9.6.2" data-path="C-Credibility.html"><a href="C-Credibility.html#nonparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.2</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.3" data-path="C-Credibility.html"><a href="C-Credibility.html#semiparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.3</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.4" data-path="C-Credibility.html"><a href="C-Credibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>9.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="C-Credibility.html"><a href="C-Credibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C-PortMgt.html"><a href="C-PortMgt.html"><i class="fa fa-check"></i><b>10</b> Insurance Portfolio Management including Reinsurance</a><ul>
<li class="chapter" data-level="10.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#introduction-to-insurance-portfolios"><i class="fa fa-check"></i><b>10.1</b> Introduction to Insurance Portfolios</a></li>
<li class="chapter" data-level="10.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Tails"><i class="fa fa-check"></i><b>10.2</b> Tails of Distributions</a><ul>
<li class="chapter" data-level="10.2.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>10.2.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="10.2.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>10.2.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>10.3</b> Risk Measures</a><ul>
<li class="chapter" data-level="10.3.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>10.3.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="10.3.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>10.3.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="10.3.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>10.3.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>10.4</b> Reinsurance</a><ul>
<li class="chapter" data-level="10.4.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>10.4.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.4.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>10.4.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.4.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>10.4.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="C-PortMgt.html"><a href="C-PortMgt.html#further-resources-and-contributors-1"><i class="fa fa-check"></i><b>10.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C-LossReserves.html"><a href="C-LossReserves.html"><i class="fa fa-check"></i><b>11</b> Loss Reserving</a><ul>
<li class="chapter" data-level="11.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:motivation"><i class="fa fa-check"></i><b>11.1</b> Motivation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:claim-types"><i class="fa fa-check"></i><b>11.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="11.1.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#why-reserving"><i class="fa fa-check"></i><b>11.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Data"><i class="fa fa-check"></i><b>11.2</b> Loss Reserve Data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#from-micro-to-macro"><i class="fa fa-check"></i><b>11.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="11.2.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#run-off-triangles"><i class="fa fa-check"></i><b>11.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="11.2.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#loss-reserve-notation"><i class="fa fa-check"></i><b>11.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Rcode"><i class="fa fa-check"></i><b>11.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Chain-ladder"><i class="fa fa-check"></i><b>11.3</b> The Chain-Ladder</a><ul>
<li class="chapter" data-level="11.3.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:DeterministicCL"><i class="fa fa-check"></i><b>11.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="11.3.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#macks-distribution-free-chain-ladder-model"><i class="fa fa-check"></i><b>11.3.2</b> Mack’s Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="11.3.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#r-code-for-chain-ladder-predictions"><i class="fa fa-check"></i><b>11.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:GLMs"><i class="fa fa-check"></i><b>11.4</b> GLMs and Bootstrap for Loss Reserves</a><ul>
<li class="chapter" data-level="11.4.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#model-specification"><i class="fa fa-check"></i><b>11.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="11.4.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>11.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="11.4.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#bootstrap"><i class="fa fa-check"></i><b>11.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="C-LossReserves.html"><a href="C-LossReserves.html#LossRe:further-reading-and-resources"><i class="fa fa-check"></i><b>11.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html"><i class="fa fa-check"></i><b>12</b> Experience Rating using Bonus-Malus</a><ul>
<li class="chapter" data-level="12.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:Intro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:NCD"><i class="fa fa-check"></i><b>12.2</b> NCD System in Several Countries</a><ul>
<li class="chapter" data-level="12.2.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#ncd-system-in-malaysia"><i class="fa fa-check"></i><b>12.2.1</b> NCD System in Malaysia</a></li>
<li class="chapter" data-level="12.2.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#ncd-system-in-other-countries"><i class="fa fa-check"></i><b>12.2.2</b> NCD System in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:BMS"><i class="fa fa-check"></i><b>12.3</b> BMS and Markov Chain Model</a><ul>
<li class="chapter" data-level="12.3.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>12.3.1</b> Transition Probability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:StatDist"><i class="fa fa-check"></i><b>12.4</b> BMS and Stationary Distribution</a><ul>
<li class="chapter" data-level="12.4.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>12.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-stationary-distribution"><i class="fa fa-check"></i><b>12.4.2</b> R Program for Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>12.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="12.4.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-premium-evolution"><i class="fa fa-check"></i><b>12.4.4</b> R Program for Premium Evolution</a></li>
<li class="chapter" data-level="12.4.5" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>12.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="12.4.6" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-convergence-rate"><i class="fa fa-check"></i><b>12.4.6</b> R Program for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:PremRtg"><i class="fa fa-check"></i><b>12.5</b> BMS and Premium Rating</a><ul>
<li class="chapter" data-level="12.5.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>12.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="12.5.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#frequency-model-poisson-and-negative-binomial-regressions"><i class="fa fa-check"></i><b>12.5.2</b> Frequency Model – Poisson and Negative Binomial Regressions</a></li>
<li class="chapter" data-level="12.5.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating-with-bonus-malus-data"><i class="fa fa-check"></i><b>12.5.3</b> Premium Rating with Bonus-Malus Data</a></li>
<li class="chapter" data-level="12.5.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating-without-bonus-malus-data"><i class="fa fa-check"></i><b>12.5.4</b> Premium Rating without Bonus-Malus Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C-DataSystems.html"><a href="C-DataSystems.html"><i class="fa fa-check"></i><b>13</b> Data and Systems</a><ul>
<li class="chapter" data-level="13.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data"><i class="fa fa-check"></i><b>13.1</b> Data</a><ul>
<li class="chapter" data-level="13.1.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-types-and-sources"><i class="fa fa-check"></i><b>13.1.1</b> Data Types and Sources</a></li>
<li class="chapter" data-level="13.1.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-structures-and-storage"><i class="fa fa-check"></i><b>13.1.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="13.1.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-quality"><i class="fa fa-check"></i><b>13.1.3</b> Data Quality</a></li>
<li class="chapter" data-level="13.1.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-cleaning"><i class="fa fa-check"></i><b>13.1.4</b> Data Cleaning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-preliminaries"><i class="fa fa-check"></i><b>13.2</b> Data Analysis Preliminaries</a><ul>
<li class="chapter" data-level="13.2.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:process"><i class="fa fa-check"></i><b>13.2.1</b> Data Analysis Process</a></li>
<li class="chapter" data-level="13.2.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-versus-confirmatory"><i class="fa fa-check"></i><b>13.2.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="13.2.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>13.2.3</b> Supervised versus Unsupervised</a></li>
<li class="chapter" data-level="13.2.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#parametric-versus-nonparametric"><i class="fa fa-check"></i><b>13.2.4</b> Parametric versus Nonparametric</a></li>
<li class="chapter" data-level="13.2.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:expred"><i class="fa fa-check"></i><b>13.2.5</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="13.2.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-modeling-versus-algorithmic-modeling"><i class="fa fa-check"></i><b>13.2.6</b> Data Modeling versus Algorithmic Modeling</a></li>
<li class="chapter" data-level="13.2.7" data-path="C-DataSystems.html"><a href="C-DataSystems.html#big-data-analysis"><i class="fa fa-check"></i><b>13.2.7</b> Big Data Analysis</a></li>
<li class="chapter" data-level="13.2.8" data-path="C-DataSystems.html"><a href="C-DataSystems.html#reproducible-analysis"><i class="fa fa-check"></i><b>13.2.8</b> Reproducible Analysis</a></li>
<li class="chapter" data-level="13.2.9" data-path="C-DataSystems.html"><a href="C-DataSystems.html#ethical-issues"><i class="fa fa-check"></i><b>13.2.9</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-techniques"><i class="fa fa-check"></i><b>13.3</b> Data Analysis Techniques</a><ul>
<li class="chapter" data-level="13.3.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-techniques"><i class="fa fa-check"></i><b>13.3.1</b> Exploratory Techniques</a></li>
<li class="chapter" data-level="13.3.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#confirmatory-techniques"><i class="fa fa-check"></i><b>13.3.2</b> Confirmatory Techniques</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#some-r-functions"><i class="fa fa-check"></i><b>13.4</b> Some R Functions</a></li>
<li class="chapter" data-level="13.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html"><i class="fa fa-check"></i><b>14</b> Dependence Modeling</a><ul>
<li class="chapter" data-level="14.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:VarTypes"><i class="fa fa-check"></i><b>14.1</b> Variable Types</a><ul>
<li class="chapter" data-level="14.1.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuaVar"><i class="fa fa-check"></i><b>14.1.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuanVar"><i class="fa fa-check"></i><b>14.1.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>14.1.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>14.2</b> Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>14.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>14.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>14.2.3</b> Nominal Variables</a></li>
<li class="chapter" data-level="14.2.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#ordinal-variables"><i class="fa fa-check"></i><b>14.2.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="14.2.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#interval-variables"><i class="fa fa-check"></i><b>14.2.5</b> Interval Variables</a></li>
<li class="chapter" data-level="14.2.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#discrete-and-continuous-variables"><i class="fa fa-check"></i><b>14.2.6</b> Discrete and Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>14.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="14.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>14.4</b> Application Using Copulas</a><ul>
<li class="chapter" data-level="14.4.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#data-description"><i class="fa fa-check"></i><b>14.4.1</b> Data Description</a></li>
<li class="chapter" data-level="14.4.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>14.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="14.4.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>14.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="14.4.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>14.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>14.5</b> Types of Copulas</a><ul>
<li class="chapter" data-level="14.5.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#elliptical-copulas"><i class="fa fa-check"></i><b>14.5.1</b> Elliptical Copulas</a></li>
<li class="chapter" data-level="14.5.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#archimedian-copulas"><i class="fa fa-check"></i><b>14.5.2</b> Archimedian Copulas</a></li>
<li class="chapter" data-level="14.5.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>14.5.3</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>14.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="14.7" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>14.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#ts-14.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 14.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C-AppA.html"><a href="C-AppA.html"><i class="fa fa-check"></i><b>15</b> Appendix A: Review of Statistical Inference</a><ul>
<li class="chapter" data-level="15.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>15.1</b> Basic Concepts</a><ul>
<li class="chapter" data-level="15.1.1" data-path="C-AppA.html"><a href="C-AppA.html#random-sampling"><i class="fa fa-check"></i><b>15.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="15.1.2" data-path="C-AppA.html"><a href="C-AppA.html#sampling-distribution"><i class="fa fa-check"></i><b>15.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="15.1.3" data-path="C-AppA.html"><a href="C-AppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>15.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>15.2</b> Point Estimation and Properties</a><ul>
<li class="chapter" data-level="15.2.1" data-path="C-AppA.html"><a href="C-AppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>15.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="15.2.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>15.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>15.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="15.3.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>15.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="15.3.2" data-path="C-AppA.html"><a href="C-AppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>15.3.2</b> Large-sample Properties of MLE</a></li>
<li class="chapter" data-level="15.3.3" data-path="C-AppA.html"><a href="C-AppA.html#confidence-interval"><i class="fa fa-check"></i><b>15.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>15.4</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="15.4.1" data-path="C-AppA.html"><a href="C-AppA.html#basic-concepts"><i class="fa fa-check"></i><b>15.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="15.4.2" data-path="C-AppA.html"><a href="C-AppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>15.4.2</b> Student-<span class="math inline">\(t\)</span> test based on MLE</a></li>
<li class="chapter" data-level="15.4.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>15.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="15.4.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>15.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C-AppB.html"><a href="C-AppB.html"><i class="fa fa-check"></i><b>16</b> Appendix B: Iterated Expectations</a><ul>
<li class="chapter" data-level="16.1" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>16.1</b> Conditional Distribution and Conditional Expectation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="C-AppB.html"><a href="C-AppB.html#conditional-distribution"><i class="fa fa-check"></i><b>16.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="16.1.2" data-path="C-AppB.html"><a href="C-AppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>16.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>16.2</b> Iterated Expectations and Total Variance</a><ul>
<li class="chapter" data-level="16.2.1" data-path="C-AppB.html"><a href="C-AppB.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>16.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="16.2.2" data-path="C-AppB.html"><a href="C-AppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>16.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="C-AppB.html"><a href="C-AppB.html#application"><i class="fa fa-check"></i><b>16.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="C-AppB.html"><a href="C-AppB.html#S:AppConjugateDistributions"><i class="fa fa-check"></i><b>16.3</b> Conjugate Distributions</a><ul>
<li class="chapter" data-level="16.3.1" data-path="C-AppB.html"><a href="C-AppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>16.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="16.3.2" data-path="C-AppB.html"><a href="C-AppB.html#conjugate-distributions"><i class="fa fa-check"></i><b>16.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C-AppC.html"><a href="C-AppC.html"><i class="fa fa-check"></i><b>17</b> Appendix C: Maximum Likelihood Theory</a><ul>
<li class="chapter" data-level="17.1" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>17.1</b> Likelihood Function</a><ul>
<li class="chapter" data-level="17.1.1" data-path="C-AppC.html"><a href="C-AppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="17.1.2" data-path="C-AppC.html"><a href="C-AppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>17.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>17.2</b> Maximum Likelihood Estimators</a><ul>
<li class="chapter" data-level="17.2.1" data-path="C-AppC.html"><a href="C-AppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>17.2.1</b> Definition and Derivation of MLE</a></li>
<li class="chapter" data-level="17.2.2" data-path="C-AppC.html"><a href="C-AppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotic Properties of MLE</a></li>
<li class="chapter" data-level="17.2.3" data-path="C-AppC.html"><a href="C-AppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>17.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>17.3</b> Statistical Inference Based on Maximum Likelhood Estimation</a><ul>
<li class="chapter" data-level="17.3.1" data-path="C-AppC.html"><a href="C-AppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>17.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="17.3.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>17.3.2</b> MLE and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html"><i class="fa fa-check"></i><b>18</b> Appendix D: Summary of Distributions</a><ul>
<li class="chapter" data-level="18.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#discrete-distributions"><i class="fa fa-check"></i><b>18.1</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="18.1.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>18.1.1</b> The (a,b,0) Class</a></li>
<li class="chapter" data-level="18.1.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>18.1.2</b> The (a,b,1) Class</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#continuous-distributions"><i class="fa fa-check"></i><b>18.2</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>18.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>18.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.3" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>18.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.4" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>18.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="18.2.5" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>18.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="18.2.6" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>18.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>18.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C:Simulation" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Simulation and Resampling</h1>
<p><em>Chapter Preview.</em> Simulation is a computationally intensive method used to solve difficult problems. Instead of creating physical processes and experimenting with them in order to understand their operational characteristics, a simulation study is based on a computer representation - it considers various hypothetical conditions as inputs and summarizes the results. Through simulation, a vast number of hypothetical conditions can be quickly and inexpensively examined. Section <a href="C-Simulation.html#S:SimulationFundamentals">6.1</a> introduces simulation, a wonderful computational tool that is especially useful in complex, multivariate settings.</p>
<p>We can also use simulation to draw from an empirical distribution - this process is known as resampling. Resampling allows us to assess the uncertainty of estimates in complex models. Section <a href="C-Simulation.html#S:Bootstrap">6.2</a> introduces resampling in the context of bootstrapping to determine the precision of estimators.</p>
<p>Subsequent sections introduce other topics in resampling. Section <a href="C-Simulation.html#S:CrossValidation">6.3</a> on cross-validation shows how to use it for model selection and validation. Section <a href="C-Simulation.html#S:ImportanceSampling">6.4</a> on importance sampling describes resampling in specific regions of interest, such as long-tailed actuarial applications. Section <a href="C-Simulation.html#S:MCMC">6.5</a> on Monte Carlo Markov Chain (MCMC) introduces the simulation and resampling engine underpinning much of modern Bayesian analysis.</p>
<div id="S:SimulationFundamentals" class="section level2">
<h2><span class="header-section-number">6.1</span> Simulation Fundamentals</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Generate approximately independent realizations that are uniformly distributed</li>
<li>Transform the uniformly distributed realizations to observations from a probability distribution of interest</li>
<li>Calculate quantities of interest and determining the precision of the calculated quantities</li>
</ul>
<hr />
<!-- ## Simulation Fundamentals -->
<div id="generating-independent-uniform-observations" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Generating Independent Uniform Observations</h3>
<p>The <a href="#" class="tooltip" style="color:green"><em>simulations</em><span style="font-size:8pt">A computer generation of various hypothetical conditions and outputs, based on the model structure provided</span></a> that we consider are generated by computers. A major strength of this approach is that they can be replicated, allowing us to check and improve our work. Naturally, this also means that they are not really random. Nonetheless, algorithms have been produced so that results appear to be random for all practical purposes. Specifically, they pass sophisticated tests of independence and can be designed so that they come from a single distribution - our <a href="#" class="tooltip" style="color:green"><em>iid</em><span style="font-size:8pt">Independent and identically distributed</span></a> assumption, identically and independently distributed.</p>
<p>To get a sense as to what these algorithms do, we consider a historically prominent method.</p>
<p><strong>Linear Congruential Generator.</strong> To generate a sequence of random numbers, start with <span class="math inline">\(B_0\)</span>, a starting value that is known as a <em>seed</em>. This value is updated using the recursive relationship <span class="math display">\[B_{n+1} = a B_n + c  \text{ modulo }m, ~~ n=0, 1, 2, \ldots .\]</span> This algorithm is called a <a href="#" class="tooltip" style="color:green"><em>linear congruential generator</em><span style="font-size:8pt">Algorithm that yields pseudo-randomized numbers calculated using a linear recursive relationship and a starting seed value</span></a>. The case of <span class="math inline">\(c=0\)</span> is called a <em>multiplicative</em> congruential generator; it is particularly useful for really fast computations.</p>
<p>For illustrative values of <span class="math inline">\(a\)</span> and <span class="math inline">\(m\)</span>, Microsoft’s Visual Basic uses <span class="math inline">\(m=2^{24}\)</span>, <span class="math inline">\(a=1,140,671,485\)</span>, and <span class="math inline">\(c = 12,820,163\)</span> (see <a href="https://en.wikipedia.org/wiki/Linear_congruential_generator" class="uri">https://en.wikipedia.org/wiki/Linear_congruential_generator</a>). This is the engine underlying the random number generation in Microsoft’s Excel program.</p>
<p>The sequence used by the analyst is defined as <span class="math inline">\(U_n=B_n/m.\)</span> The analyst may interpret the sequence {<span class="math inline">\(U_{i}\)</span>} to be (approximately) identically and independently uniformly distributed on the interval (0,1). To illustrate the algorithm, consider the following.</p>
<p><strong>Example 6.1.1. Illustrative Sequence.</strong> Take <span class="math inline">\(m=15\)</span>, <span class="math inline">\(a=3\)</span>, <span class="math inline">\(c=2\)</span> and <span class="math inline">\(B_0=1\)</span>. Then we have:</p>
<table>
<thead>
<tr class="header">
<th align="center">step <span class="math inline">\(n\)</span></th>
<th align="left"><span class="math inline">\(B_n\)</span></th>
<th align="center"><span class="math inline">\(U_n\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="left"><span class="math inline">\(B_0=1\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="left"><span class="math inline">\(B_1 =\mod(3 \times 1 +2) = 5\)</span></td>
<td align="center"><span class="math inline">\(U_1 = \frac{5}{15}\)</span></td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="left"><span class="math inline">\(B_2 =\mod(3 \times 5 +2) = 2\)</span></td>
<td align="center"><span class="math inline">\(U_2 = \frac{2}{15}\)</span></td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="left"><span class="math inline">\(B_3 =\mod(3 \times 2 +2) = 8\)</span></td>
<td align="center"><span class="math inline">\(U_3 = \frac{8}{15}\)</span></td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="left"><span class="math inline">\(B_4 =\mod(3 \times 8 +2) = 11\)</span></td>
<td align="center"><span class="math inline">\(U_4 = \frac{11}{15}\)</span></td>
</tr>
</tbody>
</table>
<p>Sometimes computer generated random results are known as <a href="#" class="tooltip" style="color:green"><em>pseudo-random numbers</em><span style="font-size:8pt">Values that appear random but can be replicated by formula</span></a> to reflect the fact that they are machine generated and can be replicated. That is, despite the fact that {<span class="math inline">\(U_{i}\)</span>} appears to be i.i.d, it can be reproduced by using the same seed number (and the same algorithm).</p>
<p><strong>Example 6.1.2. Generating uniform random numbers in <code>R</code>.</strong> The following code shows how to generate three uniform (0,1) numbers in <code>R</code> using the <code>runif</code> command. The <code>set.seed()</code> function sets the initial seed. In many computer packages, the initial seed is set using the system clock unless specified otherwise.</p>
<div id="three-uniform-random-variates" class="section level5 unnumbered">
<h5>Three Uniform Random Variates</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span>)
U &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">3</span>)
knitr<span class="op">::</span><span class="kw">kable</span>(U, <span class="dt">digits=</span><span class="dv">5</span>, <span class="dt">align =</span> <span class="st">&quot;c&quot;</span>, <span class="dt">col.names =</span> <span class="st">&quot;Uniform&quot;</span>)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="center">Uniform</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.92424</td>
</tr>
<tr class="even">
<td align="center">0.53718</td>
</tr>
<tr class="odd">
<td align="center">0.46920</td>
</tr>
</tbody>
</table>
<hr />
<p>The linear congruential generator is just one method of producing pseudo-random outcomes. It is easy to understand and is (still) widely used. The linear congruential generator does have limitations, including the fact that it is possible to detect long-run patterns over time in the sequences generated (recall that we can interpret <em>independence</em> to mean a total lack of functional patterns). Not surprisingly, advanced techniques have been developed that address some of this method’s drawbacks.</p>
</div>
</div>
<div id="S:InverseTransform" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Inverse Transform Method</h3>
<p>With the sequence of uniform random numbers, we next transform them to a distribution of interest, say <span class="math inline">\(F\)</span>. A prominent technique is the <a href="#" class="tooltip" style="color:green"><em>inverse transform method</em><span style="font-size:8pt">Samples a uniform number between 0 and 1 to represent the randomly selected percentile, then uses the inverse of the cumulative density function of the desired distribution to simulate from in order to find the simulated value from the desired distribution</span></a>, defined as</p>
<p><span class="math display">\[
X_i=F^{-1}\left( U_i \right) .
\]</span></p>
<p>Here, recall from Section 4.1.1 that we introduced the inverse of the distribution function, <span class="math inline">\(F^{-1}\)</span>, and referred to it also as the <a href="#" class="tooltip" style="color:green"><em>quantile function</em><span style="font-size:8pt">Inverse function for the cumulative density function which takes a percentile value in [0,1] as the input, and outputs the corresponding value in the distribution</span></a>. Specifically, it is defined to be</p>
<p><span class="math display">\[
F^{-1}(y) = \inf_x ~ \{ F(x) \ge y \} .
\]</span></p>
<p>Recall that <span class="math inline">\(\inf\)</span> stands for <em>infimum</em> or the <a href="#" class="tooltip" style="color:green"><em>greatest lower bound</em><span style="font-size:8pt">Largest value that is less than or equal to a specified subset of values/elements</span></a>. It is essentially the smallest value of <em>x</em> that satisfies the inequality <span class="math inline">\(\{F(x) \ge y\}\)</span>. The result is that the sequence {<span class="math inline">\(X_{i}\)</span>} is approximately <em>iid</em> with distribution function <span class="math inline">\(F\)</span>.</p>
<p>The inverse transform result is available when the underlying random variable is continuous, discrete or a hybrid combination of the two. We now present a series of examples to illustrate its scope of applications.</p>
<p><strong>Example 6.1.3. Generating exponential random numbers.</strong> Suppose that we would like to generate observations from an exponential distribution with scale parameter <span class="math inline">\(\theta\)</span> so that <span class="math inline">\(F(x) = 1 - e^{-x/\theta}\)</span>. To compute the inverse transform, we can use the following steps: <span class="math display">\[
\begin{aligned}
 y = F(x) &amp;\Leftrightarrow  y = 1-e^{-x/\theta} \\
  &amp;\Leftrightarrow -\theta \ln(1-y) = x = F^{-1}(y) .
\end{aligned}
\]</span></p>
<p>Thus, if <span class="math inline">\(U\)</span> has a uniform (0,1) distribution, then <span class="math inline">\(X = -\theta \ln(1-U)\)</span> has an exponential distribution with parameter <span class="math inline">\(\theta\)</span>.</p>
<p>The following <code>R</code> code shows how we can start with the same three uniform random numbers as in <em>Example 6.1.2</em> and transform them to independent exponentially distributed random variables with a mean of 10. Alternatively, you can directly use the <code>rexp</code> function in <code>R</code> to generate random numbers from the exponential distribution. The algorithm built into this routine is different so even with the same starting seed number, individual realizations will differ.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span>)
U &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">3</span>)
X1 &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">10</span><span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>U)
<span class="kw">set.seed</span>(<span class="dv">2017</span>)
X2 &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">3</span>, <span class="dt">rate =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">10</span>)</code></pre></div>
<div id="three-uniform-random-variates-1" class="section level5 unnumbered">
<h5>Three Uniform Random Variates</h5>
<table>
<thead>
<tr class="header">
<th align="right">Uniform</th>
<th align="right">Exponential 1</th>
<th align="right">Exponential 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.92424</td>
<td align="right">25.80219</td>
<td align="right">3.25222</td>
</tr>
<tr class="even">
<td align="right">0.53718</td>
<td align="right">7.70409</td>
<td align="right">8.47652</td>
</tr>
<tr class="odd">
<td align="right">0.46920</td>
<td align="right">6.33362</td>
<td align="right">5.40176</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Example 6.1.4. Generating Pareto random numbers.</strong> Suppose that we would like to generate observations from a Pareto distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\theta\)</span> so that <span class="math inline">\(F(x) = 1 - \left(\frac{\theta}{x+\theta} \right)^{\alpha}\)</span>. To compute the inverse transform, we can use the following steps:</p>
<p><span class="math display">\[
\begin{aligned}
 y = F(x) &amp;\Leftrightarrow 1-y = \left(\frac{\theta}{x+\theta} \right)^{\alpha} \\
  &amp;\Leftrightarrow \left(1-y\right)^{-1/\alpha} = \frac{x+\theta}{\theta} = \frac{x}{\theta} +1 \\
    &amp;\Leftrightarrow \theta \left((1-y)^{-1/\alpha} - 1\right) = x = F^{-1}(y) .\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(X = \theta \left((1-U)^{-1/\alpha} - 1\right)\)</span> has a Pareto distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\theta\)</span>.</p>
<hr />
<p><strong>Inverse Transform Justification.</strong> Why does the random variable <span class="math inline">\(X = F^{-1}(U)\)</span> have a distribution function <span class="math inline">\(F\)</span>?</p>
<h5 style="text-align: center;">
<a id="displayTheory.1" href="javascript:toggleTheory('ShowTheory.1','displayTheory.1');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="ShowTheory.1" style="display: none">
<hr />
<p>This is easy to establish in the continuous case. Because <span class="math inline">\(U\)</span> is a Uniform random variable on (0,1), we know that <span class="math inline">\(\Pr(U \le y) = y\)</span>, for <span class="math inline">\(0 \le y \le 1\)</span>. Thus,</p>
<p><span class="math display">\[
\begin{aligned}
\Pr(X \le x) &amp;= \Pr(F^{-1}(U) \le x) \\
 &amp;= \Pr(F(F^{-1}(U)) \le F(x)) \\
&amp;= \Pr(U \le F(x)) = F(x)
\end{aligned}
\]</span></p>
<p>as required. The key step is that <span class="math inline">\(F(F^{-1}(u)) = u\)</span> for each <span class="math inline">\(u\)</span>, which is clearly true when <span class="math inline">\(F\)</span> is strictly increasing.</p>
<hr />
</div>
<p>We now consider some discrete examples.</p>
<p><strong>Example 6.1.5. Generating Bernoulli random numbers.</strong> Suppose that we wish to simulate random variables from a Bernoulli distribution with parameter <span class="math inline">\(p=0.85\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:BinaryDF"></span>
<img src="LossDataAnalytics_files/figure-html/BinaryDF-1.png" alt="Distribution Function of a Binary Random Variable" width="50%" />
<p class="caption">
Figure 6.1: Distribution Function of a Binary Random Variable
</p>
</div>
<p>A graph of the cumulative distribution function in Figure <a href="C-Simulation.html#fig:BinaryDF">6.1</a> shows that the quantile function can be written as <span class="math display">\[
\begin{aligned}
F^{-1}(y) = \left\{ \begin{array}{cc}
              0 &amp; 0&lt;y \leq 0.85 \\
              1 &amp; 0.85 &lt; y  \leq  1.0 .
            \end{array} \right.
\end{aligned}
\]</span></p>
<p>Thus, with the inverse transform we may define <span class="math display">\[
\begin{aligned}
X = \left\{ \begin{array}{cc}
              0 &amp; 0&lt;U \leq 0.85  \\
              1 &amp;  0.85 &lt; U  \leq  1.0
            \end{array} \right.
\end{aligned}
\]</span> For illustration, we generate three random numbers to get</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span>)
U &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">3</span>)
X &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">*</span>(U <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.85</span>)</code></pre></div>
</div>
<div id="three-random-variates" class="section level4 unnumbered">
<h4>Three Random Variates</h4>
<table>
<thead>
<tr class="header">
<th align="right">Uniform</th>
<th align="right">Binary X</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.92424</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">0.53718</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">0.46920</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Example 6.1.6. Generating random numbers from a discrete distribution.</strong> Consider the time of a machine failure in the first five years. The distribution of failure times is given as:</p>
<div id="discrete-distribution" class="section level5 unnumbered">
<h5>Discrete Distribution</h5>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right"><span class="math inline">\(~~~~~~~~~~\)</span></th>
<th align="right"><span class="math inline">\(~~~~~~~~~~\)</span></th>
<th align="right"><span class="math inline">\(~~~~~~~~~~\)</span></th>
<th align="right"><span class="math inline">\(~~~~~~~~~~\)</span></th>
<th align="right"><span class="math inline">\(~~~~~~~~~~\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td align="right">1.0</td>
<td align="right">2.0</td>
<td align="right">3.0</td>
<td align="right">4.0</td>
<td align="right">5.0</td>
</tr>
<tr class="even">
<td>Probability</td>
<td align="right">0.1</td>
<td align="right">0.2</td>
<td align="right">0.1</td>
<td align="right">0.4</td>
<td align="right">0.2</td>
</tr>
<tr class="odd">
<td>Distribution Function <span class="math inline">\(F(x)\)</span></td>
<td align="right">0.1</td>
<td align="right">0.3</td>
<td align="right">0.4</td>
<td align="right">0.8</td>
<td align="right">1.0</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:DiscreteDF"></span>
<img src="LossDataAnalytics_files/figure-html/DiscreteDF-1.png" alt="Distribution Function of a Discrete Random Variable" width="60%" />
<p class="caption">
Figure 6.2: Distribution Function of a Discrete Random Variable
</p>
</div>
<p>Using the graph of the distribution function in Figure <a href="C-Simulation.html#fig:DiscreteDF">6.2</a>, with the inverse transform we may define</p>
<p><span class="math display">\[
\small{
\begin{aligned}
X = \left\{ \begin{array}{cc}
              1 &amp;   0&lt;U  \leq 0.1  \\
              2 &amp;  0.1 &lt; U  \leq  0.3\\
              3 &amp;  0.3 &lt; U  \leq  0.4\\
              4 &amp;  0.4 &lt; U  \leq  0.8  \\
              5 &amp;  0.8 &lt; U  \leq  1.0     .
            \end{array} \right.
\end{aligned}
}
\]</span></p>
<hr />
<p>For general discrete random variables there may not be an ordering of outcomes. For example, a person could own one of five types of life insurance products and we might use the following algorithm to generate random outcomes:</p>
<p><span class="math display">\[
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &amp;   0&lt;U  \leq 0.1  \\
 \textrm{endowment} &amp;  0.1 &lt; U  \leq  0.3\\
\textrm{term life} &amp;  0.3 &lt; U  \leq  0.4\\
  \textrm{universal life} &amp;  0.4 &lt; U  \leq  0.8  \\
  \textrm{variable life} &amp;  0.8 &lt; U  \leq  1.0 .
            \end{array} \right.
\end{aligned}
}
\]</span></p>
<p>Another analyst may use an alternative procedure such as:</p>
<p><span class="math display">\[
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &amp;   0.9&lt;U&lt;1.0  \\
 \textrm{endowment} &amp;  0.7 \leq U &lt; 0.9\\
\textrm{term life} &amp;  0.6 \leq U &lt; 0.7\\
  \textrm{universal life} &amp;  0.2 \leq U &lt; 0.6  \\
  \textrm{variable life} &amp;  0 \leq U &lt; 0.2 .
            \end{array} \right.
\end{aligned}
}
\]</span></p>
<p>Both algorithms produce (in the long-run) the same probabilities, e.g., <span class="math inline">\(\Pr(\textrm{whole life})=0.1\)</span>, and so forth. So, neither is incorrect. You should be aware that “there is more than one way to skin a cat.” (What an old expression!) Similarly, you could use an alternative algorithm for ordered outcomes (such as failure times 1, 2, 3, 4, or 5, above).</p>
<p><strong>Example 6.1.7. Generating random numbers from a hybrid distribution.</strong> Consider a random variable that is 0 with probability 70% and is exponentially distributed with parameter <span class="math inline">\(\theta= 10,000\)</span> with probability 30%. In an insurance application, this might correspond to a 70% chance of having no insurance claims and a 30% chance of a claim - if a claim occurs, then it is exponentially distributed. The distribution function, depicted in Figure <a href="C-Simulation.html#fig:MixedDF">6.3</a>, is given as</p>
<p><span class="math display">\[
\begin{aligned}
F(y) = \left\{ \begin{array}{cc}
              0 &amp;  x&lt;0  \\
              1 - 0.3 \exp(-x/10000) &amp; x \ge 0 .
            \end{array} \right.
\end{aligned}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:MixedDF"></span>
<img src="LossDataAnalytics_files/figure-html/MixedDF-1.png" alt="Distribution Function of a Hybrid Random Variable" width="60%" />
<p class="caption">
Figure 6.3: Distribution Function of a Hybrid Random Variable
</p>
</div>
<p>From Figure <a href="C-Simulation.html#fig:MixedDF">6.3</a>, we can see that the inverse transform for generating random variables with this distribution function is</p>
<p><span class="math display">\[
\begin{aligned}
X = F^{-1}(U) = \left\{ \begin{array}{cc}
              0 &amp;  0&lt; U  \leq  0.7  \\
              -1000 \ln (\frac{1-U}{0.3}) &amp; 0.7 &lt; U &lt; 1 .
            \end{array} \right.
\end{aligned}
\]</span></p>
<p>For discrete and hybrid random variables, the key is to draw a graph of the distribution function that allows you to visualize potential values of the inverse function.</p>
</div>
</div>
</div>
<div id="simulation-precision" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Simulation Precision</h3>
<p>From the prior subsections, we now know how to generate independent simulated realizations from a distribution of interest. With these realizations, we can construct an empirical distribution and approximate the underlying distribution as precisely as needed. As we introduce more actuarial applications in this book, you will see that simulation can be applied in a wide variety of contexts.</p>
<p>Many of these applications can be reduced to the problem of approximating <span class="math inline">\(\mathrm{E~}h(X)\)</span>, where <span class="math inline">\(h(\cdot)\)</span> is some known function. Based on <span class="math inline">\(R\)</span> simulations (replications), we get <span class="math inline">\(X_1,\ldots,X_R\)</span>. From this simulated sample, we calculate an average</p>
<p><span class="math display">\[
\overline{h}_R=\frac{1}{R}\sum_{i=1}^{R} h(X_i)
\]</span> that we use as our simulated approximate (estimate) of <span class="math inline">\(\mathrm{E~}h(X)\)</span>. To estimate the precision of this approximation, we use the simulation variance</p>
<p><span class="math display">\[
s_{h,R}^2 = \frac{1}{R-1} \sum_{i=1}^{R}\left( h(X_i) -\overline{h}_R
\right) ^2.
\]</span></p>
<p>From the independence, the standard error of the estimate is <span class="math inline">\(s_{h,R}/\sqrt{R}\)</span>. This can be made as small as we like by increasing the number of replications <span class="math inline">\(R\)</span>.</p>
<p><strong>Example. 6.1.8. Portfolio management.</strong> In Section 3.4, we learned how to calculate the expected value of policies with deductibles. For an example of something that cannot be done with closed form expressions, we now consider two risks. This is a variation of a more complex example that will be covered as <em>Example 10.3.6</em>.</p>
<p>We consider two property risks of a telecommunications firm:</p>
<ul>
<li><span class="math inline">\(X_1\)</span> - buildings, modeled using a gamma distribution with mean 200 and scale parameter 100.</li>
<li><span class="math inline">\(X_2\)</span> - motor vehicles, modeled using a gamma distribution with mean 400 and scale parameter 200.</li>
</ul>
<p>Denote the total risk as <span class="math inline">\(X = X_1 + X_2.\)</span> For simplicity, you assume that these risks are independent.</p>
<p>To manage the risk, you seek some insurance protection. You wish to manage internally small building and motor vehicles amounts, up to <span class="math inline">\(M\)</span>, say. Your retained risk is <span class="math inline">\(Y_{retained}=\)</span> <span class="math inline">\(\min(X_1 + X_2,M)\)</span>. The insurer’s portion is <span class="math inline">\(Y_{insurer} = X- Y_{retained}\)</span>.</p>
<p>To be specific, we use <span class="math inline">\(M=\)</span> 400 as well as <span class="math inline">\(R=\)</span> 1000000 simulations.</p>
<p><strong>a.</strong> With the settings, we wish to determine the expected claim amount and the associated standard deviation of (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount.</p>
<h5 style="text-align: center;">
<a id="displayCode.PortMgt1.8.1" href="javascript:togglecode('toggleCode.PortMgt1.8.1','displayCode.PortMgt1.8.1');"><i><strong>Show R Code to Define the Risks</strong></i></a>
</h5>
<div id="toggleCode.PortMgt1.8.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Simulate the risks</span>
nSim &lt;-<span class="st"> </span><span class="fl">1e6</span>  <span class="co">#number of simulations</span>
<span class="kw">set.seed</span>(<span class="dv">2017</span>) <span class="co">#set seed to reproduce work </span>
X1 &lt;-<span class="st"> </span><span class="kw">rgamma</span>(nSim,alpha1,<span class="dt">scale =</span> theta1)  
X2 &lt;-<span class="st"> </span><span class="kw">rgamma</span>(nSim,alpha2,<span class="dt">scale =</span> theta2)  

<span class="co"># Portfolio Risks</span>
X         &lt;-<span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2 
Yretained &lt;-<span class="st"> </span><span class="kw">pmin</span>(X, M)
Yinsurer  &lt;-<span class="st"> </span>X <span class="op">-</span><span class="st"> </span>Yretained</code></pre></div>
</div>
<p>Here is the code for the expected claim amounts.</p>
<h5 style="text-align: center;">
<a id="displayCode.PortMgt1.8.2" href="javascript:togglecode('toggleCode.PortMgt1.8.2','displayCode.PortMgt1.8.2');"><i><strong>Show R Code To Compute Amounts</strong></i></a>
</h5>
<div id="toggleCode.PortMgt1.8.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Expected Claim Amounts</span>
ExpVec &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">as.matrix</span>(<span class="kw">c</span>(<span class="kw">mean</span>(Yretained),<span class="kw">mean</span>(Yinsurer),<span class="kw">mean</span>(X))))
sdVec &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">as.matrix</span>(<span class="kw">c</span>(<span class="kw">sd</span>(Yretained),<span class="kw">sd</span>(Yinsurer),<span class="kw">sd</span>(X))))
outMat &lt;-<span class="st"> </span><span class="kw">rbind</span>(ExpVec, sdVec)
<span class="kw">colnames</span>(outMat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Retained&quot;</span>, <span class="st">&quot;Insurer&quot;</span>,<span class="st">&quot;Total&quot;</span>)
<span class="kw">row.names</span>(outMat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Mean&quot;</span>,<span class="st">&quot;Standard Deviation&quot;</span>)
<span class="kw">round</span>(outMat,<span class="dt">digits=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>                   Retained Insurer  Total
Mean                 365.17  235.01 600.18
Standard Deviation    69.51  280.86 316.36</code></pre>
</div>
<p>The results of these calculations are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(outMat,<span class="dt">digits=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>                   Retained Insurer  Total
Mean                 365.17  235.01 600.18
Standard Deviation    69.51  280.86 316.36</code></pre>
<p><strong>b.</strong> For insured claims, the standard error of the simulation approximation is <span class="math inline">\(s_{h,R}/\sqrt{1000000} =\)</span> 280.86 <span class="math inline">\(/\sqrt{1000000} =\)</span> 0.281. For this example, simulation is quick and so a large value such as 1000000 is an easy choice. However, for more complex problems, the simulation size may be an issue.</p>
<h5 style="text-align: center;">
<a id="displayCode.PortMgt1.8.3" href="javascript:togglecode('toggleCode.PortMgt1.8.3','displayCode.PortMgt1.8.3');"><i><strong>Show R Code To Set up the Visualization</strong></i></a>
</h5>
<div id="toggleCode.PortMgt1.8.3" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Yinsurefct &lt;-<span class="st"> </span><span class="cf">function</span>(numSim){
X1 &lt;-<span class="st"> </span><span class="kw">rgamma</span>(numSim,alpha1,<span class="dt">scale =</span> theta1)  
X2 &lt;-<span class="st"> </span><span class="kw">rgamma</span>(numSim,alpha2,<span class="dt">scale =</span> theta2)  
<span class="co"># Portfolio Risks</span>
X         &lt;-<span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2 
Yinsurer &lt;-<span class="st"> </span>X <span class="op">-</span><span class="st"> </span><span class="kw">pmin</span>(X, M)
<span class="kw">return</span>(Yinsurer)
}
R &lt;-<span class="st"> </span><span class="fl">1e3</span>
nPath &lt;-<span class="st"> </span><span class="dv">20</span>
<span class="kw">set.seed</span>(<span class="dv">2017</span>)
simU &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">Yinsurefct</span>(R<span class="op">*</span>nPath),R,nPath)
sumP2 &lt;-<span class="st"> </span><span class="kw">apply</span>(simU, <span class="dv">2</span>, cumsum)<span class="op">/</span>(<span class="dv">1</span><span class="op">:</span>R)</code></pre></div>
</div>
<p>Figure <a href="C-Simulation.html#fig:PortfolioDF">6.4</a> allows us to visualize the development of the approximation as the number of simulations increases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">matplot</span>(<span class="dv">1</span><span class="op">:</span>R,sumP2[,<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>],<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="kw">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">2</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">100</span>, <span class="dv">400</span>),
        <span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;Number of Simulations (&quot;</span>, <span class="kw">italic</span>(<span class="st">&#39;R&#39;</span>), <span class="st">&quot;)&quot;</span>)), <span class="dt">ylab=</span><span class="st">&quot;Expected Insurer Claims&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(Yinsurer),<span class="dt">lty=</span><span class="dv">2</span>)
bonds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(Yinsurer)<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">:</span>R)),<span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(Yinsurer)<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">:</span>R)))
<span class="kw">matlines</span>(<span class="dv">1</span><span class="op">:</span>R,bonds<span class="op">+</span><span class="kw">mean</span>(Yinsurer),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">1</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:PortfolioDF"></span>
<img src="LossDataAnalytics_files/figure-html/PortfolioDF-1.png" alt="Estimated Expected Insurer Claims versus Number of Simulations." width="768" />
<p class="caption">
Figure 6.4: Estimated Expected Insurer Claims versus Number of Simulations.
</p>
</div>
<hr />
<p><strong>Determination of Number of Simulations</strong></p>
<p>How many simulated values are recommended? 100? 1,000,000? We can use the <a href="#" class="tooltip" style="color:green"><em>central limit theorem</em><span style="font-size:8pt">The sample mean and sample sum of a random sample of n from a population will converge to a normal curve as the sample size n grows</span></a> to respond to this question.</p>
<p>As one criterion for your confidence in the result, suppose that you wish to be within 1% of the mean with 95% certainty. That is, you want <span class="math inline">\(\Pr \left( |\overline{h}_R - \mathrm{E~}h(X)| \le 0.01 \mathrm{E~}h(X) \right) \le 0.95\)</span>. According to the central limit theorem, your estimate should be approximately normally distributed and so we want to have <span class="math inline">\(R\)</span> large enough to satisfy <span class="math inline">\(0.01 \mathrm{E~}h(X)/\sqrt{\mathrm{Var~}h(X)/R}) \ge 1.96\)</span>. (Recall that 1.96 is the 97.5th percentile from the standard normal distribution.) Replacing <span class="math inline">\(\mathrm{E~}h(X)\)</span> and <span class="math inline">\(\mathrm{Var~}h(X)\)</span> with estimates, you continue your simulation until</p>
<p><span class="math display">\[
\frac{.01\overline{h}_R}{s_{h,R}/\sqrt{R}}\geq 1.96
\]</span> or equivalently</p>
<span class="math display" id="eq:NumSimulations">\[\begin{equation}
R \geq 38,416\frac{s_{h,R}^2}{\overline{h}_R^2}.
\tag{6.1}
\end{equation}\]</span>
<p>This criterion is a direct application of the approximate normality. Note that <span class="math inline">\(\overline{h}_R\)</span> and <span class="math inline">\(s_{h,R}\)</span> are not known in advance, so you will have to come up with estimates, either by doing a small pilot study in advance or by interrupting your procedure intermittently to see if the criterion is satisfied.</p>
<p><strong>Example. 6.1.8. Portfolio management - continued</strong></p>
<p>For our example, the average insurance claim is 235.011 and the corresponding standard deviation is 280.862. Using equation <a href="C-Simulation.html#eq:NumSimulations">(6.1)</a>, to be within 10% of the mean, we would only require at least 54.87 thousand simulations. However, to be within 1% we would want at least 5.49 million simulations.</p>
<hr />
<p><strong>Example. 6.1.9. Approximation choices.</strong> An important application of simulation is the approximation of <span class="math inline">\(\mathrm{E~}h(X)\)</span>. In this example, we show that the choice of the <span class="math inline">\(h(\cdot)\)</span> function and the distribution of <span class="math inline">\(X\)</span> can play a role.</p>
<p>Consider the following question : what is <span class="math inline">\(\Pr[X&gt;2]\)</span> when <span class="math inline">\(X\)</span> has a <a href="#" class="tooltip" style="color:green"><em>Cauchy distribution</em><span style="font-size:8pt">A continuous distribution that represents the distribution of the ratio of two independent normally random variables, where the denominator distribution has mean zero</span></a>, with density <span class="math inline">\(f(x) =\left(\pi(1+x^2)\right)^{-1}\)</span>, on the real line? The true value is</p>
<p><span class="math display">\[
\Pr\left[X&gt;2\right] = \int_2^\infty \frac{dx}{\pi(1+x^2)} .
\]</span> One can use an <code>R</code> numerical integration function (which works usually well on improper integrals)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">true_value &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="cf">function</span>(x) <span class="dv">1</span><span class="op">/</span>(pi<span class="op">*</span>(<span class="dv">1</span><span class="op">+</span>x<span class="op">^</span><span class="dv">2</span>)),<span class="dt">lower=</span><span class="dv">2</span>,<span class="dt">upper=</span><span class="ot">Inf</span>)<span class="op">$</span>value</code></pre></div>
<p>which is equal to 0.14758.</p>
<p>Alternatively, one can use simulation techniques to approximate that quantity. From calculus, you can check that the quantile function of the Cauchy distribution is <span class="math inline">\(F^{-1}(y) = \tan \left( \pi(y-0.5) \right)\)</span>. Then, with simulated uniform (0,1) variates, <span class="math inline">\(U_1, \ldots, U_R\)</span>, we can construct the estimator</p>
<p><span class="math display">\[
p_1 = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(F^{-1}(U_i)&gt;2) = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(\tan \left( \pi(U_i-0.5) \right)&gt;2) .
\]</span></p>
<h5 style="text-align: center;">
<a id="displayCode.Approximationchoices.1.9.1" href="javascript:togglecode('toggleCode.Approximationchoices.1.9.1','displayCode.Approximationchoices.1.9.1');"><i><strong>Show the R Code</strong></i></a>
</h5>
<div id="toggleCode.Approximationchoices.1.9.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Q &lt;-<span class="st"> </span><span class="cf">function</span>(u) <span class="kw">tan</span>(pi<span class="op">*</span>(u<span class="op">-</span>.<span class="dv">5</span>))
R &lt;-<span class="st"> </span><span class="fl">1e6</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
X &lt;-<span class="st"> </span><span class="kw">Q</span>(<span class="kw">runif</span>(R))
p1 &lt;-<span class="st"> </span><span class="kw">mean</span>(X<span class="op">&gt;</span><span class="dv">2</span>)
se.p1 &lt;-<span class="st"> </span><span class="kw">sd</span>(X<span class="op">&gt;</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sqrt</span>(R)
p1</code></pre></div>
<pre><code>[1] 0.147439</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">se.p1</code></pre></div>
<pre><code>[1] 0.0003545432</code></pre>
</div>
<p>With one million simulations, we obtain an estimate of 0.14744 with standard error 0.355 (divided by 1000). One can prove that the variance of <span class="math inline">\(p_1\)</span> is of order <span class="math inline">\(0.127/R\)</span>.</p>
<!--It can be visualized below -->
<p>With other choices of <span class="math inline">\(h(\cdot)\)</span> and <span class="math inline">\(F(\cdot)\)</span>, it is actually possible to reduce uncertainty even using the same number of simulations <code>R</code>. To begin, one can use the symmetry of the Cauchy distribution to write <span class="math inline">\(\Pr[X&gt;2]=0.5\cdot\Pr[|X|&gt;2]\)</span>. With this, can construct a new estimator</p>
<p><span class="math display">\[
p_2 = \frac{1}{2R}\sum_{i=1}^R \mathrm{I}(|F^{-1}(U_i)|&gt;2) .
\]</span></p>
<p>With one million simulations, we obtain an estimate of 0.14748 with standard error 0.228 (divided by 1000). One can prove that the variance of <span class="math inline">\(p_2\)</span> is of order <span class="math inline">\(0.052/R\)</span>.</p>
<!--It can be visualized below -->
<p>But one can go one step further. The improper integral can be written as a proper one by a simple symmetry property (since the function is symmetry and the integral on the real line is equal to <span class="math inline">\(1\)</span>) <span class="math display">\[
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\frac{1}{2}-\int_0^2\frac{dx}{\pi(1+x^2)} .
\]</span> From this expression, a natural approximation would be <span class="math display">\[
p_3 = \frac{1}{2}-\frac{1}{R}\sum_{i=1}^R h_3(2U_i), ~~~~~~\text{where}~h_3(x)=\frac{2}{\pi(1+x^2)} .
\]</span></p>
<p>With one million simulations, we obtain an estimate of 0.14756 with standard error 0.169 (divided by 1000). One can prove that the variance of <span class="math inline">\(p_3\)</span> is of order <span class="math inline">\(0.0285/R\)</span>.</p>
<!--It can be visualized below -->
<p>Finally, one can also consider some change of variable in the integral <span class="math display">\[
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\int_0^{1/2}\frac{y^{-2}dy}{\pi(1-y^{-2})} .
\]</span> From this expression, a natural approximation would be <span class="math display">\[
p_4 = \frac{1}{R}\sum_{i=1}^R h_4(U_i/2),~~~~~\text{where}~h_4(x)=\frac{1}{2\pi(1+x^2)} .
\]</span> The expression seems rather similar to the previous one,</p>
<p>With one million simulations, we obtain an estimate of 0.14759 with standard error 0.01 (divided by 1000). One can prove that the variance of <span class="math inline">\(p_4\)</span> is of order <span class="math inline">\(0.00009/R\)</span>, which is much smaller than what we had so far !</p>
<!--It can be visualized below -->
<p><a href="#tab:61">Table 6.1</a> summarizes the four choices of <span class="math inline">\(h(\cdot)\)</span> and <span class="math inline">\(F(\cdot)\)</span> to approximate <span class="math inline">\(\Pr[X&gt;2] =\)</span> 0.14758. The standard error varies dramatically. Thus, if we have a desired degree of accuracy, then the <em>number of simulations</em> depends strongly on how we write the integrals we try to approximate.</p>
<p><a id=tab:61></a></p>
<div align="center">
<table class="gmisc_table" style="border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;">
<thead>
<tr>
<td colspan="5" style="text-align: left;">
Table 6.1. Summary of Four Choices to Approximate <span class="math inline">\(\Pr[X&gt;2]\)</span>.
</td>
</tr>
<tr>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Estimator
</th>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Definition
</th>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Support Function
</th>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Estimate
</th>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Standard Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">
<span class="math inline">\(p_1\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(\frac{1}{R}\sum_{i=1}^R \mathrm{I}(F^{-1}(U_i)&gt;2)\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(~~~~~~F^{-1}(u)=\tan \left( \pi(u-0.5) \right)~~~~~~~\)</span>
</td>
<td style="text-align: right;">
0.147439
</td>
<td style="text-align: right;">
0.000355
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">\(p_2\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(\frac{1}{2R}\sum_{i=1}^R \mathrm{I}(|F^{-1}(U_i)|&gt;2)\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(F^{-1}(u)=\tan \left( \pi(u-0.5) \right)\)</span>
</td>
<td style="text-align: right;">
0.147477
</td>
<td style="text-align: right;">
0.000228
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">\(p_3\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(\frac{1}{2}-\frac{1}{R}\sum_{i=1}^R h_3(2U_i)\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(h_3(x)=\frac{2}{\pi(1+x^2)}\)</span>
</td>
<td style="text-align: right;">
0.147558
</td>
<td style="text-align: right;">
0.000169
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid grey; text-align: center;">
<span class="math inline">\(p_4\)</span>
</td>
<td style="border-bottom: 2px solid grey; text-align: center;">
<span class="math inline">\(\frac{1}{R}\sum_{i=1}^R h_4(U_i/2)\)</span>
</td>
<td style="border-bottom: 2px solid grey; text-align: center;">
<span class="math inline">\(h_4(x)=\frac{1}{2\pi(1+x^2)}\)</span>
</td>
<td style="border-bottom: 2px solid grey; text-align: right;">
0.147587
</td>
<td style="border-bottom: 2px solid grey; text-align: right;">
0.000010
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="S:SimulationStatInference" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Simulation and Statistical Inference</h3>
<p>Simulations not only help us approximate expected values but are also useful in calculating other aspects of distribution functions. In particular, they are very useful when distributions of test statistics are too complicated to derive; in this case, one can use simulations to approximate the reference distribution. We now illustrate this with the <a href="#" class="tooltip" style="color:green"><em>Kolmogorov-Smirnov test</em><span style="font-size:8pt">A nonparametric statistical test used to determine if a data sample could come from a hypothesized continuous probability distribution</span></a> that we learned about in Section 4.1.2.2.</p>
<p><strong>Example. 6.1.10. Kolmogorov-Smirnov Test of Distribution.</strong> Suppose that we have available <span class="math inline">\(n=100\)</span> observations <span class="math inline">\(\{x_1,\cdots,x_n\}\)</span> that, unknown to the analyst, was generated from a gamma distribution with parameters <span class="math inline">\(\alpha = 6\)</span> and <span class="math inline">\(\theta=2\)</span>. The analyst believes that the data come from a lognormal distribution with parameters 1 and 0.4 and would like to test this assumption.</p>
<p>The first step is to visualize the data.</p>
<h5 style="text-align: center;">
<a id="displayCode.KSTest.1.10.1" href="javascript:togglecode('toggleCode.KSTest.1.10.1','displayCode.KSTest.1.10.1');"><i><strong>Show R Code To Set up The Visualization</strong></i></a>
</h5>
<div id="toggleCode.KSTest.1.10.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
x &lt;-<span class="st"> </span><span class="kw">rgamma</span>(n, <span class="dv">6</span>, <span class="dv">2</span>)

u=<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">7</span>,<span class="dt">by=</span>.<span class="dv">01</span>)
vx =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">sort</span>(x))
vy =<span class="st"> </span>(<span class="dv">0</span><span class="op">:</span>n)<span class="op">/</span>n</code></pre></div>
</div>
<p>With this set-up, Figure <a href="C-Simulation.html#fig:KSTestData">6.5</a> provides a graph of a histogram and empirical distribution. For reference, superimposed are red dashed lines from the lognormal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(x,<span class="dt">probability =</span> <span class="ot">TRUE</span>,<span class="dt">main=</span><span class="st">&quot;Histogram&quot;</span>, <span class="dt">col=</span><span class="st">&quot;light blue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">7</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">4</span>))
<span class="kw">lines</span>(u,<span class="kw">dlnorm</span>(u,<span class="dv">1</span>,.<span class="dv">4</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(vx,vy,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;x&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Cumulative Distribution&quot;</span>,<span class="dt">main=</span><span class="st">&quot;Empirical cdf&quot;</span>)
<span class="kw">lines</span>(u,<span class="kw">plnorm</span>(u,<span class="dv">1</span>,.<span class="dv">4</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:KSTestData"></span>
<img src="LossDataAnalytics_files/figure-html/KSTestData-1.png" alt="Histogram and Empirical Distribution Function of Data used in Kolmogorov-Smirnov Test. The red dashed lines are fits based on (incorrectly) hypothesized lognormal distribution." width="672" />
<p class="caption">
Figure 6.5: Histogram and Empirical Distribution Function of Data used in Kolmogorov-Smirnov Test. The red dashed lines are fits based on (incorrectly) hypothesized lognormal distribution.
</p>
</div>
<p>Recall that the Kolmogorov-Smirnov statistic equals the largest discrepancy between the empirical and the hypothesized distribution. This is <span class="math inline">\(\max_x |F_n(x)-F_0(x)|\)</span>, where <span class="math inline">\(F_0\)</span> is the hypothesized lognormal distribution. We can calculate this directly as:</p>
<h5 style="text-align: center;">
<a id="displayCode.KSTest.1.10.2" href="javascript:togglecode('toggleCode.KSTest.1.10.2','displayCode.KSTest.1.10.2');"><i><strong>Show R Code for a Direct Calculation of the KS Statistic</strong></i></a>
</h5>
<div id="toggleCode.KSTest.1.10.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test statistic</span>
D &lt;-<span class="st"> </span><span class="cf">function</span>(data, F0){
   F &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(<span class="cf">function</span>(x) <span class="kw">mean</span>((data<span class="op">&lt;=</span>x)))
   n &lt;-<span class="st"> </span><span class="kw">length</span>(data)
   x &lt;-<span class="st"> </span><span class="kw">sort</span>(data)
   d1=<span class="kw">abs</span>(<span class="kw">F</span>(x<span class="op">+</span><span class="fl">1e-6</span>)<span class="op">-</span><span class="kw">F0</span>(x<span class="op">+</span><span class="fl">1e-6</span>))
   d2=<span class="kw">abs</span>(<span class="kw">F</span>(x<span class="op">-</span><span class="fl">1e-6</span>)<span class="op">-</span><span class="kw">F0</span>(x<span class="op">-</span><span class="fl">1e-6</span>))
   <span class="kw">return</span>(<span class="kw">max</span>(<span class="kw">c</span>(d1,d2)))
}
<span class="kw">D</span>(x,<span class="cf">function</span>(x) <span class="kw">plnorm</span>(x,<span class="dv">1</span>,.<span class="dv">4</span>))</code></pre></div>
<pre><code>[1] 0.09703627</code></pre>
</div>
<p>Fortunately, for the lognormal distribution, <code>R</code> has built-in tests that allow us to determine this without complex programming:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ks.test</span>(x, plnorm, <span class="dt">mean=</span><span class="dv">1</span>, <span class="dt">sd=</span><span class="fl">0.4</span>)</code></pre></div>
<pre><code>
    One-sample Kolmogorov-Smirnov test

data:  x
D = 0.097037, p-value = 0.3031
alternative hypothesis: two-sided</code></pre>
<p>However, for many distributions of actuarial interest, pre-built programs are not available. We can use simulation to test the relevance of the test statistic. Specifically, to compute the <span class="math inline">\(p\)</span>-value, let us generate thousands of random samples from a <span class="math inline">\(LN(1,0.4)\)</span> distribution (with the same size), and compute empirically the distribution of the statistic,</p>
<h5 style="text-align: center;">
<a id="displayCode.KSTest.1.10.3" href="javascript:togglecode('toggleCode.KSTest.1.10.3','displayCode.KSTest.1.10.3');"><i><strong>Show R Code for the Simulation Distribution of the KS Statistic</strong></i></a>
</h5>
<div id="toggleCode.KSTest.1.10.3" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ns &lt;-<span class="st"> </span><span class="fl">1e4</span>
d_KS &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>,ns)
<span class="co"># compute the test statistics for a large (ns) number of simulated samples</span>
<span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>ns) d_KS[s] &lt;-<span class="st"> </span><span class="kw">D</span>(<span class="kw">rlnorm</span>(n,<span class="dv">1</span>,.<span class="dv">4</span>),<span class="cf">function</span>(x) <span class="kw">plnorm</span>(x,<span class="dv">1</span>,.<span class="dv">4</span>))

<span class="kw">mean</span>(d_KS<span class="op">&gt;</span><span class="kw">D</span>(x,<span class="cf">function</span>(x) <span class="kw">plnorm</span>(x,<span class="dv">1</span>,.<span class="dv">4</span>)))</code></pre></div>
<pre><code>[1] 0.2843</code></pre>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(d_KS,<span class="dt">probability =</span> <span class="ot">TRUE</span>,<span class="dt">col=</span><span class="st">&quot;light blue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Test Statistic&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(d_KS),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">D</span>(x,<span class="cf">function</span>(x) <span class="kw">plnorm</span>(x,<span class="dv">1</span>,.<span class="dv">4</span>)),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:KSSimulatedDistribution"></span>
<img src="LossDataAnalytics_files/figure-html/KSSimulatedDistribution-1.png" alt="Simulated Distribution of the Kolmogorov-Smirnov Test Statistic. The vertical red dashed line marks the test statistic for the sample of 100." width="672" />
<p class="caption">
Figure 6.6: Simulated Distribution of the Kolmogorov-Smirnov Test Statistic. The vertical red dashed line marks the test statistic for the sample of 100.
</p>
</div>
<p>The simulated distribution based on 10,000 random samples is summarized in Figure <a href="C-Simulation.html#fig:KSSimulatedDistribution">6.6</a>. Here, the statistic exceeded the empirical value (0.09704) in 28.43% of the scenarios, while the <em>theoretical</em> <span class="math inline">\(p\)</span>-value is 0.3031. For both the simulation and the theoretical <span class="math inline">\(p\)</span>-values, the conclusions are the same; the data do not provide sufficient evidence to reject the hypothesis of a lognormal distribution.</p>
<p>Although only an approximation, the simulation approach works in a variety of distributions and test statistics without needing to develop the nuances of the underpinning theory for each situation. We summarize the procedure for developing simulated distributions and <em>p</em>-values as follows:</p>
<ol style="list-style-type: decimal">
<li>Draw a sample of size <em>n</em>, say, <span class="math inline">\(X_1, \ldots, X_n\)</span>, from a known distribution function <span class="math inline">\(F\)</span>. Compute a statistic of interest, denoted as <span class="math inline">\(\hat{\theta}(X_1, \ldots, X_n)\)</span>. Call this <span class="math inline">\(\hat{\theta}^r\)</span> for the <em>r</em>th replication.</li>
<li>Repeat this <span class="math inline">\(r=1, \ldots, R\)</span> times to get a sample of statistics, <span class="math inline">\(\hat{\theta}^1, \ldots,\hat{\theta}^R\)</span>.</li>
<li>From the sample of statistics in Step 2, <span class="math inline">\(\{\hat{\theta}^1, \ldots,\hat{\theta}^R\}\)</span>, compute a summary measure of interest, such as a <em>p</em>-value.</li>
</ol>
</div>
</div>
<div id="S:Bootstrap" class="section level2">
<h2><span class="header-section-number">6.2</span> Bootstrapping and Resampling</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Generate a nonparametric bootstrap distribution for a statistic of interest</li>
<li>Use the bootstrap distribution to generate estimates of precision for the statistic of interest, including bias, standard deviations, and confidence intervals</li>
<li>Perform bootstrap analyses for parametric distributions</li>
</ul>
<hr />
<!--  Bootstrap -->
<div id="bootstrap-foundations" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Bootstrap Foundations</h3>
<p>Simulation presented up to now is based on sampling from a <strong>known</strong> distribution. Section <a href="C-Simulation.html#S:SimulationFundamentals">6.1</a> showed how to use simulation techniques to sample and compute quantities from known distributions. However, statistical science is dedicated to providing inferences about distributions that are <em>unknown</em>. We gather summary statistics based on this unknown population distribution. But how do we sample from an unknown distribution?</p>
<p>Naturally, we cannot simulate draws from an unknown distribution but we can draw from a sample of observations. If the sample is a good representation from the population, then our simulated draws from the sample should well approximate the simulated draws from a population. The process of sampling from a sample is called <em>resampling</em> or <em>bootstrapping</em>. The term <a href="#" class="tooltip" style="color:green"><em>bootstrap</em><span style="font-size:8pt">A method of sampling with replacement from the original dataset to create additional simulated datasets of the same size as the original</span></a> comes from the phrase “pulling oneself up by one’s bootstraps” (Efron, 1979). With resampling, the original sample plays the role of the population and estimates from the sample play the role of true population parameters.</p>
<p>The resampling algorithm is the same as introduced in Section <a href="C-Simulation.html#S:SimulationStatInference">6.1.4</a> except that now we use simulated draws from a sample. It is common to use <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span> to denote the original sample and let <span class="math inline">\(\{X_1^*, \ldots, X_n^*\}\)</span> denote the simulated draws. We draw them with replacement so that the simulated draws will be independent from one another, the same assumption as with the original sample. For each sample, we also use <em>n</em> simulated draws, the same number as the original sample size. To distinguish this procedure from the simulation, it is common to use <em>B</em> (for bootstrap) to be the number of simulated samples. We could also write <span class="math inline">\(\{X_1^{(b)}, \ldots, X_n^{(b)}\}\)</span>, <span class="math inline">\(b=1,\ldots, B\)</span> to clarify this.</p>
<p>There are two basic resampling methods, <em>model-free</em> and <em>model-based</em>, which are, respectively, as <em>nonparametric</em> and <em>parametric</em>. In the <a href="#" class="tooltip" style="color:green"><em>nonparametric approach</em><span style="font-size:8pt">A statistical method where no assumption is made about the distribution of the population</span></a>, no assumption is made about the distribution of the parent population. The simulated draws come from the empirical distribution function <span class="math inline">\(F_n(\cdot)\)</span>, so each draw comes from <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span> with probability 1/<em>n</em>.</p>
<p>In contrast, for the <a href="#" class="tooltip" style="color:green"><em>parametric approach</em><span style="font-size:8pt">A statistical method where a prior assumption is made about the distribution or model form</span></a>, we assume that we have knowledge of the distribution family <em>F</em>. The original sample <span class="math inline">\(X_1, \ldots, X_n\)</span> is used to estimate parameters of that family, say, <span class="math inline">\(\hat{\theta}\)</span>. Then, simulated draws are taken from the <span class="math inline">\(F(\hat{\theta})\)</span>. Section <a href="C-Simulation.html#S:ParametricBootStrap">6.2.4</a> discusses this approach in further detail.</p>
<div id="nonparametric-bootstrap" class="section level4 unnumbered">
<h4>Nonparametric Bootstrap</h4>
<p>The idea of the nonparametric bootstrap is to use the inverse method on <span class="math inline">\(F_n\)</span>, the empirical cumulative distribution function, depicted in Figure <a href="C-Simulation.html#fig:InverseDFboot">6.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:InverseDFboot"></span>
<img src="LossDataAnalytics_files/figure-html/InverseDFboot-1.png" alt="Inverse of an Empirical Distribution Function" width="60%" />
<p class="caption">
Figure 6.7: Inverse of an Empirical Distribution Function
</p>
</div>
<p>Because <span class="math inline">\(F_n\)</span> is a step-function, <span class="math inline">\(F_n^{-1}\)</span> takes values in <span class="math inline">\(\{x_1,\cdots,x_n\}\)</span>. More precisely, as illustrated in Figure <a href="C-Simulation.html#fig:InverseDFboot2">6.8</a>.</p>
<ul>
<li>if <span class="math inline">\(y\in(0,1/n)\)</span> (with probability <span class="math inline">\(1/n\)</span>) we draw the smallest value (<span class="math inline">\(\min\{x_i\}\)</span>)</li>
<li>if <span class="math inline">\(y\in(1/n,2/n)\)</span> (with probability <span class="math inline">\(1/n\)</span>) we draw the second smallest value,</li>
<li>…</li>
<li>if <span class="math inline">\(y\in((n-1)/n,1)\)</span> (with probability <span class="math inline">\(1/n\)</span>) we draw the largest value (<span class="math inline">\(\max\{x_i\}\)</span>).</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:InverseDFboot2"></span>
<img src="LossDataAnalytics_files/figure-html/InverseDFboot2-1.png" alt="Inverse of an Empirical Distribution Function" width="60%" />
<p class="caption">
Figure 6.8: Inverse of an Empirical Distribution Function
</p>
</div>
<p>Using the inverse method with <span class="math inline">\(F_n\)</span> means sampling from <span class="math inline">\(\{x_1,\cdots,x_n\}\)</span>, with probability <span class="math inline">\(1/n\)</span>. Generating a bootstrap sample of size <span class="math inline">\(B\)</span> means sampling from <span class="math inline">\(\{x_1,\cdots,x_n\}\)</span>, with probability <span class="math inline">\(1/n\)</span>, with replacement. See the following illustrative <code>R</code> code.</p>
<h5 style="text-align: center;">
<a id="displayCode.BootStrap.A" href="javascript:togglecode('toggleCode.BootStrap.A','displayCode.BootStrap.A');"><i><strong>Show R Code For Creating a Bootstrap Sample</strong></i></a>
</h5>
<div id="toggleCode.BootStrap.A" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
n &lt;-<span class="st"> </span><span class="dv">10</span>
x &lt;-<span class="st"> </span><span class="kw">rexp</span>(n, <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)
m &lt;-<span class="st"> </span><span class="dv">8</span>
bootvalues &lt;-<span class="st"> </span><span class="kw">sample</span>(x, <span class="dt">size=</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>)</code></pre></div>
</div>
<pre><code>[1] 7.0899 0.8742 0.8388 4.5311 0.8388 5.7394 0.8388 2.6164</code></pre>
<p>Observe that value 0.8388 was obtained three times.</p>
</div>
</div>
<div id="bootstrap-precision-bias-standard-deviation-and-mse" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Bootstrap Precision: Bias, Standard Deviation, and MSE</h3>
<p>We summarize the nonparametric bootstrap procedure as follows:</p>
<ol style="list-style-type: decimal">
<li>From the sample <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span>, draw a sample of size <em>n</em> (with replacement), say, <span class="math inline">\(X_1^*, \ldots, X_n^*\)</span>. From the simulated draws compute a statistic of interest, denoted as <span class="math inline">\(\hat{\theta}(X_1^*, \ldots, X_n^*)\)</span>. Call this <span class="math inline">\(\hat{\theta}_b^*\)</span> for the <em>b</em>th replicate.</li>
<li>Repeat this <span class="math inline">\(b=1, \ldots, B\)</span> times to get a sample of statistics, <span class="math inline">\(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\)</span>.</li>
<li>From the sample of statistics in Step 2, <span class="math inline">\(\{\hat{\theta}_1^*, \ldots, \hat{\theta}_B^*\}\)</span>, compute a summary measure of interest.</li>
</ol>
<p>In this section, we focus on three summary measures, the <a href="#" class="tooltip" style="color:green"><em>bias</em><span style="font-size:8pt">The difference between the expected value of an estimator and the parameter being estimated. bias is an estimation error that does not become smaller as one observes larger sample sizes.</span></a>, the standard deviation, and the mean square error (<em>MSE</em>). <a href="#tab:62">Table 6.2</a> summarizes these three measures. Here, <span class="math inline">\(\overline{\hat{\theta^*}}\)</span> is the average of <span class="math inline">\(\{\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\}\)</span>.</p>
<p><a id=tab:62></a></p>
<p><span class="math display">\[
{\small
\begin{matrix}
\text{Table 6.2. Bootstrap Summary Measures}\\
\begin{array}{l|c|c|c}
\hline
\text{Population Measure}&amp; \text{Population Definition}&amp;\text{Bootstrap Approximation}&amp;\text{Bootstrap Symbol}\\
\hline
\text{Bias} &amp; \mathrm{E}(\hat{\theta})-\theta&amp;\overline{\hat{\theta^*}}-\hat{\theta}&amp; Bias_{boot}(\hat{\theta})  \\\hline
\text{Standard Deviation} &amp;   \sqrt{\mathrm{Var}(\hat{\theta})}
&amp; \sqrt{\frac{1}{B-1} \sum_{b=1}^{B}\left(\hat{\theta}_b^* -\overline{\hat{\theta^*}} \right) ^2}&amp;s_{boot}(\hat{\theta})  \\\hline
\text{Mean Square Error} &amp;\mathrm{E}(\hat{\theta}-\theta)^2 &amp; \frac{1}{B} \sum_{b=1}^{B}\left(\hat{\theta}_b^* -\hat{\theta}
\right)^2&amp;MSE_{boot}(\hat{\theta})\\
\hline
\end{array}\end{matrix}
}
\]</span></p>
<hr />
<p><strong>Example 6.2.1. Bodily Injury Claims and Loss Elimination Ratios.</strong> To show how the bootstrap can be used to quantify the precision of estimators, we return to the Example 4.1.11 bodily injury claims data where we introduced a nonparametric estimator of the loss elimination ratio.</p>
<p><a href="#tab:63">Table 6.3</a> summarizes the results of the bootstrap estimation. For example, at <span class="math inline">\(d=14000\)</span>, we saw in Example 4.1.11 that the nonparametric estimate of <em>LER</em> is 0.97678. This has an estimated bias of 0.00018 with a standard deviation of 0.00701. For some applications, you may wish to apply the estimated bias to the original estimate to give a <a href="#" class="tooltip" style="color:green"><em>bias-corrected estimator</em><span style="font-size:8pt">If an estimator is known to be consistently biased in a manner, it can be corrected using a factor to be come less biased or unbiased</span></a>. This is the focus of the next example. For this illustration, the bias is small and so such a correction is not relevant.</p>
<h5 style="text-align: center;">
<a id="displayCode.LER6.2.1" href="javascript:togglecode('toggleCode.LER6.2.1','displayCode.LER6.2.1');"><i><strong>Show R Code For Bootstrap Estimates of LER</strong></i></a>
</h5>
<div id="toggleCode.LER6.2.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example from Derrig et al</span>
BIData &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;../../Data/DerrigResampling.csv&quot;</span>, <span class="dt">header =</span>T)
BIData<span class="op">$</span>Censored &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">*</span>(BIData<span class="op">$</span>AmountPaid <span class="op">&gt;=</span><span class="st"> </span>BIData<span class="op">$</span>PolicyLimit)
BIDataUncensored &lt;-<span class="st"> </span><span class="kw">subset</span>(BIData, Censored <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)
LER.boot &lt;-<span class="st"> </span><span class="cf">function</span>(ded, data, indices){
  resample.data &lt;-<span class="st"> </span>data[indices,]
  sumClaims &lt;-<span class="st"> </span><span class="kw">sum</span>(resample.data<span class="op">$</span>AmountPaid)
  sumClaims_d &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">pmin</span>(resample.data<span class="op">$</span>AmountPaid,ded))
  LER &lt;-<span class="st">   </span>sumClaims_d<span class="op">/</span>sumClaims
  <span class="kw">return</span>(LER)  
}

##Derrig et al
<span class="kw">set.seed</span>(<span class="dv">2019</span>)
dVec2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">4000</span>, <span class="dv">5000</span>, <span class="dv">10500</span>, <span class="dv">11500</span>, <span class="dv">14000</span>, <span class="dv">18500</span>)
OutBoot &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="kw">length</span>(dVec2),<span class="dv">6</span>)
<span class="kw">colnames</span>(OutBoot) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;d&quot;</span>,<span class="st">&quot;NP Estimate&quot;</span>,<span class="st">&quot;Bootstrap Bias&quot;</span>, <span class="st">&quot;Bootstrap SD&quot;</span>, 
                           <span class="st">&quot;Lower Normal 95% CI&quot;</span>, <span class="st">&quot;Upper Normal 95% CI&quot;</span>)
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(dVec2)) {
OutBoot[i,<span class="dv">1</span>] &lt;-<span class="st"> </span>dVec2[i]
results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>BIDataUncensored, <span class="dt">statistic=</span>LER.boot, <span class="dt">R=</span><span class="dv">1000</span>, <span class="dt">ded=</span>dVec2[i])
OutBoot[i,<span class="dv">2</span>] &lt;-<span class="st"> </span>results<span class="op">$</span>t0
biasboot &lt;-<span class="st"> </span><span class="kw">mean</span>(results<span class="op">$</span>t)<span class="op">-</span>results<span class="op">$</span>t0 -&gt;<span class="st"> </span>OutBoot[i,<span class="dv">3</span>]
sdboot &lt;-<span class="st"> </span><span class="kw">sd</span>(results<span class="op">$</span>t) -&gt;<span class="st"> </span>OutBoot[i,<span class="dv">4</span>]
temp &lt;-<span class="st"> </span><span class="kw">boot.ci</span>(results)
OutBoot[i,<span class="dv">5</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>normal[<span class="dv">2</span>]
OutBoot[i,<span class="dv">6</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>normal[<span class="dv">3</span>]
}</code></pre></div>
</div>
<p><a id=tab:63></a></p>
<div id="table-6.3.-bootstrap-estimates-of-ler-at-selected-deductibles" class="section level5 unnumbered">
<h5>Table 6.3. Bootstrap Estimates of LER at Selected Deductibles</h5>
<table>
<thead>
<tr class="header">
<th align="right">d</th>
<th align="right">NP Estimate</th>
<th align="right">Bootstrap Bias</th>
<th align="right">Bootstrap SD</th>
<th align="right">Lower Normal 95% CI</th>
<th align="right">Upper Normal 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4000</td>
<td align="right">0.54113</td>
<td align="right">-0.00012</td>
<td align="right">0.01228</td>
<td align="right">0.51719</td>
<td align="right">0.56531</td>
</tr>
<tr class="even">
<td align="right">5000</td>
<td align="right">0.64960</td>
<td align="right">-0.00038</td>
<td align="right">0.01400</td>
<td align="right">0.62254</td>
<td align="right">0.67741</td>
</tr>
<tr class="odd">
<td align="right">10500</td>
<td align="right">0.93563</td>
<td align="right">0.00018</td>
<td align="right">0.01065</td>
<td align="right">0.91458</td>
<td align="right">0.95632</td>
</tr>
<tr class="even">
<td align="right">11500</td>
<td align="right">0.95281</td>
<td align="right">-0.00016</td>
<td align="right">0.00918</td>
<td align="right">0.93497</td>
<td align="right">0.97097</td>
</tr>
<tr class="odd">
<td align="right">14000</td>
<td align="right">0.97678</td>
<td align="right">0.00018</td>
<td align="right">0.00701</td>
<td align="right">0.96286</td>
<td align="right">0.99035</td>
</tr>
<tr class="even">
<td align="right">18500</td>
<td align="right">0.99382</td>
<td align="right">0.00006</td>
<td align="right">0.00342</td>
<td align="right">0.98706</td>
<td align="right">1.00047</td>
</tr>
</tbody>
</table>
<p>The bootstrap standard deviation gives a measure of precision. For one application of standard deviations, we can use the normal approximation to create a confidence interval. For example, the <code>R</code> function <code>boot.ci</code> produces the normal confidence intervals at 95%. These are produced by creating an interval of twice the length of 1.95994 bootstrap standard deviations, centered about the bias-corrected estimator (1.95994 is the 97.5th quantile of the standard normal distribution). For example, the lower normal 95% CI at <span class="math inline">\(d=14000\)</span> is <span class="math inline">\((0.97678-0.00018)- 1.95994*0.00701\)</span> <span class="math inline">\(= 0.96286\)</span>. We further discuss bootstrap confidence intervals in the next section.</p>
<hr />
<p><strong>Example 6.2.2. Estimating <span class="math inline">\(\exp(\mu)\)</span>.</strong> The bootstrap can be used to quantify the bias of an estimator, for instance. Consider here a sample <span class="math inline">\(\mathbf{x}=\{x_1,\cdots,x_n\}\)</span> that is <a href="#" class="tooltip" style="color:green"><em>iid</em><span style="font-size:8pt">Independent and identically distributed</span></a> with mean <span class="math inline">\(\mu\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample_x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">2.46</span>,<span class="fl">2.80</span>,<span class="fl">3.28</span>,<span class="fl">3.86</span>,<span class="fl">2.85</span>,<span class="fl">3.67</span>,<span class="fl">3.37</span>,<span class="fl">3.40</span>,<span class="fl">5.22</span>,<span class="fl">2.55</span>,
              <span class="fl">2.79</span>,<span class="fl">4.50</span>,<span class="fl">3.37</span>,<span class="fl">2.88</span>,<span class="fl">1.44</span>,<span class="fl">2.56</span>,<span class="fl">2.00</span>,<span class="fl">2.07</span>,<span class="fl">2.19</span>,<span class="fl">1.77</span>)</code></pre></div>
<p>Suppose that the quantity of interest is <span class="math inline">\(\theta=\exp(\mu)\)</span>. A natural estimator would be <span class="math inline">\(\widehat{\theta}_1=\exp(\overline{x})\)</span>. This estimator is biased (due to the <a href="#" class="tooltip" style="color:green"><em>Jensen inequality</em><span style="font-size:8pt">For a convex function f(x), f(expected value of x) &lt;= expected value of f(x)</span></a>) but is asymptotically unbiased. For our sample, the estimate is as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(theta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">mean</span>(sample_x)))</code></pre></div>
<pre><code>[1] 19.13463</code></pre>
<p>One can use the central limit theorem to get a correction using <span class="math display">\[
\overline{X}\approx\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right)\text{ where }\sigma^2=\text{Var}[X_i] ,
\]</span> so that, with the normal moment generating function, we have <span class="math display">\[
\mathrm{E}~\exp[\overline{X}] \approx \exp\left(\mu+\frac{\sigma^2}{2n}\right) .
\]</span> Hence, one can consider naturally <span class="math display">\[
\widehat{\theta}_2=\exp\left(\overline{x}-\frac{\widehat{\sigma}^2}{2n}\right) .
\]</span> For our data, this turns out to be as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">length</span>(sample_x)
(theta_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">mean</span>(sample_x)<span class="op">-</span><span class="kw">var</span>(sample_x)<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>n)))</code></pre></div>
<pre><code>[1] 18.73334</code></pre>
<p>As another strategy (that we do not pursue here), one can also use Taylor’s approximation to get a more accurate estimator (as in the delta method), <span class="math display">\[
g(\overline{x})=g(\mu)+(\overline{x}-\mu)g&#39;(\mu)+(\overline{x}-\mu)^2\frac{g&#39;&#39;(\mu)}{2}+\cdots
\]</span> The alternative we do explore is to use a bootstrap strategy: given a bootstrap sample, <span class="math inline">\(\mathbf{x}^{\ast}_{b}\)</span>, let <span class="math inline">\(\overline{x}^{\ast}_{b}\)</span> denotes its mean, and set <span class="math display">\[
\widehat{\theta}_3=\frac{1}{B}\sum_{b=1}^B\exp(\overline{x}^{\ast}_{b}) .
\]</span> To implement this, we have the following code.</p>
<h5 style="text-align: center;">
<a id="displayCode.bootstrapdisn.1" href="javascript:togglecode('toggleCode.bootstrapdisn.1','displayCode.bootstrapdisn.1');"><i><strong>Show R Code for Creating Bootstrap Samples</strong></i></a>
</h5>
<div id="toggleCode.bootstrapdisn.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)
results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>sample_x, 
                <span class="dt">statistic=</span><span class="cf">function</span>(y,indices) <span class="kw">exp</span>(<span class="kw">mean</span>(y[indices])), 
                <span class="dt">R=</span><span class="dv">1000</span>)
theta_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(results<span class="op">$</span>t)</code></pre></div>
</div>
<p>Then, you can <code>plot(results)</code> and <code>print(results)</code> to see the following.</p>
<div class="figure" style="text-align: center"><span id="fig:BootstrapDistn"></span>
<img src="LossDataAnalytics_files/figure-html/BootstrapDistn-1.png" alt="Distribution of Bootstrap Replicates. The left-hand panel is a histogram of replicates. The right-hand panel is a quantile-quantile plot, comparing the bootstrap distribution to the standard normal distribution." width="672" />
<p class="caption">
Figure 6.9: Distribution of Bootstrap Replicates. The left-hand panel is a histogram of replicates. The right-hand panel is a quantile-quantile plot, comparing the bootstrap distribution to the standard normal distribution.
</p>
</div>
<pre><code>
ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = sample_x, statistic = function(y, indices) exp(mean(y[indices])), 
    R = 1000)


Bootstrap Statistics :
    original    bias    std. error
t1* 19.13463 0.5589631    4.021793</code></pre>
<p>This results in three estimators, the raw estimator <span class="math inline">\(\widehat{\theta}_1=\)</span> 19.135, the second-order correction <span class="math inline">\(\widehat{\theta}_2=\)</span> 18.733, and the bootstrap estimator <span class="math inline">\(\widehat{\theta}_3=\)</span> 19.694.</p>
<p>How does this work with differing sample sizes? We now suppose that the <span class="math inline">\(x_i\)</span>’s are generated from a lognormal distribution <span class="math inline">\(LN(0,1)\)</span>, so that <span class="math inline">\(\mu = \exp(0 + 1/2) = 1.648721\)</span> and <span class="math inline">\(\theta = \exp(1.648721)\)</span> <span class="math inline">\(= 5.200326\)</span>. We use simulation to draw the sample sizes but then act as if they were a realized set of observations. See the following illustrative code.</p>
<h5 style="text-align: center;">
<a id="displayCode.bootstrapdisn.2" href="javascript:togglecode('toggleCode.bootstrapdisn.2','displayCode.bootstrapdisn.2');"><i><strong>Show R Code for Creating Bootstrap Samples</strong></i></a>
</h5>
<div id="toggleCode.bootstrapdisn.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">param &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  n &lt;-<span class="st"> </span><span class="kw">length</span>(x)
  theta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">mean</span>(x))
  theta_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">mean</span>(x)<span class="op">-</span><span class="kw">var</span>(x)<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>n))
  results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>x, 
                <span class="dt">statistic=</span><span class="cf">function</span>(y,indices) <span class="kw">exp</span>(<span class="kw">mean</span>(y[indices])), 
                <span class="dt">R=</span><span class="dv">999</span>)
  theta_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(results<span class="op">$</span>t)
  <span class="kw">return</span>(<span class="kw">c</span>(theta_<span class="dv">1</span>,theta_<span class="dv">2</span>,theta_<span class="dv">3</span>))
}
<span class="kw">set.seed</span>(<span class="dv">2074</span>)
ns&lt;-<span class="st"> </span><span class="dv">200</span>
est &lt;-<span class="st"> </span><span class="cf">function</span>(n){
call_param &lt;-<span class="st"> </span><span class="cf">function</span>(i) <span class="kw">param</span>(<span class="kw">rlnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>))
V &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(call_param)(<span class="dv">1</span><span class="op">:</span>ns)
<span class="kw">apply</span>(V,<span class="dv">1</span>,median)
}
VN=<span class="kw">seq</span>(<span class="dv">15</span>,<span class="dv">100</span>,<span class="dt">by=</span><span class="dv">5</span>)
Est &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(est)(VN)</code></pre></div>
</div>
<p>The results of the comparison are summarized in Figure <a href="C-Simulation.html#fig:BootstrapCompare">6.10</a>. This figure shows that the bootstrap estimator is closer to the true parameter value for almost all sample sizes. The bias of all three estimators decreases as the sample size increases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">matplot</span>(VN,<span class="kw">t</span>(Est),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">lty=</span><span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">ylim=</span><span class="kw">exp</span>(<span class="kw">exp</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>))<span class="op">+</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
        <span class="dt">xlab=</span><span class="st">&quot;sample size (n)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;estimator&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">exp</span>(<span class="kw">exp</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)),<span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="dv">1</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;raw estimator&quot;</span>, <span class="st">&quot;second order correction&quot;</span>, <span class="st">&quot;bootstrap&quot;</span>),
       <span class="dt">col=</span><span class="dv">2</span><span class="op">:</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:BootstrapCompare"></span>
<img src="LossDataAnalytics_files/figure-html/BootstrapCompare-1.png" alt="Comparision of Estimates. True value of the parameter is given by the solid horizontal line at 5.20." width="672" />
<p class="caption">
Figure 6.10: Comparision of Estimates. True value of the parameter is given by the solid horizontal line at 5.20.
</p>
</div>
</div>
</div>
<div id="confidence-intervals" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Confidence Intervals</h3>
<p>The bootstrap procedure generates <em>B</em> replicates <span class="math inline">\(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\)</span> of the estimator <span class="math inline">\(\hat{\theta}\)</span>. In <em>Example 6.2.1,</em> we saw how to use standard normal approximations to create a confidence interval for parameters of interest. However, given that a major point is to use bootstrapping to avoid relying on assumptions of approximate normality, it is not surprising that there are alternative confidence intervals available.</p>
<p>For an estimator <span class="math inline">\(\hat{\theta}\)</span>, the <em>basic</em> bootstrap confidence interval is</p>
<span class="math display" id="eq:basicBootCI">\[\begin{equation} 
  \left(2 \hat{\theta} - q_U, 2 \hat{\theta} - q_L \right) ,
  \tag{6.2}
\end{equation}\]</span>
<p>where <span class="math inline">\(q_L\)</span> and <span class="math inline">\(q_U\)</span> are lower and upper 2.5% quantiles from the bootstrap sample <span class="math inline">\(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\)</span>.</p>
<p>To see where this comes from, start with the idea that <span class="math inline">\((q_L, q_U)\)</span> provides a 95% interval for <span class="math inline">\(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\)</span>. So, for a random <span class="math inline">\(\hat{\theta}_b^*\)</span>, there is a 95% chance that <span class="math inline">\(q_L \le \hat{\theta}_b^* \le q_U\)</span>. Reversing the inequalities and adding <span class="math inline">\(\hat{\theta}\)</span> to each side gives a 95% interval</p>
<p><span class="math display">\[
\hat{\theta} -q_U \le \hat{\theta} - \hat{\theta}_b^* \le  \hat{\theta} -q_L .
\]</span> So, <span class="math inline">\(\left( \hat{\theta}-q_U, \hat{\theta} -q_L\right)\)</span> is an 95% interval for <span class="math inline">\(\hat{\theta} - \hat{\theta}_b^*\)</span>. The bootstrap approximation idea says that this is also a 95% interval for <span class="math inline">\(\theta - \hat{\theta}\)</span>. Adding <span class="math inline">\(\hat{\theta}\)</span> to each side gives the 95% interval in equation <a href="C-Simulation.html#eq:basicBootCI">(6.2)</a>.</p>
<p>Many alternative bootstrap intervals are available. The easiest to explain is the <a href="#" class="tooltip" style="color:green"><em>percentile bootstrap interval</em><span style="font-size:8pt">Confidence interval for the parameter estimates determined using the actual percentile results from the bootstrap sampling approach, as every bootstrap sample has an associated parameter estimate(s) that can be ranked against the others</span></a> which is defined as <span class="math inline">\(\left(q_L, q_U\right)\)</span>. However, this has the drawback of potentially poor behavior in the tails which can be of concern in some actuarial problems of interest.</p>
<p><strong>Example 6.2.3. Bodily Injury Claims and Risk Measures.</strong> To see how the bootstrap confidence intervals work, we return to the bodily injury auto claims considered in Example 6.2.1. Instead of the loss elimination ratio, suppose we wish to estimate the 95th percentile <span class="math inline">\(F^{-1}(0.95)\)</span> and a measure defined as <span class="math display">\[
TVaR_{0.95)}[X] = \mathrm{E}[X | X &gt; F^{-1}(0.95)] .
\]</span> This measure is called the <a href="#" class="tooltip" style="color:green"><em>tail value-at-risk</em><span style="font-size:8pt">The expected value of a risk given that the risk exceeds a value-at-risk</span></a>; it is the expected value of <span class="math inline">\(X\)</span> conditional on <span class="math inline">\(X\)</span> exceeding the 95th percentile. Section 10.2 explains how quantiles and the tail value-at-risk are the two most important examples of so-called <em>risk measures</em>. For now, we will simply think of these as measures that we wish to estimate. For the percentile, we use the nonparametric estimator <span class="math inline">\(F^{-1}_n(0.95)\)</span> defined in Section 4.1.1.3. For the tail value-at-risk, we use the plug-in principle to define the nonparametric estimator</p>
<p><span class="math display">\[
TVaR_{n,0.95}[X] = \frac{\sum_{i=1}^n X_i I(X_i &gt; F^{-1}_n(0.95))}{\sum_{i=1}^n I(X_i &gt; F^{-1}_n(0.95))} ~.
\]</span> In this expression, the denominator counts the number of observations that exceed the 95th percentile <span class="math inline">\(F^{-1}_n(0.95)\)</span>. The numerator adds up losses for those observations that exceed <span class="math inline">\(F^{-1}_n(0.95)\)</span>. <a href="#tab:64">Table 6.4</a> summarizes the estimator for selected fractions.</p>
<h5 style="text-align: center;">
<a id="displayCode.bootstrapquantiles.1" href="javascript:togglecode('toggleCode.bootstrapquantiles.1','displayCode.bootstrapquantiles.1');"><i><strong>Show R Code for Creating Quantile Bootstrap Samples</strong></i></a>
</h5>
<div id="toggleCode.bootstrapquantiles.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example from Derrig et al</span>
<span class="co">#BIData &lt;- read.csv(&quot;./Data/DerrigResampling.csv&quot;, header =T)</span>
BIData<span class="op">$</span>Censored &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">*</span>(BIData<span class="op">$</span>AmountPaid <span class="op">&gt;=</span><span class="st"> </span>BIData<span class="op">$</span>PolicyLimit)
BIDataUncensored &lt;-<span class="st"> </span><span class="kw">subset</span>(BIData, Censored <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)

<span class="kw">set.seed</span>(<span class="dv">2017</span>)
PercentVec &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.80</span>, <span class="fl">0.90</span>, <span class="fl">0.95</span>, <span class="fl">0.98</span>)
OutBoot1 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">10</span>)
<span class="kw">colnames</span>(OutBoot1) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Fraction&quot;</span>,<span class="st">&quot;NP Estimate&quot;</span>, <span class="st">&quot;Bootstrap Bias&quot;</span>, <span class="st">&quot;Bootstrap SD&quot;</span>, 
                        <span class="st">&quot;Lower Normal 95% CI&quot;</span>, <span class="st">&quot;Upper Normal  95% CI&quot;</span>,
                        <span class="st">&quot;Lower Basic 95% CI&quot;</span>, <span class="st">&quot;Upper Basic 95% CI&quot;</span>,
                        <span class="st">&quot;Lower Percentile 95% CI&quot;</span>, <span class="st">&quot;Upper  Percentile 95% CI&quot;</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(PercentVec)) {
OutBoot1[i,<span class="dv">1</span>] &lt;-<span class="st"> </span>PercentVec[i]
results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>BIDataUncensored<span class="op">$</span>AmountPaid,
                <span class="dt">statistic=</span><span class="cf">function</span>(X,indices)
                    <span class="kw">quantile</span>(X[indices],PercentVec[i]),
                 <span class="dt">R=</span><span class="dv">1000</span>)
<span class="cf">if</span> (i<span class="op">==</span><span class="dv">1</span>){bootreal &lt;-<span class="st"> </span>results<span class="op">$</span>t}
OutBoot1[i,<span class="dv">2</span>] &lt;-<span class="st"> </span>results<span class="op">$</span>t0
OutBoot1[i,<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">mean</span>(results<span class="op">$</span>t)<span class="op">-</span>results<span class="op">$</span>t0 
OutBoot1[i,<span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">sd</span>(results<span class="op">$</span>t) 
temp &lt;-<span class="st"> </span><span class="kw">boot.ci</span>(results, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;norm&quot;</span>, <span class="st">&quot;basic&quot;</span>, <span class="st">&quot;perc&quot;</span>))
OutBoot1[i,<span class="dv">5</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>normal[<span class="dv">2</span>]
OutBoot1[i,<span class="dv">6</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>normal[<span class="dv">3</span>]
OutBoot1[i,<span class="dv">7</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>basic[<span class="dv">4</span>]
OutBoot1[i,<span class="dv">8</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>basic[<span class="dv">5</span>]
OutBoot1[i,<span class="dv">9</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>percent[<span class="dv">4</span>]
OutBoot1[i,<span class="dv">10</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>percent[<span class="dv">5</span>]
}</code></pre></div>
</div>
<p><a id=tab:64></a></p>
<div id="table-6.4.-bootstrap-estimates-of-quantiles-at-selected-fractions" class="section level5 unnumbered">
<h5>Table 6.4. Bootstrap Estimates of Quantiles at Selected Fractions</h5>
<table>
<thead>
<tr class="header">
<th align="right">Fraction</th>
<th align="right">NP Estimate</th>
<th align="right">Bootstrap Bias</th>
<th align="right">Bootstrap SD</th>
<th align="right">Lower Normal 95% CI</th>
<th align="right">Upper Normal 95% CI</th>
<th align="right">Lower Basic 95% CI</th>
<th align="right">Upper Basic 95% CI</th>
<th align="right">Lower Percentile 95% CI</th>
<th align="right">Upper Percentile 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">6500.00</td>
<td align="right">-126.66</td>
<td align="right">205.29</td>
<td align="right">6224.30</td>
<td align="right">7029.01</td>
<td align="right">6297.00</td>
<td align="right">7000.00</td>
<td align="right">6000.00</td>
<td align="right">6703.00</td>
</tr>
<tr class="even">
<td align="right">0.80</td>
<td align="right">9078.40</td>
<td align="right">92.14</td>
<td align="right">205.90</td>
<td align="right">8582.70</td>
<td align="right">9389.82</td>
<td align="right">8493.54</td>
<td align="right">9168.80</td>
<td align="right">8988.00</td>
<td align="right">9663.26</td>
</tr>
<tr class="odd">
<td align="right">0.90</td>
<td align="right">11454.00</td>
<td align="right">38.17</td>
<td align="right">462.87</td>
<td align="right">10508.62</td>
<td align="right">12323.04</td>
<td align="right">10535.02</td>
<td align="right">12373.00</td>
<td align="right">10535.00</td>
<td align="right">12372.98</td>
</tr>
<tr class="even">
<td align="right">0.95</td>
<td align="right">13313.40</td>
<td align="right">47.27</td>
<td align="right">721.87</td>
<td align="right">11851.29</td>
<td align="right">14680.96</td>
<td align="right">11126.80</td>
<td align="right">14352.14</td>
<td align="right">12274.66</td>
<td align="right">15500.00</td>
</tr>
<tr class="odd">
<td align="right">0.98</td>
<td align="right">16758.72</td>
<td align="right">58.44</td>
<td align="right">1272.69</td>
<td align="right">14205.86</td>
<td align="right">19194.70</td>
<td align="right">14517.44</td>
<td align="right">19207.79</td>
<td align="right">14309.65</td>
<td align="right">19000.00</td>
</tr>
</tbody>
</table>
<p>For example, when the fraction is 0.50, we see that lower and upper 2.5th quantiles of the bootstrap simulations are <span class="math inline">\(q_L=\)</span> 6000 and <span class="math inline">\(q_u=\)</span> 6703, respectively. These form the percentile bootstrap confidence interval. With the nonparametric estimator 6500, these yield the lower and upper bounds of the basic confidence interval 6297 and 7000, respectively. <a href="#tab:64">Table 6.4</a> also shows bootstrap estimates of the bias, standard deviation, and a normal confidence interval, concepts introduced in the prior section.</p>
<p><a href="#tab:65">Table 6.5</a> shows similar calculations for the tail value-at-risk. In each case, we see that the bootstrap standard deviation increases as the fraction increases. This is a reflection of the fewer observations available to estimate quantiles as the fraction increases, hence greater imprecision. Width of confidence intervals also increase. Interestingly, there does not seem to be the same pattern in the estimates of the bias.</p>
<h5 style="text-align: center;">
<a id="displayCode.bootstrapquantiles.2" href="javascript:togglecode('toggleCode.bootstrapquantiles.2','displayCode.bootstrapquantiles.2');"><i><strong>Show R Code for Creating TVar Bootstrap Samples</strong></i></a>
</h5>
<div id="toggleCode.bootstrapquantiles.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CTE.boot &lt;-<span class="st"> </span><span class="cf">function</span>(data, indices, RiskLevel){
  resample.data &lt;-<span class="st"> </span>data[indices,]
  X &lt;-<span class="st"> </span>resample.data<span class="op">$</span>AmountPaid
  cutoff &lt;-<span class="st"> </span><span class="kw">quantile</span>(X, RiskLevel)
  CTE &lt;-<span class="st"> </span><span class="kw">sum</span>(X<span class="op">*</span>(X <span class="op">&gt;</span><span class="st"> </span>cutoff))<span class="op">/</span><span class="kw">sum</span>(X <span class="op">&gt;</span><span class="st"> </span>cutoff)
  <span class="kw">return</span>(CTE) 
}

<span class="kw">set.seed</span>(<span class="dv">2017</span>)  
PercentVec &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.80</span>, <span class="fl">0.90</span>, <span class="fl">0.95</span>, <span class="fl">0.98</span>)
OutBoot1 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">10</span>)
<span class="kw">colnames</span>(OutBoot1) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Fraction&quot;</span>,<span class="st">&quot;NP Estimate&quot;</span>, <span class="st">&quot;Bootstrap Bias&quot;</span>, 
       <span class="st">&quot;Bootstrap SD&quot;</span>, <span class="st">&quot;Lower Normal 95% CI&quot;</span>, <span class="st">&quot;Upper Normal  95% CI&quot;</span>,
       <span class="st">&quot;Lower Basic 95% CI&quot;</span>, <span class="st">&quot;Upper Basic 95% CI&quot;</span>,
       <span class="st">&quot;Lower Percentile 95% CI&quot;</span>, <span class="st">&quot;Upper  Percentile 95% CI&quot;</span>)
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(PercentVec)) {
OutBoot1[i,<span class="dv">1</span>] &lt;-<span class="st"> </span>PercentVec[i]
results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>BIDataUncensored, <span class="dt">statistic=</span>CTE.boot, <span class="dt">R=</span><span class="dv">1000</span>, <span class="dt">RiskLevel=</span>PercentVec[i])
OutBoot1[i,<span class="dv">2</span>] &lt;-<span class="st"> </span>results<span class="op">$</span>t0
OutBoot1[i,<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">mean</span>(results<span class="op">$</span>t)<span class="op">-</span>results<span class="op">$</span>t0 
OutBoot1[i,<span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">sd</span>(results<span class="op">$</span>t) 
temp &lt;-<span class="st"> </span><span class="kw">boot.ci</span>(results, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;norm&quot;</span>, <span class="st">&quot;basic&quot;</span>, <span class="st">&quot;perc&quot;</span>))
OutBoot1[i,<span class="dv">5</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>normal[<span class="dv">2</span>]
OutBoot1[i,<span class="dv">6</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>normal[<span class="dv">3</span>]
OutBoot1[i,<span class="dv">7</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>basic[<span class="dv">4</span>]
OutBoot1[i,<span class="dv">8</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>basic[<span class="dv">5</span>]
OutBoot1[i,<span class="dv">9</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>percent[<span class="dv">4</span>]
OutBoot1[i,<span class="dv">10</span>] &lt;-<span class="st"> </span>temp<span class="op">$</span>percent[<span class="dv">5</span>]
  }</code></pre></div>
</div>
<p><a id=tab:65></a></p>
</div>
<div id="table-6.5.-bootstrap-estimates-of-tvar-at-selected-risk-levels" class="section level5 unnumbered">
<h5>Table 6.5. Bootstrap Estimates of TVaR at Selected Risk Levels</h5>
<table>
<thead>
<tr class="header">
<th align="right">Fraction</th>
<th align="right">NP Estimate</th>
<th align="right">Bootstrap Bias</th>
<th align="right">Bootstrap SD</th>
<th align="right">Lower Normal 95% CI</th>
<th align="right">Upper Normal 95% CI</th>
<th align="right">Lower Basic 95% CI</th>
<th align="right">Upper Basic 95% CI</th>
<th align="right">Lower Percentile 95% CI</th>
<th align="right">Upper Percentile 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">9794.69</td>
<td align="right">-126.05</td>
<td align="right">293.39</td>
<td align="right">9345.71</td>
<td align="right">10495.78</td>
<td align="right">9346.48</td>
<td align="right">10472.56</td>
<td align="right">9116.82</td>
<td align="right">10242.90</td>
</tr>
<tr class="even">
<td align="right">0.80</td>
<td align="right">12454.18</td>
<td align="right">58.79</td>
<td align="right">475.60</td>
<td align="right">11463.23</td>
<td align="right">13327.55</td>
<td align="right">11437.50</td>
<td align="right">13308.25</td>
<td align="right">11600.12</td>
<td align="right">13470.86</td>
</tr>
<tr class="odd">
<td align="right">0.90</td>
<td align="right">14720.05</td>
<td align="right">-40.96</td>
<td align="right">704.89</td>
<td align="right">13379.45</td>
<td align="right">16142.56</td>
<td align="right">13352.52</td>
<td align="right">16022.97</td>
<td align="right">13417.13</td>
<td align="right">16087.57</td>
</tr>
<tr class="even">
<td align="right">0.95</td>
<td align="right">17072.43</td>
<td align="right">-7.29</td>
<td align="right">1141.01</td>
<td align="right">14843.37</td>
<td align="right">19316.07</td>
<td align="right">14860.29</td>
<td align="right">19297.01</td>
<td align="right">14847.85</td>
<td align="right">19284.57</td>
</tr>
<tr class="odd">
<td align="right">0.98</td>
<td align="right">20140.56</td>
<td align="right">81.22</td>
<td align="right">1623.89</td>
<td align="right">16876.56</td>
<td align="right">23242.10</td>
<td align="right">16883.05</td>
<td align="right">23117.58</td>
<td align="right">17163.53</td>
<td align="right">23398.07</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="S:ParametricBootStrap" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Parametric Bootstrap</h3>
<p>The idea of the nonparametric bootstrap is to resample by drawing independent variables from the empirical cumulative distribution function <span class="math inline">\(F_n\)</span>. In contrast, with parametric bootstrap, we draw independent variables from <span class="math inline">\(F_{\widehat{\theta}}\)</span> where the underlying distribution is assume to be in a parametric family <span class="math inline">\(\mathcal{F}=\{F_{\theta},\theta\in\Theta\}\)</span>. Typically, parameters from this distribution are estimated based on a sample and denoted as <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p><strong>Example 6.2.4. Lognormal distribution.</strong> Consider again the dataset</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample_x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">2.46</span>,<span class="fl">2.80</span>,<span class="fl">3.28</span>,<span class="fl">3.86</span>,<span class="fl">2.85</span>,<span class="fl">3.67</span>,<span class="fl">3.37</span>,<span class="fl">3.40</span>,
              <span class="fl">5.22</span>,<span class="fl">2.55</span>,<span class="fl">2.79</span>,<span class="fl">4.50</span>,<span class="fl">3.37</span>,<span class="fl">2.88</span>,<span class="fl">1.44</span>,<span class="fl">2.56</span>,<span class="fl">2.00</span>,<span class="fl">2.07</span>,<span class="fl">2.19</span>,<span class="fl">1.77</span>)</code></pre></div>
<p>The classical (nonparametric) bootstrap was based on samples</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">sample</span>(sample_x,<span class="dt">replace=</span><span class="ot">TRUE</span>)</code></pre></div>
<p>while for the parametric bootstrap, we have to assume that the distribution of <span class="math inline">\(x_i\)</span>’s is from a specific family, for instance a lognormal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
fit &lt;-<span class="st"> </span><span class="kw">fitdistr</span>(sample_x, dlnorm, <span class="kw">list</span>(<span class="dt">meanlog =</span> <span class="dv">1</span>, <span class="dt">sdlog =</span> <span class="dv">1</span>))
fit</code></pre></div>
<pre><code>    meanlog       sdlog   
  1.03630697   0.30593440 
 (0.06840901) (0.04837027)</code></pre>
<p>Then we draw from that distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(<span class="kw">length</span>(sample_x), <span class="dt">meanlog=</span>fit<span class="op">$</span>estimate[<span class="dv">1</span>], <span class="dt">sdlog=</span>fit<span class="op">$</span>estimate[<span class="dv">2</span>])</code></pre></div>
<h5 style="text-align: center;">
<a id="displayCode.BootTVar.1" href="javascript:togglecode('toggleCode.BootTVar.1','displayCode.BootTVar.1');"><i><strong>Show R Code for Parametric Bootstrap Samples</strong></i></a>
</h5>
<div id="toggleCode.BootTVar.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2074</span>)
CV &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="fl">1e5</span>,<span class="dv">2</span>)
<span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(CV)){
x1 &lt;-<span class="st"> </span><span class="kw">sample</span>(sample_x,<span class="dt">replace=</span><span class="ot">TRUE</span>)
x2 &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(<span class="kw">length</span>(sample_x), <span class="dt">meanlog=</span>fit<span class="op">$</span>estimate[<span class="dv">1</span>], <span class="dt">sdlog=</span>fit<span class="op">$</span>estimate[<span class="dv">2</span>])
CV[s,] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">sd</span>(x1)<span class="op">/</span><span class="kw">mean</span>(x1),<span class="kw">sd</span>(x2)<span class="op">/</span><span class="kw">mean</span>(x2))
}</code></pre></div>
</div>
<p>Figure <a href="C-Simulation.html#fig:CoefVarCompare">6.11</a> compares the bootstrap distributions for the coefficient of variation, one based on the nonparametric approach and the other based on a parametric approach, assuming a lognormal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">density</span>(CV[,<span class="dv">1</span>]),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Coefficient of Variation&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(CV[,<span class="dv">2</span>]),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">sd</span>(sample_x)<span class="op">/</span><span class="kw">mean</span>(sample_x),<span class="dt">lty=</span><span class="dv">3</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;nonparametric&quot;</span>,<span class="st">&quot;parametric(LN)&quot;</span>),
       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>),<span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dt">bty=</span><span class="st">&quot;n&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:CoefVarCompare"></span>
<img src="LossDataAnalytics_files/figure-html/CoefVarCompare-1.png" alt="Comparision of Nonparametric and Parametric Bootstrap Distributions for the Coefficient of Variation" width="672" />
<p class="caption">
Figure 6.11: Comparision of Nonparametric and Parametric Bootstrap Distributions for the Coefficient of Variation
</p>
</div>
<hr />
<p><strong>Example 6.2.5. Bootstrapping Censored Observations.</strong> The parametric bootstrap draws simulated realizations from a parametric estimate of the distribution function. In the same way, we can draw simulated realizations from estimates of a distribution function. As one example, we might draw from smoothed estimates of a distribution function introduced in Section 4.1.1.4. Another special case, considered here, is to draw an estimate from the Kaplan-Meier estimator introduced in Section 4.3.2.2. In this way, we can handle observations that are censored.</p>
<p>Specifically, return to the bodily injury data in Examples 6.2.1 and 6.2.3 but now we include the 17 claims that were censored by policy limits. In Example 4.3.6, we used this full dataset to estimate the Kaplan-Meier estimator of the survival function introduced in Section 4.3.2.2. <a href="#tab:66">Table 6.6</a> present bootstrap estimates of the quantiles from the Kaplan-Meier survival function estimator. These include the bootstrap precision estimates, bias and standard deviation, as well as the basic 95% confidence interval.</p>
<h5 style="text-align: center;">
<a id="displayCode.KMCode.1" href="javascript:togglecode('toggleCode.KMCode.1','displayCode.KMCode.1');"><i><strong>Show R Code For Bootstrap Kaplan-Meier Estimates</strong></i></a>
</h5>
<div id="toggleCode.KMCode.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example from Derrig et al</span>
<span class="kw">library</span>(survival)                <span class="co"># for Surv(), survfit()</span>
BIData<span class="op">$</span>UnCensored &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">*</span>(BIData<span class="op">$</span>AmountPaid <span class="op">&lt;</span><span class="st"> </span>BIData<span class="op">$</span>PolicyLimit)
## KM estimate
KM0 &lt;-<span class="st"> </span><span class="kw">survfit</span>(<span class="kw">Surv</span>(AmountPaid, UnCensored) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,  
               <span class="dt">type=</span><span class="st">&quot;kaplan-meier&quot;</span>, <span class="dt">data=</span>BIData)

<span class="kw">set.seed</span>(<span class="dv">2019</span>)
PercentVec &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.80</span>, <span class="fl">0.90</span>, <span class="fl">0.95</span>, <span class="fl">0.98</span>)
OutBoot1 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="dv">5</span>,<span class="dv">6</span>)
<span class="kw">colnames</span>(OutBoot1) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Fraction&quot;</span>,<span class="st">&quot;KM NP Estimate&quot;</span>, <span class="st">&quot;Bootstrap Bias&quot;</span>,
                        <span class="st">&quot;Bootstrap SD&quot;</span>,  <span class="st">&quot;Lower Basic 95% CI&quot;</span>, <span class="st">&quot;Upper Basic 95% CI&quot;</span>)
KM.survobj &lt;-<span class="st"> </span><span class="kw">Surv</span>(BIData<span class="op">$</span>AmountPaid, BIData<span class="op">$</span>UnCensored) 
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(PercentVec)) {
OutBoot1[i,<span class="dv">1</span>] &lt;-<span class="st"> </span>PercentVec[i]
results &lt;-<span class="st"> </span><span class="kw">bootkm</span>(KM.survobj, <span class="dt">q=</span><span class="dv">1</span><span class="op">-</span>PercentVec[i], <span class="dt">B=</span><span class="dv">1000</span>, <span class="dt">pr =</span> <span class="ot">FALSE</span>)
<span class="cf">if</span> (i<span class="op">==</span><span class="dv">1</span>){bootreal &lt;-<span class="st"> </span>results}
OutBoot1[i,<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">quantile</span>(KM0, PercentVec[i])<span class="op">$</span>quantile
OutBoot1[i,<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">mean</span>(results)<span class="op">-</span>OutBoot1[i,<span class="dv">2</span>]
OutBoot1[i,<span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">sd</span>(results) 
<span class="co"># temp &lt;- boot.ci(results, type = c(&quot;norm&quot;,  &quot;basic&quot;,&quot;perc&quot;))</span>
OutBoot1[i,<span class="dv">5</span>] &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>OutBoot1[i,<span class="dv">2</span>]<span class="op">-</span><span class="kw">quantile</span>(results,.<span class="dv">975</span>, <span class="dt">type=</span><span class="dv">6</span>)
OutBoot1[i,<span class="dv">6</span>] &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>OutBoot1[i,<span class="dv">2</span>]<span class="op">-</span><span class="kw">quantile</span>(results,.<span class="dv">025</span>, <span class="dt">type=</span><span class="dv">6</span>)
}</code></pre></div>
</div>
<p><a id=tab:66></a></p>
<div id="table-6.6.-bootstrap-kaplan-meier-estimates-of-quantiles-at-selected-fractions" class="section level5 unnumbered">
<h5>Table 6.6. Bootstrap Kaplan-Meier Estimates of Quantiles at Selected Fractions</h5>
<table>
<thead>
<tr class="header">
<th align="right">Fraction</th>
<th align="right">KM NP Estimate</th>
<th align="right">Bootstrap Bias</th>
<th align="right">Bootstrap SD</th>
<th align="right">Lower Basic 95% CI</th>
<th align="right">Upper Basic 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">6500</td>
<td align="right">13.58</td>
<td align="right">181.23</td>
<td align="right">6093</td>
<td align="right">6923</td>
</tr>
<tr class="even">
<td align="right">0.80</td>
<td align="right">9500</td>
<td align="right">173.61</td>
<td align="right">423.41</td>
<td align="right">8445</td>
<td align="right">9949</td>
</tr>
<tr class="odd">
<td align="right">0.90</td>
<td align="right">12756</td>
<td align="right">20.17</td>
<td align="right">675.86</td>
<td align="right">10812</td>
<td align="right">14012</td>
</tr>
<tr class="even">
<td align="right">0.95</td>
<td align="right">18500</td>
<td align="right">Inf</td>
<td align="right">NaN</td>
<td align="right">12500</td>
<td align="right">22300</td>
</tr>
<tr class="odd">
<td align="right">0.98</td>
<td align="right">25000</td>
<td align="right">Inf</td>
<td align="right">NaN</td>
<td align="right">-Inf</td>
<td align="right">27308</td>
</tr>
</tbody>
</table>
<p>Results in <a href="#tab:66">Table 6.6</a> are consistent with the results for the uncensored subsample in <a href="#tab:64">Table 6.4</a>. In <a href="#tab:66">Table 6.6</a>, we note the difficulty in estimating quantiles at large fractions due to the censoring. However, for moderate size fractions (0.50, 0.80, and 0.90), the Kaplan-Meier nonparametric (KM NP) estimates of the quantile are consistent with those <a href="#tab:64">Table 6.4</a>. The bootstrap standard deviation is smaller at the 0.50 (corresponding to the median) but larger at the 0.80 and 0.90 levels. The censored data analysis summarized in <a href="#tab:66">Table 6.6</a> uses more data than the uncensored subsample analysis in <a href="#tab:64">Table 6.4</a> but also has difficulty extracting information for large quantiles.</p>
</div>
</div>
</div>
<div id="S:CrossValidation" class="section level2">
<h2><span class="header-section-number">6.3</span> Cross-Validation</h2>
<p>In this section, you learn how to:</p>
<ul>
<li>Compare and contrast cross-validation to simulation techniques and bootstrap methods.</li>
<li>Use cross-validation techniques for model selection</li>
<li>Explain the jackknife method as a special case of cross-validation and calculate jackknife estimates of bias and standard errors</li>
</ul>
<hr />
<!-- ## Cross-Validation -->
<p>Cross-validation, briefly introduced in Section 4.2.4, is a technique based on simulated outcomes and so let’s think about its purposes in contrast to other simulation techniques already introduced in this chapter.</p>
<ul>
<li>Simulation, or Monte-Carlo, introduced in Section <a href="C-Simulation.html#S:SimulationFundamentals">6.1</a>, allow us to compute expected values and other summaries of statistical distributions, such as <span class="math inline">\(p\)</span>-values, readily.</li>
<li>Bootstrap, and other resampling methods introduced in Section <a href="C-Simulation.html#S:Bootstrap">6.2</a>, provide estimators of the precision, or variability, of statistics.</li>
<li>Cross-validation is important when assessing how accurately a predictive model will perform in practice.</li>
</ul>
<p>Overlap exists but nonetheless it is helpful to think about the broad goals as associated with each statistical method.</p>
<p>To discuss cross-validation, let us recall from Section 4.2 some of the key ideas of model validation. When assessing, or validating, a model, we look to performance measured on <em>new</em> data, or at least not those that were used to fit the model. A classical approach, described in Section 4.2.3, is to split the sample in two: a subpart (the <em>training</em> dataset) is used to fit the model and another one (the <em>testing</em> dataset) is used to validate. However, a limitation of this approach is that results depend on the split; even though the overall sample is fixed, the split between training and test subsamples varies randomly. A different training sample means that model estimated parameters will differ. Different model parameters and a different test sample means that validation statistics will differ. Two analysts may use the same data and same models yet reach different conclusions about the viability of a model (based on different random splits), a frustrating situation.</p>
<!-- 
Cross-validation techniques are used to avoid the basic two part split. Note that two techniques will be mentioned here : an exhaustive approach, where all observations will be used (once, and only once) as a testing observation and a non-exhaustive one, based on bootstrap techniques. See [Arlot & Celisse (2010)](https://projecteuclid.org/euclid.ssu/1268143839) for a survey.
 -->
<div id="k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">6.3.1</span> k-Fold Cross-Validation</h3>
<p>To mitigate this difficulty, it is common to use a cross-validation approach as introduced in Section 4.2.4. The key idea is to emulate the basic test/training approach to model validation by repeating it many times through averaging over different splits of the data. A key advantage is that the validation statistic is not tied to a specific parametric (or nonparametric) model - one can use a nonparametric statistic or a statistic that has economic interpretations - and so this can be used to compare models that are not nested (unlike likelihood ratio procedures).</p>
<p><strong>Example 6.3.1. Wisconsin Property Fund.</strong> For the 2010 property fund data introduced in Section 1.3, we fit gamma and Pareto distributions to the 1,377 claims data. For details of the related goodness of fit, see Appendix Section 15.4.4. We now consider the Kolmogorov-Smirnov statistic introduced in Section 4.1.2.2. When the entire dataset was fit, the Kolmogorov-Smirnov goodness of fit statistic for the gamma distribution turns out to be 0.0478 and for the Pareto distribution is 0.2639. The lower value for the Pareto distribution indicates that this distribution is a better fit than the gamma.</p>
<p>To see how <a href="#" class="tooltip" style="color:green"><em>k-fold cross-validation</em><span style="font-size:8pt">A type of validation method where the data is randomly split into k groups, and each of the k groups is held out as a test dataset in turn, while the other k-1 gropus are used for distribution or model fitting, with the process repeated k times in total</span></a> works, we randomly split the data into <span class="math inline">\(k=8\)</span> groups, or folds, each having about <span class="math inline">\(1377/8 \approx 172\)</span> observations. Then, we fit gamma and Pareto models to a data set with the first seven folds (about <span class="math inline">\(172*7 = 1204\)</span> observations), determine estimated parameters, and then used these fitted models with the held-out data to determine the Kolmogorov-Smirnov statistic.</p>
<h5 style="text-align: center;">
<a id="displayCode.KFoldCV.1" href="javascript:togglecode('toggleCode.KFoldCV.1','displayCode.KFoldCV.1');"><i><strong>Show R Code for Kolmogorov-Smirnov Cross-Validation</strong></i></a>
</h5>
<div id="toggleCode.KFoldCV.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Randomly re-order the data - &quot;shuffle it&quot;</span>
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(claim_data)
<span class="kw">set.seed</span>(<span class="dv">12347</span>)
cvdata &lt;-<span class="st"> </span>claim_data[<span class="kw">sample</span>(n), ]
<span class="co"># Number of folds</span>
k &lt;-<span class="st"> </span><span class="dv">8</span>
cvalvec &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dv">2</span>,k)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {
  indices &lt;-<span class="st"> </span>(((i<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">round</span>((<span class="dv">1</span><span class="op">/</span>k)<span class="op">*</span><span class="kw">nrow</span>(cvdata))) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>((i<span class="op">*</span><span class="kw">round</span>((<span class="dv">1</span><span class="op">/</span>k) <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(cvdata))))
<span class="co"># Pareto</span>
  fit.pareto &lt;-<span class="st"> </span><span class="kw">vglm</span>(Claim <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, paretoII, <span class="dt">loc =</span> <span class="dv">0</span>, <span class="dt">data =</span> cvdata[<span class="op">-</span>indices,])
  ksResultPareto &lt;-<span class="st"> </span><span class="kw">ks.test</span>(cvdata[indices,]<span class="op">$</span>Claim, <span class="st">&quot;pparetoII&quot;</span>, <span class="dt">loc =</span> <span class="dv">0</span>, <span class="dt">shape =</span> <span class="kw">exp</span>(<span class="kw">coef</span>(fit.pareto)[<span class="dv">2</span>]), 
        <span class="dt">scale =</span> <span class="kw">exp</span>(<span class="kw">coef</span>(fit.pareto)[<span class="dv">1</span>]))
  cvalvec[<span class="dv">1</span>,i] &lt;-<span class="st"> </span>ksResultPareto<span class="op">$</span>statistic
<span class="co"># Gamma</span>
  fit.gamma &lt;-<span class="st"> </span><span class="kw">glm</span>(Claim <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> cvdata[<span class="op">-</span>indices,], <span class="dt">family =</span> <span class="kw">Gamma</span>(<span class="dt">link =</span> log)) 
  gamma_theta &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">coef</span>(fit.gamma)) <span class="op">*</span><span class="st"> </span><span class="kw">gamma.dispersion</span>(fit.gamma)  
  alpha &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">gamma.dispersion</span>(fit.gamma)
  ksResultGamma &lt;-<span class="st"> </span><span class="kw">ks.test</span>(cvdata[indices,]<span class="op">$</span>Claim, <span class="st">&quot;pgamma&quot;</span>, <span class="dt">shape =</span> alpha, <span class="dt">scale =</span> gamma_theta)
  cvalvec[<span class="dv">2</span>,i] &lt;-<span class="st"> </span>ksResultGamma<span class="op">$</span>statistic
}
KScv &lt;-<span class="st"> </span><span class="kw">rowSums</span>(cvalvec)<span class="op">/</span>k</code></pre></div>
</div>
<p>The results appear in Figure <a href="C-Simulation.html#fig:KScvFig">6.12</a> where horizontal axis is Fold=1. This process was repeated for the other seven folds. The results summarized in Figure <a href="C-Simulation.html#fig:KScvFig">6.12</a> show that the Pareto consistently provides a more reliable predictive distribution than the gamma.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the statistics</span>
<span class="kw">matplot</span>(<span class="dv">1</span><span class="op">:</span>k,<span class="kw">t</span>(cvalvec),<span class="dt">type=</span><span class="st">&quot;b&quot;</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, 
        <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>), <span class="dt">pch =</span> <span class="dv">0</span>, <span class="dt">xlab=</span><span class="st">&quot;Fold&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;KS Statistic&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;left&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;Pareto&quot;</span>, <span class="st">&quot;Gamma&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>),<span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:KScvFig"></span>
<img src="LossDataAnalytics_files/figure-html/KScvFig-1.png" alt="Cross Validated Kolmogorov-Smirnov (KS) Statistics for the Property Fund Claims Data. The solid black line is for the Pareto distribution, the green dashed line is for the gamma distribution. The KS statistic measures the largest deviation between the fitted distribution and the empirical distribution for each of 8 groups, or folds, of randomly selected data." width="672" />
<p class="caption">
Figure 6.12: Cross Validated Kolmogorov-Smirnov (KS) Statistics for the Property Fund Claims Data. The solid black line is for the Pareto distribution, the green dashed line is for the gamma distribution. The KS statistic measures the largest deviation between the fitted distribution and the empirical distribution for each of 8 groups, or folds, of randomly selected data.
</p>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Leave-One-Out Cross-Validation</h3>
<p>A special case where <span class="math inline">\(k=n\)</span> is known as <a href="#" class="tooltip" style="color:green"><em>leave-one-out cross validation</em><span style="font-size:8pt">A special case of k-fold cross validation, where each single data point gets a turn in being the lone hold-out test data point, and n separate models in total are built and tested</span></a>. This case is historically prominent and is closely related to <a href="#" class="tooltip" style="color:green"><em>jackknife statistics</em><span style="font-size:8pt">To calculate an estimator, leave out each observation in turn, calculate the sample estimator statistic each time, and average over the n separate estimates</span></a>, a precursor of the bootstrap technique.</p>
<p>Even though we present it as a special case of cross-validation, it is helpful to given an explicit definition. Consider a generic statistic <span class="math inline">\(\widehat{\theta}=t(\boldsymbol{x})\)</span> that is an estimator for a parameter of interest <span class="math inline">\(\theta\)</span>. The idea of the jackknife is to compute <span class="math inline">\(n\)</span> values <span class="math inline">\(\widehat{\theta}_{-i}=t(\boldsymbol{x}_{-i})\)</span>, where <span class="math inline">\(\boldsymbol{x}_{-i}\)</span> is the subsample of <span class="math inline">\(\boldsymbol{x}\)</span> with the <span class="math inline">\(i\)</span>-th value removed. The average of these values is denoted as <span class="math display">\[
\overline{\widehat{\theta}}_{(\cdot)}=\frac{1}{n}\sum_{i=1}^n \widehat{\theta}_{-i} .
\]</span> These values can be used to create estimates of the bias of the statistic <span class="math inline">\(\widehat{\theta}\)</span></p>
<span class="math display" id="eq:Biasjack">\[\begin{equation}
Bias_{jack} = (n-1) \left(\overline{\widehat{\theta}}_{(\cdot)} - \widehat{\theta}\right)
\tag{6.3}
\end{equation}\]</span>
<p>as well as a standard deviation estimate</p>
<span class="math display" id="eq:sdjack">\[\begin{equation}
s_{jack} =\sqrt{\frac{n-1}{n}\sum_{i=1}^n \left(\widehat{\theta}_{-i} -\overline{\widehat{\theta}}_{(\cdot)}\right)^2} ~.
\tag{6.4}
\end{equation}\]</span>
<p><strong>Example 6.3.2. Coefficient of Variation.</strong> To illustrate, consider a small fictitious sample <span class="math inline">\(\boldsymbol{x}=\{x_1,\ldots,x_n\}\)</span> with realizations</p>
<pre><code>sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,
              5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77)</code></pre>
<p>Suppose that we are interested in the <a href="#" class="tooltip" style="color:green"><em>coefficient of variation</em><span style="font-size:8pt">Standard deviation divided by the mean of a distribution, to measure variability in terms of units of the mean</span></a> <span class="math inline">\(\theta = CV = \sqrt{\mathrm{Var~}X}/\mathrm{E~}X\)</span>.</p>
<p>With this dataset, the estimator of the coefficient of variation turns out to be 0.31196. But how reliable is it? To answer this question, we can compute the jackknife estimates of bias and its standard deviation. The following code shows that the jackknife estimator of the bias is <span class="math inline">\(Bias_{jack} =\)</span> -0.00627 and the jackknife standard deviation is <span class="math inline">\(s_{jack} =\)</span> 0.01293.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CVar &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">sqrt</span>(<span class="kw">var</span>(x))<span class="op">/</span><span class="kw">mean</span>(x)
JackCVar &lt;-<span class="st"> </span><span class="cf">function</span>(i) <span class="kw">sqrt</span>(<span class="kw">var</span>(sample_x[<span class="op">-</span>i]))<span class="op">/</span><span class="kw">mean</span>(sample_x[<span class="op">-</span>i])
JackTheta &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(JackCVar)(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(sample_x))
BiasJack &lt;-<span class="st"> </span>(<span class="kw">length</span>(sample_x)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="kw">mean</span>(JackTheta) <span class="op">-</span><span class="st"> </span><span class="kw">CVar</span>(sample_x))
<span class="kw">sd</span>(JackTheta)</code></pre></div>
<hr />
<p><strong>Example 6.3.3. Bodily Injury Claims and Loss Elimination Ratios.</strong> In Example 6.2.1, we showed how to compute bootstrap estimates of the bias and standard deviation for the loss elimination ratio using the Example 4.1.11 bodily injury claims data. We follow up now by providing comparable quantities using jackknife statistics.</p>
<p><a href="#tab:67">Table 6.7</a> summarizes the results of the jackknife estimation. It shows that jackknife estimates of the bias and standard deviation of the loss elimination ratio <span class="math inline">\(\mathrm{E}~\min(X,d)/\mathrm{E}~X\)</span> are largely consistent with the bootstrap methodology. Moreover, one can use the standard deviations to construct normal based confidence intervals, centered around a bias-corrected estimator. For example, at <span class="math inline">\(d=14000\)</span>, we saw in Example 4.1.11 that the nonparametric estimate of <em>LER</em> is 0.97678. This has an estimated bias of 0.00010, resulting in the (jackknife) <em>bias-corrected</em> estimator 0.97688. The 95% confidence intervals are produced by creating an interval of twice the length of 1.96 jackknife standard deviations, centered about the bias-corrected estimator (1.96 is the approximate 97.5th quantile of the standard normal distribution).</p>
<h5 style="text-align: center;">
<a id="displayCode.Jackknife.1" href="javascript:togglecode('toggleCode.Jackknife.1','displayCode.Jackknife.1');"><i><strong>Show the R Code</strong></i></a>
</h5>
<div id="toggleCode.Jackknife.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example from Derrig et al</span>
BIData &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;../../Data/DerrigResampling.csv&quot;</span>, <span class="dt">header =</span>T)
BIData<span class="op">$</span>Censored &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">*</span>(BIData<span class="op">$</span>AmountPaid <span class="op">&gt;=</span><span class="st"> </span>BIData<span class="op">$</span>PolicyLimit)
BIDataUncensored &lt;-<span class="st"> </span><span class="kw">subset</span>(BIData, Censored <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)
LER.boot &lt;-<span class="st"> </span><span class="cf">function</span>(ded, data, indices){
  resample.data &lt;-<span class="st"> </span>data[indices,]
  sumClaims &lt;-<span class="st"> </span><span class="kw">sum</span>(resample.data<span class="op">$</span>AmountPaid)
  sumClaims_d &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">pmin</span>(resample.data<span class="op">$</span>AmountPaid,ded))
  LER &lt;-<span class="st">   </span>sumClaims_d<span class="op">/</span>sumClaims
  <span class="kw">return</span>(LER)  
}

x &lt;-<span class="st"> </span>BIDataUncensored<span class="op">$</span>AmountPaid
LER.jack&lt;-<span class="st"> </span><span class="cf">function</span>(ded,i){
  LER &lt;-<span class="st">   </span><span class="kw">sum</span>(<span class="kw">pmin</span>(x[<span class="op">-</span>i],ded))<span class="op">/</span><span class="kw">sum</span>(x[<span class="op">-</span>i])
  <span class="kw">return</span>(LER)  
}
LER &lt;-<span class="st"> </span><span class="cf">function</span>(ded) <span class="kw">sum</span>(<span class="kw">pmin</span>(x,ded))<span class="op">/</span><span class="kw">sum</span>(x)
##Derrig et al
<span class="kw">set.seed</span>(<span class="dv">2019</span>)
dVec2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">4000</span>, <span class="dv">5000</span>, <span class="dv">10500</span>, <span class="dv">11500</span>, <span class="dv">14000</span>, <span class="dv">18500</span>)
OutJack &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="kw">length</span>(dVec2),<span class="dv">8</span>)
<span class="kw">colnames</span>(OutJack) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;d&quot;</span>,<span class="st">&quot;NP Estimate&quot;</span>,<span class="st">&quot;Bootstrap Bias&quot;</span>, <span class="st">&quot;Bootstrap SD&quot;</span>, <span class="st">&quot;Jackknife Bias&quot;</span>, <span class="st">&quot;Jackknife SD&quot;</span>,<span class="st">&quot;Lower Jackknife 95% CI&quot;</span>, <span class="st">&quot;Upper Jackknife 95% CI&quot;</span>)
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(dVec2)) {
OutJack[j,<span class="dv">1</span>] &lt;-<span class="st"> </span>dVec2[j]
results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>BIDataUncensored, <span class="dt">statistic=</span>LER.boot, <span class="dt">R=</span><span class="dv">1000</span>, <span class="dt">ded=</span>dVec2[j])
OutJack[j,<span class="dv">2</span>] &lt;-<span class="st"> </span>results<span class="op">$</span>t0
biasboot &lt;-<span class="st"> </span><span class="kw">mean</span>(results<span class="op">$</span>t)<span class="op">-</span>results<span class="op">$</span>t0 -&gt;<span class="st"> </span>OutJack[j,<span class="dv">3</span>]
sdboot &lt;-<span class="st"> </span><span class="kw">sd</span>(results<span class="op">$</span>t) -&gt;<span class="st"> </span>OutJack[j,<span class="dv">4</span>]
temp &lt;-<span class="st"> </span><span class="kw">boot.ci</span>(results)

LER.jack.ded&lt;-<span class="st"> </span><span class="cf">function</span>(i) <span class="kw">LER.jack</span>(<span class="dt">ded=</span>dVec2[j],i)
JackTheta.ded &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(LER.jack.ded)(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x))
OutJack[j,<span class="dv">5</span>] &lt;-<span class="st"> </span>BiasJack.ded &lt;-<span class="st"> </span>(<span class="kw">length</span>(x)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="kw">mean</span>(JackTheta.ded) <span class="op">-</span><span class="st"> </span><span class="kw">LER</span>(<span class="dt">ded=</span>dVec2[j]))
OutJack[j,<span class="dv">6</span>] &lt;-<span class="st"> </span><span class="kw">sd</span>(JackTheta.ded)
OutJack[j,<span class="dv">7</span><span class="op">:</span><span class="dv">8</span>] &lt;-<span class="st"> </span><span class="kw">mean</span>(JackTheta.ded)<span class="op">+</span><span class="kw">qt</span>(<span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>),<span class="kw">length</span>(x)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>OutJack[j,<span class="dv">6</span>]
}</code></pre></div>
</div>
<p><a id=tab:67></a></p>
<div id="table-6.7.-jackknife-estimates-of-ler-at-selected-deductibles" class="section level5 unnumbered">
<h5>Table 6.7. Jackknife Estimates of LER at Selected Deductibles</h5>
<table>
<thead>
<tr class="header">
<th align="right">d</th>
<th align="right">NP Estimate</th>
<th align="right">Bootstrap Bias</th>
<th align="right">Bootstrap SD</th>
<th align="right">Jackknife Bias</th>
<th align="right">Jackknife SD</th>
<th align="right">Lower Jackknife 95% CI</th>
<th align="right">Upper Jackknife 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4000</td>
<td align="right">0.54113</td>
<td align="right">-0.00012</td>
<td align="right">0.01228</td>
<td align="right">0.00031</td>
<td align="right">0.00061</td>
<td align="right">0.53993</td>
<td align="right">0.54233</td>
</tr>
<tr class="even">
<td align="right">5000</td>
<td align="right">0.64960</td>
<td align="right">-0.00038</td>
<td align="right">0.01400</td>
<td align="right">0.00033</td>
<td align="right">0.00068</td>
<td align="right">0.64825</td>
<td align="right">0.65094</td>
</tr>
<tr class="odd">
<td align="right">10500</td>
<td align="right">0.93563</td>
<td align="right">0.00018</td>
<td align="right">0.01065</td>
<td align="right">0.00019</td>
<td align="right">0.00053</td>
<td align="right">0.93460</td>
<td align="right">0.93667</td>
</tr>
<tr class="even">
<td align="right">11500</td>
<td align="right">0.95281</td>
<td align="right">-0.00016</td>
<td align="right">0.00918</td>
<td align="right">0.00016</td>
<td align="right">0.00047</td>
<td align="right">0.95189</td>
<td align="right">0.95373</td>
</tr>
<tr class="odd">
<td align="right">14000</td>
<td align="right">0.97678</td>
<td align="right">0.00018</td>
<td align="right">0.00701</td>
<td align="right">0.00010</td>
<td align="right">0.00034</td>
<td align="right">0.97612</td>
<td align="right">0.97745</td>
</tr>
<tr class="even">
<td align="right">18500</td>
<td align="right">0.99382</td>
<td align="right">0.00006</td>
<td align="right">0.00342</td>
<td align="right">0.00003</td>
<td align="right">0.00017</td>
<td align="right">0.99350</td>
<td align="right">0.99415</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Discussion.</strong> One of the many interesting things about the leave-one-out special case is the ability to replicate estimates exactly. That is, when the size of the fold is only one, then there is no additional uncertainty induced by the cross-validation. This means that analysts can exactly replicate work of one another, an important consideration.</p>
<p>Jackknife statistics were developed to understand precision of estimators, producing estimators of bias and standard deviation in equations <a href="C-Simulation.html#eq:Biasjack">(6.3)</a> and <a href="C-Simulation.html#eq:sdjack">(6.4)</a>. This crosses into goals that we have associated with bootstrap techniques, not cross-validation methods. This demonstrates how statistical techniques can be used to achieve different goals.</p>
</div>
</div>
<div id="cross-validation-and-bootstrap" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Cross-Validation and Bootstrap</h3>
<p>The bootstrap is useful in providing estimators of the precision, or variability, of statistics. It can also be useful for model validation. The bootstrap approach to model validation is similar to the leave-one-out and <em>k</em>-fold validation procedures:</p>
<ul>
<li>Create a bootstrap sample by re-sampling (with replacement) <span class="math inline">\(n\)</span> indices in <span class="math inline">\(\{1,\cdots,n\}\)</span>. That will be our <em>training sample</em>. Estimate the model under consideration based on this sample.</li>
<li>The <em>test</em>, or <em>validation sample</em>, consists of those observations not selected for training. Evaluate the fitted model (based on the training data) using the test data.</li>
</ul>
<p>Repeat this process many (say <span class="math inline">\(B\)</span>) times. Take an average over the results and choose the model based on the average evaluation statistic.</p>
<p><strong>Example 6.3.4. Wisconsin Property Fund.</strong> Return to Example 6.3.1 where we investigate the fit of the gamma and Pareto distributions on the property fund data. We again compare the predictive performance using the Kolmogorov-Smirnov statistic but this time using the bootstrap procedure to split the data between training and testing samples. The following provides illustrative code.</p>
<h5 style="text-align: center;">
<a id="displayCode.BootstrapValidation.1" href="javascript:togglecode('toggleCode.BootstrapValidation.1','displayCode.BootstrapValidation.1');"><i><strong>Show R Code for Bootstrapping Validation Procedures</strong></i></a>
</h5>
<div id="toggleCode.BootstrapValidation.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(MASS)</span>
<span class="co"># library(VGAM)</span>
<span class="co"># library(goftest)</span>
<span class="co"># claim_lev &lt;- read.csv(&quot;../../Data/CLAIMLEVEL.csv&quot;, header = TRUE) </span>
<span class="co"># # 2010 subset </span>
<span class="co"># claim_data &lt;- subset(claim_lev, Year == 2010); </span>
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(claim_data)
<span class="kw">set.seed</span>(<span class="dv">12347</span>)
indices &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>n
<span class="co"># Number of Bootstrap Samples</span>
B &lt;-<span class="st"> </span><span class="dv">100</span>
cvalvec &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dv">2</span>,B)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>B) {
  bootindex &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">sample</span>(indices, <span class="dt">size=</span>n, <span class="dt">replace=</span> <span class="ot">TRUE</span>))
  traindata &lt;-<span class="st"> </span>claim_data[bootindex,]
  testdata  &lt;-<span class="st"> </span>claim_data[<span class="op">-</span>bootindex,]
<span class="co"># Pareto</span>
  fit.pareto &lt;-<span class="st"> </span><span class="kw">vglm</span>(Claim <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, paretoII, <span class="dt">loc =</span> <span class="dv">0</span>, <span class="dt">data =</span> traindata)
  ksResultPareto &lt;-<span class="st"> </span><span class="kw">ks.test</span>(testdata<span class="op">$</span>Claim, <span class="st">&quot;pparetoII&quot;</span>, <span class="dt">loc =</span> <span class="dv">0</span>, <span class="dt">shape =</span> <span class="kw">exp</span>(<span class="kw">coef</span>(fit.pareto)[<span class="dv">2</span>]), 
        <span class="dt">scale =</span> <span class="kw">exp</span>(<span class="kw">coef</span>(fit.pareto)[<span class="dv">1</span>]))
  cvalvec[<span class="dv">1</span>,i] &lt;-<span class="st"> </span>ksResultPareto<span class="op">$</span>statistic
<span class="co"># Gamma</span>
  fit.gamma &lt;-<span class="st"> </span><span class="kw">glm</span>(Claim <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> traindata, <span class="dt">family =</span> <span class="kw">Gamma</span>(<span class="dt">link =</span> log)) 
  gamma_theta &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">coef</span>(fit.gamma)) <span class="op">*</span><span class="st"> </span><span class="kw">gamma.dispersion</span>(fit.gamma)  
  alpha &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">gamma.dispersion</span>(fit.gamma)
  ksResultGamma &lt;-<span class="st"> </span><span class="kw">ks.test</span>(testdata<span class="op">$</span>Claim, <span class="st">&quot;pgamma&quot;</span>, <span class="dt">shape =</span> alpha, <span class="dt">scale =</span> gamma_theta)
  cvalvec[<span class="dv">2</span>,i] &lt;-<span class="st"> </span>ksResultGamma<span class="op">$</span>statistic
}
KSBoot &lt;-<span class="st"> </span><span class="kw">rowSums</span>(cvalvec)<span class="op">/</span>B</code></pre></div>
</div>
<p>We did the sampling using <span class="math inline">\(B=\)</span> 100 replications. The average <em>KS</em> statistic for the Pareto distribution was 0.058 compared to the average for the gamma distribution, 0.261. This is consistent with earlier results and provides another piece of evidence that the Pareto is a better model for these data than the gamma.</p>
</div>
</div>
<div id="S:ImportanceSampling" class="section level2">
<h2><span class="header-section-number">6.4</span> Importance Sampling</h2>
<!-- ## Importance Sampling -->
<p>Section <a href="C-Simulation.html#S:SimulationFundamentals">6.1</a> introduced Monte Carlo techniques using the inversion technique : to generate a random variable <span class="math inline">\(X\)</span> with distribution <span class="math inline">\(F\)</span>, apply <span class="math inline">\(F^{-1}\)</span> to calls of a random generator (uniform on the unit interval). What if we what to draw according to <span class="math inline">\(X\)</span>, conditional on <span class="math inline">\(X\in[a,b]\)</span> ?</p>
<p>One can use an <a href="#" class="tooltip" style="color:green"><em>accept-reject mechanism</em><span style="font-size:8pt">A sampling method that is used where the random sample is discarded if not within a certain pre-specified range [a, b] and is commonly used when the traditional inverse transform method cannot be easily used</span></a> : draw <span class="math inline">\(x\)</span> from distribution <span class="math inline">\(F\)</span></p>
<ul>
<li>if <span class="math inline">\(x\in[a,b]\)</span> : keep it (“<em>accept</em>”)</li>
<li>if <span class="math inline">\(x\notin[a,b]\)</span> : draw another one (“<em>reject</em>”)</li>
</ul>
<p>Observe that from <span class="math inline">\(n\)</span> values initially generated, we keep here only <span class="math inline">\([F(b)-F(a)]\cdot n\)</span> draws, on average.</p>
<p><strong>Example 6.4.1. Draws from a Normal Distribution.</strong> Suppose that we draw from a normal distribution with mean 2.5 and variance 1, <span class="math inline">\(N(2.5,1)\)</span>, but are only interested in draws greater that <span class="math inline">\(a \ge 2\)</span> and less than <span class="math inline">\(b \le 4\)</span>. That is, we can only use <span class="math inline">\(F(4)-F(2) = \Phi(4-2.5)-\Phi(2-2.5)\)</span> = 0.9332 - 0.3085 = 0.6247 proportion of the draws. Figure <a href="C-Simulation.html#fig:sampleani1">6.13</a> demonstrates that some draws lie with the interval <span class="math inline">\((2,4)\)</span> and some are outside.</p>
<h5 style="text-align: center;">
<a id="displayCode.ImportanceSampling.1" href="javascript:togglecode('toggleCode.ImportanceSampling.1','displayCode.ImportanceSampling.1');"><i><strong>Show R Code for Accept-Reject Mechanism</strong></i></a>
</h5>
<div id="toggleCode.ImportanceSampling.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu =<span class="st"> </span><span class="fl">2.5</span>
sigma =<span class="st"> </span><span class="dv">1</span>
a =<span class="st"> </span><span class="dv">2</span>
b =<span class="st"> </span><span class="dv">4</span>
Fa =<span class="st"> </span><span class="kw">pnorm</span>(a,mu,sigma)
Fb =<span class="st"> </span><span class="kw">pnorm</span>(b,mu,sigma)
pic_ani =<span class="st"> </span><span class="cf">function</span>(){
  u=<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dt">by=</span>.<span class="dv">01</span>)
  <span class="kw">plot</span>(u,<span class="kw">pnorm</span>(u,mu,sigma),<span class="dt">col=</span><span class="st">&quot;white&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>)
  <span class="kw">rect</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>,<span class="dv">2</span>,<span class="dt">col=</span><span class="kw">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">2</span>),<span class="dt">border=</span><span class="ot">NA</span>)
  <span class="kw">rect</span>(a,Fa,b,Fb,<span class="dt">col=</span><span class="st">&quot;white&quot;</span>,<span class="dt">border=</span><span class="ot">NA</span>)
  <span class="kw">lines</span>(u,<span class="kw">pnorm</span>(u,mu,sigma),<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(a,b),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
  ru &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
  clr &lt;-<span class="st"> &quot;red&quot;</span>
  <span class="cf">if</span>((<span class="kw">qnorm</span>(ru,mu,sigma)<span class="op">&gt;=</span>a)<span class="op">&amp;</span>(<span class="kw">qnorm</span>(ru,mu,sigma)<span class="op">&lt;=</span>b)) clr &lt;-<span class="st"> &quot;blue&quot;</span>
  <span class="kw">segments</span>(<span class="op">-</span><span class="dv">1</span>,ru,<span class="kw">qnorm</span>(ru,mu,sigma),ru,<span class="dt">col=</span>clr,<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">arrows</span>(<span class="kw">qnorm</span>(ru,mu,sigma),ru,<span class="kw">qnorm</span>(ru,mu,sigma),<span class="dv">0</span>,<span class="dt">col=</span>clr,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">length =</span> .<span class="dv">1</span>)
}</code></pre></div>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>numAnimation) {<span class="kw">pic_ani</span>()}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampleani1"></span>
<img src="LossDataAnalytics_files/figure-html/sampleani1-.gif" alt="Animated Demonstration of Draws In and Outside of (2,4)" width="50%" />
<p class="caption">
Figure 6.13: Animated Demonstration of Draws In and Outside of (2,4)
</p>
</div>
<hr />
<p>Instead, one can draw according to the conditional distribution <span class="math inline">\(F^{\star}\)</span> defined as</p>
<p><span class="math display">\[
F^{\star}(x) = \Pr(X \le x | a &lt; X \le b) =\frac{F(x)-F(a)}{F(b)-F(a)}, \ \  \ \text{for } a &lt; x \le b .
\]</span></p>
<p>Using the inverse transform method in Section <a href="C-Simulation.html#S:InverseTransform">6.1.2</a>, we have that the draw</p>
<p><span class="math display">\[
X^\star=F^{\star-1}\left( U \right) = F^{-1}\left(F(a)+U\cdot[F(b)-F(a)]\right)
\]</span></p>
<p>has distribution <span class="math inline">\(F^{\star}\)</span>. Expressed another way, define <span class="math display">\[
\tilde{U} = (1-U)\cdot F(a)+U\cdot F(b)
\]</span> and then use <span class="math inline">\(F^{-1}(\tilde{U})\)</span>. With this approach, each draw counts.</p>
<p>This can be related to the <a href="#" class="tooltip" style="color:green"><em>importance sampling mechanism</em><span style="font-size:8pt">Type of sampling method where values in the region of interest can be over-sampled or values outside the region of interest can be under-sampled</span></a> : we draw more frequently in regions where we expect to have quantities that have some interest. This transform can be considered as a “a change of measure.”</p>
<h5 style="text-align: center;">
<a id="displayCode.ImportanceSampling.2" href="javascript:togglecode('toggleCode.ImportanceSampling.2','displayCode.ImportanceSampling.2');"><i><strong>Show R Code for Importance Sampling by the Inverse Transform Method</strong></i></a>
</h5>
<div id="toggleCode.ImportanceSampling.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pic_ani =<span class="st"> </span><span class="cf">function</span>(){
  u=<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dt">by=</span>.<span class="dv">01</span>)
  <span class="kw">plot</span>(u,<span class="kw">pnorm</span>(u,mu,sigma),<span class="dt">col=</span><span class="st">&quot;white&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>)
  <span class="kw">rect</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>,<span class="dv">2</span>,<span class="dt">col=</span><span class="kw">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">2</span>),<span class="dt">border=</span><span class="ot">NA</span>)
  <span class="kw">rect</span>(a,Fa,b,Fb,<span class="dt">col=</span><span class="st">&quot;white&quot;</span>,<span class="dt">border=</span><span class="ot">NA</span>)
  <span class="kw">lines</span>(u,<span class="kw">pnorm</span>(u,mu,sigma),<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">pnorm</span>(<span class="kw">c</span>(a,b),mu,sigma),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
  ru &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
  rutilde &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>ru)<span class="op">*</span>Fa<span class="op">+</span>ru<span class="op">*</span>Fb
  <span class="kw">segments</span>(<span class="op">-</span><span class="dv">1</span>,rutilde,<span class="kw">qnorm</span>(rutilde,mu,sigma),rutilde,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">arrows</span>(<span class="kw">qnorm</span>(rutilde,mu,sigma),rutilde,<span class="kw">qnorm</span>(rutilde,mu,sigma),<span class="dv">0</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">length =</span> .<span class="dv">1</span>)
}</code></pre></div>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>numAnimation) {<span class="kw">pic_ani</span>()}</code></pre></div>
<p><img src="LossDataAnalytics_files/figure-html/sampleani_IS_2-.gif" width="50%" style="display: block; margin: auto;" /></p>
<p>In Example 6.4.1., the inverse of the normal distribution is readily available (in <code>R</code>, the function is <code>qnorm</code>). However, for other applications, this is not the case. Then, one simply uses numerical methods to determine <span class="math inline">\(X^\star\)</span> as the solution of the equation <span class="math inline">\(F(X^\star) =\tilde{U}\)</span> where <span class="math inline">\(\tilde{U}=(1-U)\cdot F(a)+U\cdot F(b)\)</span>. See the following illustrative code.</p>
<h5 style="text-align: center;">
<a id="displayCode.ImportanceSampling.3" href="javascript:togglecode('toggleCode.ImportanceSampling.3','displayCode.ImportanceSampling.3');"><i><strong>Show R Code for Importance Sampling via Numerical Solutions</strong></i></a>
</h5>
<div id="toggleCode.ImportanceSampling.3" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pic_ani =<span class="st"> </span><span class="cf">function</span>(){
  u=<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dt">by=</span>.<span class="dv">01</span>)
  <span class="kw">plot</span>(u,<span class="kw">pnorm</span>(u,mu,sigma),<span class="dt">col=</span><span class="st">&quot;white&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>)
  <span class="kw">rect</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>,<span class="dv">2</span>,<span class="dt">col=</span><span class="kw">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">2</span>),<span class="dt">border=</span><span class="ot">NA</span>)
  <span class="kw">rect</span>(<span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;white&quot;</span>,<span class="dt">border=</span><span class="ot">NA</span>)
  <span class="kw">lines</span>(u,<span class="kw">pnorm</span>(u,mu,sigma),<span class="dt">lty=</span><span class="dv">2</span>)
  pnormstar &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(<span class="cf">function</span>(x){
    y=(<span class="kw">pnorm</span>(x,mu,sigma)<span class="op">-</span>Fa)<span class="op">/</span>(Fb<span class="op">-</span>Fa)
    <span class="cf">if</span>(x<span class="op">&lt;=</span>a) y &lt;-<span class="st"> </span><span class="dv">0</span>
    <span class="cf">if</span>(x<span class="op">&gt;=</span>b) y &lt;-<span class="st"> </span><span class="dv">1</span>
    <span class="kw">return</span>(y)
    })
  qnormstar &lt;-<span class="st"> </span><span class="cf">function</span>(u) <span class="kw">as.numeric</span>(<span class="kw">uniroot</span>((<span class="cf">function</span> (x) <span class="kw">pnormstar</span>(x) <span class="op">-</span><span class="st"> </span>u), <span class="dt">lower =</span> <span class="dv">2</span>, <span class="dt">upper =</span> <span class="dv">4</span>)[<span class="dv">1</span>])
  <span class="kw">lines</span>(u,<span class="kw">pnormstar</span>(u),<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
  ru &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
  <span class="kw">segments</span>(<span class="op">-</span><span class="dv">1</span>,ru,<span class="kw">qnormstar</span>(ru),ru,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">arrows</span>(<span class="kw">qnormstar</span>(ru),ru,<span class="kw">qnormstar</span>(ru),<span class="dv">0</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">length =</span> .<span class="dv">1</span>)
}</code></pre></div>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>numAnimation) {<span class="kw">pic_ani</span>()}</code></pre></div>
<p><img src="LossDataAnalytics_files/figure-html/sampleani_IS_1-.gif" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="S:MCMC" class="section level2">
<h2><span class="header-section-number">6.5</span> Monte Carlo Markov Chain (MCMC)</h2>
<hr />
<p><strong>This section is being written and is not yet complete nor edited. It is here to give you a flavor of what will be in the final version.</strong></p>
<hr />
<!-- ## MCMC -->
<p>The idea of Monte Carlo techniques rely on the law of large number (that insures the convengence of the average towards the integral) and the central limit theorem (that is used to quantify uncertainty in the computations). Recall that if <span class="math inline">\((X_i)\)</span> is an i.id. sequence of random variables with distribution <span class="math inline">\(F\)</span>, then <span class="math display">\[
\frac{1}{\sqrt{n}}\left(\sum_{i=1}^n h(X_i)-\int h(x)dF(x)\right)\overset{\mathcal{L}}{\rightarrow }\mathcal{N}(0,\sigma^2),\text{ as }n\rightarrow\infty.
\]</span> for some variance <span class="math inline">\(\sigma^2&gt;0\)</span>. But actually, the <a href="#" class="tooltip" style="color:green"><em>ergodic theorem</em><span style="font-size:8pt">Ergodic theory studies the behavior of a dynamical system when it is allowed to run for an extended time</span></a> can be used to weaker the previous result, since it is not necessary to have independence of the variables. More precisely, if <span class="math inline">\((X_i)\)</span> is a <a href="#" class="tooltip" style="color:green"><em>Markov Process</em><span style="font-size:8pt">A stochastic (time dependent) process that satisfies memorylessness, meaning future predictions of the process can be made solely based on its present state and not the historical path</span></a> with <a href="#" class="tooltip" style="color:green"><em>invariant measure</em><span style="font-size:8pt">Any mathematical measure that is preserved by a function (the mean is an example)</span></a> <span class="math inline">\(\mu\)</span>, under some additional technical assumptions, we can obtain that <span class="math display">\[
\frac{1}{\sqrt{n}}\left(\sum_{i=1}^n h(X_i)-\int h(x)d\mu(x)\right)\overset{\mathcal{L}}{\rightarrow }\mathcal{N}(0,\sigma_\star^2),\text{ as }n\rightarrow\infty.
\]</span> for some variance <span class="math inline">\(\sigma_\star^2&gt;0\)</span>.</p>
<p>Hence, from this property, we can see that it is possible not necessarily to generate independent values from <span class="math inline">\(F\)</span>, but to generate a markov process with invariante measure <span class="math inline">\(F\)</span>, and to consider means over the process (not necessarily independent).</p>
<p>Consider the case of a constraint Gaussian vector : we want to generate random pairs from a random vector <span class="math inline">\(\boldsymbol{X}\)</span>, but we are interested only on the case where the sum of the <a href="#" class="tooltip" style="color:green"><em>composants</em><span style="font-size:8pt">Component (smaller, self-contained part of larger entity)</span></a> is large enough, which can be written <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{1}&gt; m\)</span> for some real valued <span class="math inline">\(m\)</span>. Of course, it is possible to use the <em>accept-reject</em> algorithm, but we have seen that it might be quite inefficient. One can use <a href="#" class="tooltip" style="color:green"><em>Hastings Metropolis</em><span style="font-size:8pt">A markov chain monte carlo (mcmc) method for random sampling from a probability distribution where values are iteratively generated, with the distribution of the next sample dependent only on the current sample value, and at each iteration, the candidate sample can be either accepted or rejected</span></a> and <a href="#" class="tooltip" style="color:green"><em>Gibbs sampler</em><span style="font-size:8pt">A markov chain monte carlo (mcmc) method to obtain a sequence of random samples from a specified multivariate continuous probability distribution</span></a> to generate a Markov process with such an invariant measure.</p>
<div id="hastings-metropolis" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Hastings Metropolis</h3>
<p>The algorithm is rather simple to generate from <span class="math inline">\(f\)</span> : we start with a feasible value <span class="math inline">\(x_1\)</span>. Then, at step <span class="math inline">\(t\)</span>, we need to specify a transition kernel : given <span class="math inline">\(x_t\)</span>, we need a conditional distribution for <span class="math inline">\(X_{t+1}\)</span> given <span class="math inline">\(x_t\)</span>. The algorithm will work well if that conditional distribution can easily be simulated. Let <span class="math inline">\(\pi(\cdot|x_t)\)</span> denote that probability.</p>
<p>Draw a potential value <span class="math inline">\(x_{t+1}^\star\)</span>, and <span class="math inline">\(u\)</span>, from a uniform distribution. Compute <span class="math display">\[
R=  \frac{f(x_{t+1}^\star)}{f(x_t)}
\]</span> and</p>
<ul>
<li>if <span class="math inline">\(u &lt; r\)</span>, then set <span class="math inline">\(x_{t+1}=x_t^\star\)</span></li>
<li>if <span class="math inline">\(u\leq r\)</span>, then set <span class="math inline">\(x_{t+1}=x_t\)</span></li>
</ul>
<p>Here <span class="math inline">\(r\)</span> is called the <em>acceptance</em>-ratio: we accept the new value with probability <span class="math inline">\(r\)</span> (or actually the smallest between <span class="math inline">\(1\)</span> and <span class="math inline">\(r\)</span> since <span class="math inline">\(r\)</span> can exceed <span class="math inline">\(1\)</span>).</p>
<p>For instance, assume that <span class="math inline">\(f(\cdot|x_t)\)</span> is uniform on <span class="math inline">\([x_t-\varepsilon,x_t+\varepsilon]\)</span> for some <span class="math inline">\(\varepsilon&gt;0\)</span>, and where <span class="math inline">\(f\)</span> (our target distribution) is the <span class="math inline">\(\mathcal{N}(0,1)\)</span>. We will never <em>draw</em> from <span class="math inline">\(f\)</span>, but we will use it to compute our acceptance ratio at each step.</p>
<h5 style="text-align: center;">
<a id="displayCode.MCMC.1" href="javascript:togglecode('toggleCode.MCMC.1','displayCode.MCMC.1');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleCode.MCMC.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">metrop1 &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n=</span><span class="dv">1000</span>,<span class="dt">eps=</span><span class="fl">0.5</span>){
 vec &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, n, <span class="dv">3</span>)
 x=<span class="dv">0</span>
 vec[<span class="dv">1</span>] &lt;-<span class="st"> </span>x
 <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n) {
 innov &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>,<span class="op">-</span>eps,eps)
 mov &lt;-<span class="st"> </span>x<span class="op">+</span>innov
 R &lt;-<span class="st"> </span><span class="kw">min</span>(<span class="dv">1</span>,<span class="kw">dnorm</span>(mov)<span class="op">/</span><span class="kw">dnorm</span>(x))
 u &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
 <span class="cf">if</span> (u <span class="op">&lt;</span><span class="st"> </span>R) x &lt;-<span class="st"> </span>mov
 vec[i,] &lt;-<span class="st"> </span><span class="kw">c</span>(x,mov,R)
 }
<span class="kw">return</span>(vec)}</code></pre></div>
</div>
<p>In the code above, <code>vec</code> contains values of <span class="math inline">\(\boldsymbol{x}=(x_1,x_2,\cdots)\)</span>, <code>innov</code> is the innovation.</p>
<h5 style="text-align: center;">
<a id="displayCode.MCMC.2" href="javascript:togglecode('toggleCode.MCMC.2','displayCode.MCMC.2');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleCode.MCMC.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#install.packages(&#39;gifski&#39;)</span>
<span class="co">#if (packageVersion(&#39;knitr&#39;) &lt; &#39;1.20.14&#39;) {</span>
<span class="co">#  remotes::install_github(&#39;yihui/knitr&#39;)</span>
<span class="co">#}</span>
vec &lt;-<span class="st"> </span><span class="kw">metrop1</span>(<span class="dv">25</span>)
u=<span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">by=</span>.<span class="dv">01</span>)
pic_ani =<span class="st"> </span><span class="cf">function</span>(k){
  <span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span>k,vec[<span class="dv">1</span><span class="op">:</span>k,<span class="dv">1</span>],<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">25</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;&quot;</span>)
    <span class="cf">if</span>(vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">1</span>]<span class="op">==</span>vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>]) <span class="kw">points</span>(k<span class="op">+</span><span class="dv">1</span>,vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">19</span>)
    <span class="cf">if</span>(vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">1</span>]<span class="op">!=</span>vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>]) <span class="kw">points</span>(k<span class="op">+</span><span class="dv">1</span>,vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">19</span>)
  <span class="kw">points</span>(k<span class="op">+</span><span class="dv">1</span>,vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>],<span class="dt">cex=</span><span class="fl">1.5</span>)
  <span class="kw">arrows</span>(k<span class="op">+</span><span class="dv">1</span>,vec[k,<span class="dv">1</span>]<span class="op">-</span>.<span class="dv">5</span>,k<span class="op">+</span><span class="dv">1</span>,vec[k,<span class="dv">1</span>]<span class="op">+</span>.<span class="dv">5</span>,<span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">angle=</span><span class="dv">90</span>,<span class="dt">code =</span> <span class="dv">3</span>,<span class="dt">length=</span>.<span class="dv">1</span>)
  <span class="kw">polygon</span>(<span class="kw">c</span>(k<span class="op">+</span><span class="kw">dnorm</span>(u)<span class="op">*</span><span class="dv">10</span>,<span class="kw">rep</span>(k,<span class="kw">length</span>(u))),<span class="kw">c</span>(u,<span class="kw">rev</span>(u)),<span class="dt">col=</span><span class="kw">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,.<span class="dv">3</span>),
          <span class="dt">border=</span><span class="ot">NA</span>)  
  <span class="kw">segments</span>(k,vec[k,<span class="dv">1</span>],k<span class="op">+</span><span class="kw">dnorm</span>(vec[k,<span class="dv">1</span>])<span class="op">*</span><span class="dv">10</span>,vec[k,<span class="dv">1</span>])
  <span class="kw">segments</span>(k,vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>],k<span class="op">+</span><span class="kw">dnorm</span>(vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>])<span class="op">*</span><span class="dv">10</span>,vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">2</span>])
  <span class="kw">text</span>(k,<span class="dv">2</span>,<span class="kw">round</span>(vec[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">3</span>],<span class="dt">digits=</span><span class="dv">3</span>))
}</code></pre></div>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">23</span>) {<span class="kw">pic_ani</span>(k)}</code></pre></div>
<p><img src="LossDataAnalytics_files/figure-html/sampleani_HM_1-.gif" width="672" /></p>
<p>Now, if we use more simulations, we get</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vec &lt;-<span class="st"> </span><span class="kw">metrop1</span>(<span class="dv">10000</span>)
simx &lt;-<span class="st"> </span>vec[<span class="dv">1000</span><span class="op">:</span><span class="dv">10000</span>,<span class="dv">1</span>]
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>))
<span class="kw">plot</span>(simx,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)
<span class="kw">hist</span>(simx,<span class="dt">probability =</span> <span class="ot">TRUE</span>,<span class="dt">col=</span><span class="st">&quot;light blue&quot;</span>,<span class="dt">border=</span><span class="st">&quot;white&quot;</span>)
<span class="kw">lines</span>(u,<span class="kw">dnorm</span>(u),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">qqnorm</span>(simx)
<span class="kw">acf</span>(simx,<span class="dt">lag=</span><span class="dv">100</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;light blue&quot;</span>)</code></pre></div>
<p><img src="LossDataAnalytics_files/figure-html/unnamed-chunk-208-1.png" width="672" /></p>
</div>
<div id="gibbs-sampler" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Gibbs sampler</h3>
<p>Consider some vector <span class="math inline">\(\boldsymbol{X}=(X_1,\cdots,X_d)\)</span> with ind'ependent components, <span class="math inline">\(X_i\sim\mathcal{E}(\lambda_i)\)</span>. We sample to sample from <span class="math inline">\(\boldsymbol{X}\)</span> given <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{1}&gt;s\)</span> for some threshold <span class="math inline">\(s&gt;0\)</span>.</p>
<ul>
<li>start with some starting point <span class="math inline">\(\boldsymbol{x}_0\)</span> such that</li>
<li>pick up (randomly) <span class="math inline">\(i\in\{1,\cdots,d\}\)</span></li>
<li><span class="math inline">\(X_i\)</span> given <span class="math inline">\(X_i &gt; s-\boldsymbol{x}_{(-i)}^T\boldsymbol{1}\)</span> has an Exponential distribution <span class="math inline">\(\mathcal{E}(\lambda_i)\)</span></li>
<li>draw <span class="math inline">\(Y\sim \mathcal{E}(\lambda_i)\)</span> and set <span class="math inline">\(x_i=y +(s-\boldsymbol{x}_{(-i)}^T\boldsymbol{1})_+\)</span> until <span class="math inline">\(\boldsymbol{x}_{(-i)}^T\boldsymbol{1}+x_i&gt;s\)</span></li>
</ul>
<h5 style="text-align: center;">
<a id="displayCode.Gibbs.1" href="javascript:togglecode('toggleCode.Gibbs.1','displayCode.Gibbs.1');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleCode.Gibbs.1" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim &lt;-<span class="st"> </span><span class="ot">NULL</span>
 lambda &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)
 X &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>)
 s &lt;-<span class="st"> </span><span class="dv">5</span>
 <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>){
 i &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dv">1</span>)
 X[i] &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1</span>,lambda[i])<span class="op">+</span><span class="kw">max</span>(<span class="dv">0</span>,s<span class="op">-</span><span class="kw">sum</span>(X[<span class="op">-</span>i]))
 <span class="cf">while</span>(<span class="kw">sum</span>(X)<span class="op">&lt;</span>s){
 X[i] &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1</span>,lambda[i])<span class="op">+</span><span class="kw">max</span>(<span class="dv">0</span>,s<span class="op">-</span><span class="kw">sum</span>(X[<span class="op">-</span>i])) }
 sim &lt;-<span class="st"> </span><span class="kw">rbind</span>(sim,X) }</code></pre></div>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(sim,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">11</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">4.3</span>))
<span class="kw">polygon</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>,<span class="op">-</span><span class="dv">1</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">density=</span><span class="dv">15</span>,<span class="dt">border=</span><span class="ot">NA</span>)
<span class="kw">abline</span>(<span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="LossDataAnalytics_files/figure-html/unnamed-chunk-210-1.png" width="672" /></p>
<p>The construction of the sequence (<a href="#" class="tooltip" style="color:green"><em>MCMC</em><span style="font-size:8pt">Markov Chain Monte Carlo</span></a> algorithms are iterative) can be visualized below</p>
<h5 style="text-align: center;">
<a id="displayCode.Gibbs.2" href="javascript:togglecode('toggleCode.Gibbs.2','displayCode.Gibbs.2');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleCode.Gibbs.2" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambda &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)
X &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>)
sim &lt;-<span class="st"> </span>X
s &lt;-<span class="st"> </span><span class="dv">5</span>
<span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>){
 <span class="kw">set.seed</span>(k)
 i &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dv">1</span>)
 X[i] &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1</span>,lambda[i])<span class="op">+</span><span class="kw">max</span>(<span class="dv">0</span>,s<span class="op">-</span><span class="kw">sum</span>(X[<span class="op">-</span>i]))
 <span class="cf">while</span>(<span class="kw">sum</span>(X)<span class="op">&lt;</span>s){
 X[i] &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1</span>,lambda[i])<span class="op">+</span><span class="kw">max</span>(<span class="dv">0</span>,s<span class="op">-</span><span class="kw">sum</span>(X[<span class="op">-</span>i])) }
 sim &lt;-<span class="st"> </span><span class="kw">rbind</span>(sim,X) }
pic_ani =<span class="st"> </span><span class="cf">function</span>(n){
<span class="kw">plot</span>(sim[<span class="dv">1</span><span class="op">:</span>n,],<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">11</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>),<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;&quot;</span>)
i=<span class="kw">which</span>(<span class="kw">apply</span>(sim[(n<span class="op">-</span><span class="dv">1</span>)<span class="op">:</span>n,],<span class="dv">2</span>,diff)<span class="op">==</span><span class="dv">0</span>)
<span class="cf">if</span>(i<span class="op">==</span><span class="dv">1</span>) <span class="kw">abline</span>(<span class="dt">v=</span>sim[n,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)
<span class="cf">if</span>(i<span class="op">==</span><span class="dv">2</span>) <span class="kw">abline</span>(<span class="dt">h=</span>sim[n,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)
<span class="cf">if</span>(n<span class="op">&gt;=</span><span class="dv">1</span>) <span class="kw">points</span>(sim[n,<span class="dv">1</span>],sim[n,<span class="dv">2</span>],<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">cex=</span><span class="fl">1.4</span>)
<span class="cf">if</span>(n<span class="op">&gt;=</span><span class="dv">2</span>) <span class="kw">points</span>(sim[n<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>],sim[n<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>],<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">cex=</span><span class="fl">1.4</span>)
<span class="kw">polygon</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">6</span>,<span class="op">-</span><span class="dv">1</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">density=</span><span class="dv">15</span>,<span class="dt">border=</span><span class="ot">NA</span>)
<span class="kw">abline</span>(<span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">100</span>) {<span class="kw">pic_ani</span>(i)}</code></pre></div>
<p><img src="LossDataAnalytics_files/figure-html/sampleani_HM-.gif" width="672" /></p>
</div>
</div>
<div id="Simulation:further-reading-and-resources" class="section level2">
<h2><span class="header-section-number">6.6</span> Further Resources and Contributors</h2>
<ul>
<li>Include historical references for jackknife (Quenouille, Tukey, Efron)</li>
<li>Here are some links to learn more about <a href="https://freakonometrics.hypotheses.org/6470">reproducibility and randomness</a> and how to go <a href="https://freakonometrics.hypotheses.org/6638">from a random generator to a sample function</a>.</li>
</ul>
<div id="contributors-5" class="section level4 unnumbered">
<h4>Contributors</h4>
<ul>
<li><strong>Arthur Charpentier</strong>, Université du Quebec á Montreal, and <strong>Edward W. (Jed) Frees</strong>, University of Wisconsin-Madison, are the principal authors of the initial version of this chapter. Email: <a href="mailto:jfrees@bus.wisc.edu">jfrees@bus.wisc.edu</a> and/or <a href="mailto:arthur.charpentier@gmail.com">arthur.charpentier@gmail.com</a> for chapter comments and suggested improvements.</li>
<li>Chapter reviewers include: Write Jed or Arthur; to add you name here.</li>
</ul>
</div>
<div id="ts-6.a.-bootstrap-applications-in-predictive-modeling" class="section level3">
<h3><span class="header-section-number">6.6.1</span> TS 6.A. Bootstrap Applications in Predictive Modeling</h3>
<hr />
<p><strong>This section is being written.</strong></p>
<hr />

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C-AggLossModels.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C-PremiumFoundations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["LossDataAnalytics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
