<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Loss Data Analytics</title>
  <meta name="description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="<a href="https://github.com/openacttexts/Loss-Data-Analytics" class="uri">https://github.com/openacttexts/Loss-Data-Analytics</a>" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Loss Data Analytics" />
  
  <meta name="twitter:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="C-Severity.html">
<link rel="next" href="C-AggLossModels.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
</script>

<script language="javascript">
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
</script>
<script language="javascript">
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
</script>
<script language="javascript">
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
</script>

<script language="javascript">
$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});
</script>


<script>
$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="C-Intro.html"><a href="C-Intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="C-Intro.html"><a href="C-Intro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="C-Intro.html"><a href="C-Intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.1</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.2" data-path="C-Intro.html"><a href="C-Intro.html#short-and-long-term-insurance"><i class="fa fa-check"></i><b>1.1.2</b> Short and Long-term Insurance</a></li>
<li class="chapter" data-level="1.1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="C-Intro.html"><a href="C-Intro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="C-Intro.html"><a href="C-Intro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="C-Intro.html"><a href="C-Intro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="C-Intro.html"><a href="C-Intro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="C-Intro.html"><a href="C-Intro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.3.1" data-path="C-Intro.html"><a href="C-Intro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="C-Intro.html"><a href="C-Intro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="C-Intro.html"><a href="C-Intro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="C-Intro.html"><a href="C-Intro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html"><i class="fa fa-check"></i><b>2</b> Frequency Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>2.1</b> Frequency Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Basic Frequency Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>2.2.1</b> Foundations</a></li>
<li class="chapter" data-level="2.2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>2.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> The (a, b, 0) Class</a></li>
<li class="chapter" data-level="2.4" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimating Frequency Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>2.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Frequency Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="2.7" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="2.8" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#r-code-for-plots-in-this-chapter"><i class="fa fa-check"></i><b>2.9</b> R Code for Plots in this Chapter</a></li>
<li class="chapter" data-level="2.10" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>2.10</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C-Severity.html"><a href="C-Severity.html"><i class="fa fa-check"></i><b>3</b> Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.1" data-path="C-Severity.html"><a href="C-Severity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Basic Distributional Quantities</a><ul>
<li class="chapter" data-level="3.1.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>3.1.1</b> Moments</a></li>
<li class="chapter" data-level="3.1.2" data-path="C-Severity.html"><a href="C-Severity.html#quantiles"><i class="fa fa-check"></i><b>3.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="C-Severity.html"><a href="C-Severity.html#moment-generating-function"><i class="fa fa-check"></i><b>3.1.3</b> Moment Generating Function</a></li>
<li class="chapter" data-level="3.1.4" data-path="C-Severity.html"><a href="C-Severity.html#probability-generating-function"><i class="fa fa-check"></i><b>3.1.4</b> Probability Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C-Severity.html"><a href="C-Severity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions for Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="C-Severity.html"><a href="C-Severity.html#gamma-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="C-Severity.html"><a href="C-Severity.html#pareto-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="C-Severity.html"><a href="C-Severity.html#weibull-distribution"><i class="fa fa-check"></i><b>3.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="C-Severity.html"><a href="C-Severity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>3.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C-Severity.html"><a href="C-Severity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Methods of Creating New Distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="C-Severity.html"><a href="C-Severity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="3.3.2" data-path="C-Severity.html"><a href="C-Severity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>3.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="3.3.3" data-path="C-Severity.html"><a href="C-Severity.html#raising-to-a-power"><i class="fa fa-check"></i><b>3.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="3.3.4" data-path="C-Severity.html"><a href="C-Severity.html#exponentiation"><i class="fa fa-check"></i><b>3.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="3.3.5" data-path="C-Severity.html"><a href="C-Severity.html#finite-mixtures"><i class="fa fa-check"></i><b>3.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="3.3.6" data-path="C-Severity.html"><a href="C-Severity.html#continuous-mixtures"><i class="fa fa-check"></i><b>3.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="C-Severity.html"><a href="C-Severity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>3.4</b> Coverage Modifications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="3.4.2" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="3.4.3" data-path="C-Severity.html"><a href="C-Severity.html#coinsurance"><i class="fa fa-check"></i><b>3.4.3</b> Coinsurance</a></li>
<li class="chapter" data-level="3.4.4" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C-Severity.html"><a href="C-Severity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>3.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="3.5.2" data-path="C-Severity.html"><a href="C-Severity.html#MLEGrouped"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Likelihood Estimators for Grouped Data</a></li>
<li class="chapter" data-level="3.5.3" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-censored-data"><i class="fa fa-check"></i><b>3.5.3</b> Maximum Likelihood Estimators for Censored Data</a></li>
<li class="chapter" data-level="3.5.4" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-truncated-data"><i class="fa fa-check"></i><b>3.5.4</b> Maximum Likelihood Estimators for Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C-Severity.html"><a href="C-Severity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>3.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html"><i class="fa fa-check"></i><b>4</b> Model Selection and Estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation"><i class="fa fa-check"></i><b>4.1.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.1.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>4.1.2</b> Tools for Model Selection and Diagnostics</a></li>
<li class="chapter" data-level="4.1.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#starting-values"><i class="fa fa-check"></i><b>4.1.3</b> Starting Values</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>4.2</b> Model Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#iterative-model-selection"><i class="fa fa-check"></i><b>4.2.1</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="4.2.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-training-dataset"><i class="fa fa-check"></i><b>4.2.2</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="4.2.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>4.2.3</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="4.2.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-cross-validation"><i class="fa fa-check"></i><b>4.2.4</b> Model Selection Based on Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>4.3</b> Estimation using Modified Data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#parametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.1</b> Parametric Estimation using Modified Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.2</b> Nonparametric Estimation using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:BayesInference"><i class="fa fa-check"></i><b>4.4</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:IntroBayes"><i class="fa fa-check"></i><b>4.4.1</b> Introduction to Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-model"><i class="fa fa-check"></i><b>4.4.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-inference"><i class="fa fa-check"></i><b>4.4.3</b> Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>4.4.4</b> Conjugate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#technical-supplement-a.-gini-statistic"><i class="fa fa-check"></i>Technical Supplement A. Gini Statistic</a><ul>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#ts-a.1.-the-classic-lorenz-curve"><i class="fa fa-check"></i>TS A.1. The Classic Lorenz Curve</a></li>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#ts-a.2.-ordered-lorenz-curve-and-the-gini-index"><i class="fa fa-check"></i>TS A.2. Ordered Lorenz Curve and the Gini Index</a></li>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#ts-a.3.-out-of-sample-validation"><i class="fa fa-check"></i>TS A.3. Out-of-Sample Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html"><i class="fa fa-check"></i><b>5</b> Aggregate Loss Models</a><ul>
<li class="chapter" data-level="5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>5.2</b> Individual Risk Model</a></li>
<li class="chapter" data-level="5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#collective-risk-model"><i class="fa fa-check"></i><b>5.3</b> Collective Risk Model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>5.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>5.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="5.3.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#analytic-results"><i class="fa fa-check"></i><b>5.3.3</b> Analytic Results</a></li>
<li class="chapter" data-level="5.3.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#tweedie-distribution"><i class="fa fa-check"></i><b>5.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>5.4</b> Computing the Aggregate Claims Distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>5.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="5.4.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#simulation"><i class="fa fa-check"></i><b>5.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>5.5</b> Effects of Coverage Modifications</a><ul>
<li class="chapter" data-level="5.5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>5.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="5.5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>5.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="5.5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>5.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>5.6</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#technical-supplement-b.-aggregate-loss-models"><i class="fa fa-check"></i>Technical Supplement B. Aggregate Loss Models</a><ul>
<li class="chapter" data-level="" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#ts-b.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS B.1. Individual Risk Model Properties</a></li>
<li><a href="C-AggLossModels.html#ts-b.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it">TS B.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li><a href="C-AggLossModels.html#ts-b.3.-example-5.3.8-moment-generating-function-of-aggregate-loss-s_n">TS B.3. Example 5.3.8 Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="C-Simulation.html"><a href="C-Simulation.html"><i class="fa fa-check"></i><b>6</b> Simulation</a><ul>
<li class="chapter" data-level="6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>6.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="6.2" data-path="C-Simulation.html"><a href="C-Simulation.html#inverse-transform"><i class="fa fa-check"></i><b>6.2</b> Inverse Transform</a></li>
<li class="chapter" data-level="6.3" data-path="C-Simulation.html"><a href="C-Simulation.html#how-many-simulated-values"><i class="fa fa-check"></i><b>6.3</b> How Many Simulated Values?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C-PremCalc.html"><a href="C-PremCalc.html"><i class="fa fa-check"></i><b>7</b> Premium Calculation Fundamentals</a></li>
<li class="chapter" data-level="8" data-path="C-RiskClass.html"><a href="C-RiskClass.html"><i class="fa fa-check"></i><b>8</b> Risk Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>8.2</b> Poisson Regression Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>8.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>8.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>8.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="8.2.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#exercises-4"><i class="fa fa-check"></i><b>8.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>8.3</b> Categorical Variables and Multiplicative Tariff</a><ul>
<li class="chapter" data-level="8.3.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>8.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="8.3.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>8.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="8.3.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>8.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="8.3.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>8.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>8.4</b> Contributors and Further Resources</a></li>
<li class="chapter" data-level="8.5" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:mle-Pois-reg"><i class="fa fa-check"></i><b>8.5</b> Technical Supplement – Estimating Poisson Regression Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C-Credibility.html"><a href="C-Credibility.html"><i class="fa fa-check"></i><b>9</b> Experience Rating Using Credibility Theory</a><ul>
<li class="chapter" data-level="9.1" data-path="C-Credibility.html"><a href="C-Credibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>9.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="9.2" data-path="C-Credibility.html"><a href="C-Credibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.2</b> Limited Fluctuation Credibility</a><ul>
<li class="chapter" data-level="9.2.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:frequency"><i class="fa fa-check"></i><b>9.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="9.2.2" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>9.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="9.2.3" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>9.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="9.2.4" data-path="C-Credibility.html"><a href="C-Credibility.html#partial-credibility"><i class="fa fa-check"></i><b>9.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-credibility"><i class="fa fa-check"></i><b>9.3</b> Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.3.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>9.3.1</b> Credibility Z, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-straub-credibility"><i class="fa fa-check"></i><b>9.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="9.5" data-path="C-Credibility.html"><a href="C-Credibility.html#bayesian-inference-and-buhlmann"><i class="fa fa-check"></i><b>9.5</b> Bayesian Inference and Bühlmann</a><ul>
<li class="chapter" data-level="9.5.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:Gamma-Poisson"><i class="fa fa-check"></i><b>9.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="9.5.2" data-path="C-Credibility.html"><a href="C-Credibility.html#exact-credibility"><i class="fa fa-check"></i><b>9.5.2</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="C-Credibility.html"><a href="C-Credibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>9.6</b> Estimating Credibility Parameters</a><ul>
<li class="chapter" data-level="9.6.1" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="9.6.2" data-path="C-Credibility.html"><a href="C-Credibility.html#nonparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.2</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.3" data-path="C-Credibility.html"><a href="C-Credibility.html#semiparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.3</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.4" data-path="C-Credibility.html"><a href="C-Credibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>9.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="C-Credibility.html"><a href="C-Credibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C-PortMgt.html"><a href="C-PortMgt.html"><i class="fa fa-check"></i><b>10</b> Insurance Portfolio Management including Reinsurance</a><ul>
<li class="chapter" data-level="" data-path="C-PortMgt.html"><a href="C-PortMgt.html#overview"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="10.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Tails"><i class="fa fa-check"></i><b>10.1</b> Tails of Distributions</a><ul>
<li class="chapter" data-level="10.1.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>10.1.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="10.1.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>10.1.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>10.2</b> Risk Measures</a><ul>
<li class="chapter" data-level="10.2.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>10.2.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="10.2.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>10.2.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="10.2.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>10.2.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>10.3</b> Reinsurance</a><ul>
<li class="chapter" data-level="10.3.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>10.3.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.3.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>10.3.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.3.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>10.3.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C-LossReserves.html"><a href="C-LossReserves.html"><i class="fa fa-check"></i><b>11</b> Loss Reserving</a></li>
<li class="chapter" data-level="12" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html"><i class="fa fa-check"></i><b>12</b> Experience Rating using Bonus-Malus</a></li>
<li class="chapter" data-level="13" data-path="C-DataSystems.html"><a href="C-DataSystems.html"><i class="fa fa-check"></i><b>13</b> Data Systems</a><ul>
<li class="chapter" data-level="13.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data"><i class="fa fa-check"></i><b>13.1</b> Data</a><ul>
<li class="chapter" data-level="13.1.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-types-and-sources"><i class="fa fa-check"></i><b>13.1.1</b> Data Types and Sources</a></li>
<li class="chapter" data-level="13.1.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-structures-and-storage"><i class="fa fa-check"></i><b>13.1.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="13.1.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-quality"><i class="fa fa-check"></i><b>13.1.3</b> Data Quality</a></li>
<li class="chapter" data-level="13.1.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-cleaning"><i class="fa fa-check"></i><b>13.1.4</b> Data Cleaning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-preliminary"><i class="fa fa-check"></i><b>13.2</b> Data Analysis Preliminary</a><ul>
<li class="chapter" data-level="13.2.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:process"><i class="fa fa-check"></i><b>13.2.1</b> Data Analysis Process</a></li>
<li class="chapter" data-level="13.2.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-versus-confirmatory"><i class="fa fa-check"></i><b>13.2.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="13.2.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>13.2.3</b> Supervised versus Unsupervised</a></li>
<li class="chapter" data-level="13.2.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#parametric-versus-nonparametric"><i class="fa fa-check"></i><b>13.2.4</b> Parametric versus Nonparametric</a></li>
<li class="chapter" data-level="13.2.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:expred"><i class="fa fa-check"></i><b>13.2.5</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="13.2.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-modeling-versus-algorithmic-modeling"><i class="fa fa-check"></i><b>13.2.6</b> Data Modeling versus Algorithmic Modeling</a></li>
<li class="chapter" data-level="13.2.7" data-path="C-DataSystems.html"><a href="C-DataSystems.html#big-data-analysis"><i class="fa fa-check"></i><b>13.2.7</b> Big Data Analysis</a></li>
<li class="chapter" data-level="13.2.8" data-path="C-DataSystems.html"><a href="C-DataSystems.html#reproducible-analysis"><i class="fa fa-check"></i><b>13.2.8</b> Reproducible Analysis</a></li>
<li class="chapter" data-level="13.2.9" data-path="C-DataSystems.html"><a href="C-DataSystems.html#ethical-issues"><i class="fa fa-check"></i><b>13.2.9</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-techniques"><i class="fa fa-check"></i><b>13.3</b> Data Analysis Techniques</a><ul>
<li class="chapter" data-level="13.3.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-techniques"><i class="fa fa-check"></i><b>13.3.1</b> Exploratory Techniques</a></li>
<li class="chapter" data-level="13.3.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#descriptive-statistics"><i class="fa fa-check"></i><b>13.3.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="13.3.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#cluster-analysis"><i class="fa fa-check"></i><b>13.3.3</b> Cluster Analysis</a></li>
<li class="chapter" data-level="13.3.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#confirmatory-techniques"><i class="fa fa-check"></i><b>13.3.4</b> Confirmatory Techniques</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#some-r-functions"><i class="fa fa-check"></i><b>13.4</b> Some R Functions</a></li>
<li class="chapter" data-level="13.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html"><i class="fa fa-check"></i><b>14</b> Dependence Modeling</a><ul>
<li class="chapter" data-level="14.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:VarTypes"><i class="fa fa-check"></i><b>14.1</b> Variable Types</a><ul>
<li class="chapter" data-level="14.1.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuaVar"><i class="fa fa-check"></i><b>14.1.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuanVar"><i class="fa fa-check"></i><b>14.1.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>14.1.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>14.2</b> Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>14.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>14.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>14.2.3</b> Nominal Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>14.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="14.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>14.4</b> Application Using Copulas</a><ul>
<li class="chapter" data-level="14.4.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#data-description"><i class="fa fa-check"></i><b>14.4.1</b> Data Description</a></li>
<li class="chapter" data-level="14.4.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>14.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="14.4.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>14.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="14.4.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>14.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>14.5</b> Types of Copulas</a><ul>
<li class="chapter" data-level="14.5.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#elliptical-copulas"><i class="fa fa-check"></i><b>14.5.1</b> Elliptical Copulas</a></li>
<li class="chapter" data-level="14.5.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#archimedian-copulas"><i class="fa fa-check"></i><b>14.5.2</b> Archimedian Copulas</a></li>
<li class="chapter" data-level="14.5.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>14.5.3</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>14.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="14.7" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>14.7</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#technical-supplement-a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>Technical Supplement A. Other Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#a.1.-blomqvists-beta"><i class="fa fa-check"></i>A.1. Blomqvist’s Beta</a></li>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#a.2.-nonparametric-approach-using-spearman-correlation-with-tied-ranks"><i class="fa fa-check"></i>A.2. Nonparametric Approach Using Spearman Correlation with Tied Ranks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C-AppA.html"><a href="C-AppA.html"><i class="fa fa-check"></i><b>15</b> Appendix A: Review of Statistical Inference</a><ul>
<li class="chapter" data-level="15.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>15.1</b> Basic Concepts</a><ul>
<li class="chapter" data-level="15.1.1" data-path="C-AppA.html"><a href="C-AppA.html#random-sampling"><i class="fa fa-check"></i><b>15.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="15.1.2" data-path="C-AppA.html"><a href="C-AppA.html#sampling-distribution"><i class="fa fa-check"></i><b>15.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="15.1.3" data-path="C-AppA.html"><a href="C-AppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>15.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>15.2</b> Point Estimation and Properties</a><ul>
<li class="chapter" data-level="15.2.1" data-path="C-AppA.html"><a href="C-AppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>15.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="15.2.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>15.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>15.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="15.3.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>15.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="15.3.2" data-path="C-AppA.html"><a href="C-AppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>15.3.2</b> Large-sample Properties of MLE</a></li>
<li class="chapter" data-level="15.3.3" data-path="C-AppA.html"><a href="C-AppA.html#confidence-interval"><i class="fa fa-check"></i><b>15.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>15.4</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="15.4.1" data-path="C-AppA.html"><a href="C-AppA.html#basic-concepts"><i class="fa fa-check"></i><b>15.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="15.4.2" data-path="C-AppA.html"><a href="C-AppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>15.4.2</b> Student-<span class="math inline">\(t\)</span> test based on MLE</a></li>
<li class="chapter" data-level="15.4.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>15.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="15.4.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>15.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C-AppB.html"><a href="C-AppB.html"><i class="fa fa-check"></i><b>16</b> Appendix B: Iterated Expectations</a><ul>
<li class="chapter" data-level="16.1" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>16.1</b> Conditional Distribution and Conditional Expectation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="C-AppB.html"><a href="C-AppB.html#conditional-distribution"><i class="fa fa-check"></i><b>16.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="16.1.2" data-path="C-AppB.html"><a href="C-AppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>16.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>16.2</b> Iterated Expectations and Total Variance</a><ul>
<li class="chapter" data-level="16.2.1" data-path="C-AppB.html"><a href="C-AppB.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>16.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="16.2.2" data-path="C-AppB.html"><a href="C-AppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>16.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="C-AppB.html"><a href="C-AppB.html#application"><i class="fa fa-check"></i><b>16.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>16.3</b> Conjugate Distributions</a><ul>
<li class="chapter" data-level="16.3.1" data-path="C-AppB.html"><a href="C-AppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>16.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="16.3.2" data-path="C-AppB.html"><a href="C-AppB.html#conjugate-distributions"><i class="fa fa-check"></i><b>16.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C-AppC.html"><a href="C-AppC.html"><i class="fa fa-check"></i><b>17</b> Appendix C: Maximum Likelihood Theory</a><ul>
<li class="chapter" data-level="17.1" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>17.1</b> Likelihood Function</a><ul>
<li class="chapter" data-level="17.1.1" data-path="C-AppC.html"><a href="C-AppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="17.1.2" data-path="C-AppC.html"><a href="C-AppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>17.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>17.2</b> Maximum Likelihood Estimators</a><ul>
<li class="chapter" data-level="17.2.1" data-path="C-AppC.html"><a href="C-AppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>17.2.1</b> Definition and Derivation of MLE</a></li>
<li class="chapter" data-level="17.2.2" data-path="C-AppC.html"><a href="C-AppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotic Properties of MLE</a></li>
<li class="chapter" data-level="17.2.3" data-path="C-AppC.html"><a href="C-AppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>17.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>17.3</b> Statistical Inference Based on Maximum Likelhood Estimation</a><ul>
<li class="chapter" data-level="17.3.1" data-path="C-AppC.html"><a href="C-AppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>17.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="17.3.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>17.3.2</b> MLE and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://openacttexts.github.io/Loss-Data-Analytics/" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C:ModelSelection" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Model Selection and Estimation</h1>
<p><em>Chapter Preview</em>. Chapters <a href="C-Frequency-Modeling.html#C:Frequency-Modeling">2</a> and <a href="C-Severity.html#C:Severity">3</a> have described how to fit parametric models to frequency and severity data, respectively. This chapter begins with the selection of models. To compare alternative parametric models, it is helpful to summarize data without reference to a specific parametric distribution. Section <a href="C-ModelSelection.html#S:MS:NonParInf">4.1</a> describes nonparametric estimation, how we can use it for model comparisons and how it can be used to provide starting values for parametric procedures. The process of model selection is then summarized in Section <a href="C-ModelSelection.html#S:MS:ModelSelection">4.2</a>. Although our focus is on continuous data, the same process can be used for discrete data or data that come from a hybrid combination of discrete and continuous data.</p>
<p>Model selection and estimation are fundamental aspects of statistical modeling. To provide a flavor as to how they can be adapted to alternative sampling schemes, Section <a href="C-ModelSelection.html#S:MS:ModifiedData">4.3</a> describes estimation for grouped, censored and truncated data (following the Section <a href="C-Severity.html#S:MaxLikeEstimation">3.5</a> introduction). To see how they can be adapted to alternative models, the chapter closes with Section <a href="C-ModelSelection.html#S:MS:BayesInference">4.4</a> on Bayesian inference, an alternative procedure where the (typically unknown) parameters are treated as random variables.</p>
<div id="S:MS:NonParInf" class="section level2">
<h2><span class="header-section-number">4.1</span> Nonparametric Inference</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Estimate moments, quantiles, and distributions without reference to a parametric distribution</li>
<li>Summarize the data graphically without reference to a parametric distribution</li>
<li>Determine measures that summarize deviations of a parametric from a nonparametric fit</li>
<li>Use nonparametric estimators to approximate parameters that can be used to start a parametric estimation procedure</li>
</ul>
<hr />
<div id="nonparametric-estimation" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Nonparametric Estimation</h3>
<p>In Section <a href="C-Frequency-Modeling.html#S:basic-frequency-distributions">2.2</a> for frequency and Section <a href="C-Severity.html#S:BasicQuantities">3.1</a> for severity, we learned how to summarize a distribution by computing means, variances, quantiles/percentiles, and so on. To approximate these summary measures using a dataset, one strategy is to:</p>
<ol style="list-style-type: lower-roman">
<li>assume a parametric form for a distribution, such as a negative binomial for frequency or a gamma distribution for severity,</li>
<li>estimate the parameters of that distribution, and then</li>
<li>use the distribution with the estimated parameters to calculate the desired summary measure.</li>
</ol>
<p>This is the <strong>parametric</strong> approach. Another strategy is to estimate the desired summary measure directly from the observations <em>without</em> reference to a parametric model. Not surprisingly, this is known as the <a href="#" class="tooltip" style="color:green"><strong>nonparametric</strong><span style="font-size:8pt"> An approach to inference that does not rely on references to a parametric model.</span></a> approach.</p>
<p>Let us start by considering the most basic type of sampling scheme and assume that observations are realizations from a set of random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> that are <a href="#" class="tooltip" style="color:green"><em>iid</em><span style="font-size:8pt"> identically and independently distributed</span></a> draws from an unknown population distribution <span class="math inline">\(F(\cdot)\)</span>. An equivalent way of saying this is that <span class="math inline">\(X_1, \ldots, X_n\)</span>, is a <em>random sample</em> (with replacement) from <span class="math inline">\(F(\cdot)\)</span>. To see how this works, we now describe nonparametric estimators of many important measures that summarize a distribution.</p>
<div id="S:MS:MomentEstimator" class="section level4">
<h4><span class="header-section-number">4.1.1.1</span> Moment Estimators</h4>
<p>We learned how to define moments in Section <a href="C-Frequency-Modeling.html#S:generating-functions">2.2.2</a> for frequency and Section <a href="C-Severity.html#S:Chap3Moments">3.1.1</a> for severity. In particular, the <span class="math inline">\(k\)</span>-th moment, <span class="math inline">\(\mathrm{E~}[X^k] = \mu^{\prime}_k\)</span>, summarizes many aspects of the distribution for different choices of <em>k</em>. Here, <span class="math inline">\(\mu^{\prime}_k\)</span> is sometimes called the <em>k</em>th <em>population</em> moment to distinguish it from the <em>k</em>th sample moment, <span class="math display">\[
\frac{1}{n} \sum_{i=1}^n X_i^k ,
\]</span> which is the corresponding nonparametric estimator. In typical applications, <span class="math inline">\(k\)</span> is a positive integer, although it need not be.</p>
<p>An important special case is the first moment where <em>k=1</em>. In this case, the prime symbol (<span class="math inline">\(\prime\)</span>) and the <span class="math inline">\(1\)</span> subscript are usually dropped and one uses <span class="math inline">\(\mu=\mu^{\prime}_1\)</span> to denote the population mean, or simply the <em>mean</em>. The corresponding sample estimator for <span class="math inline">\(\mu\)</span> is called the <em>sample mean</em>, denoted with a bar on top of the random variable: <span class="math display">\[
\bar{X} =\frac{1}{n} \sum_{i=1}^n X_i .
\]</span> Another type of summary measure of interest is the the <span class="math inline">\(k\)</span><em>-th central moment</em>, <span class="math inline">\(\mathrm{E~} [(X-\mu)^k] = \mu_k\)</span>. (Sometimes, <span class="math inline">\(\mu^{\prime}_k\)</span> is called the <span class="math inline">\(k\)</span>-th <em>raw</em> moment to distinguish it from the central moment <span class="math inline">\(\mu_k\)</span>.). A nonparametric, or sample, estimator of <span class="math inline">\(\mu_k\)</span> is <span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \left(X_i - \bar{X}\right)^k .
\]</span> The second central moment (<span class="math inline">\(k=2\)</span>) is an important case for which we typically assign a new symbol, <span class="math inline">\(\sigma^2 = \mathrm{E~} [(X-\mu)^2]\)</span>, known as the <em>variance</em>. Properties of sample moment estimator of the variance such as <span class="math inline">\(n^{-1}\sum_{i=1}^n \left(X_i - \bar{X}\right)^2\)</span> have been studied extensively and so it is natural that many variations have been proposed. The most widely used variation is one where the effective sample size is reduced by one, and so we define <span class="math display">\[
s^2 = \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \bar{X}\right)^2.
\]</span> Here, the statistic <span class="math inline">\(s^2\)</span> known as the <em>sample variance</em>. Dividing by <em>n-1</em> instead of <em>n</em> matters little when you have a sample size <em>n</em> in the thousands as is common in insurance applications. Still, the resulting estimator is unbiased in the sense that <span class="math inline">\(\mathrm{E~} s^2 = \sigma^2\)</span>, a desirable property particularly when interpreting results of an analysis.</p>
</div>
<div id="empirical-distribution-function" class="section level4">
<h4><span class="header-section-number">4.1.1.2</span> Empirical Distribution Function</h4>
<p>We have seen how to compute nonparametric estimators of the <em>k</em>th moment <span class="math inline">\(\mathrm{E~} X^k\)</span>. In the same way, for any known function <span class="math inline">\(\mathrm{g}(\cdot)\)</span>, we can estimate <span class="math inline">\(\mathrm{E~} \mathrm{g}(X)\)</span> using <span class="math inline">\(n^{-1}\sum_{i=1}^n \mathrm{g}(X_i)\)</span>. This is sometimes known as the <em>analog principle</em>.</p>
<p>Now suppose that we fix a value of <em>x</em> and consider the function <span class="math inline">\(\mathrm{g}(X) = I(X \le x)\)</span>. Here, the notation <span class="math inline">\(I(\cdot)\)</span> is the indicator function; it returns 1 if the event <span class="math inline">\((\cdot)\)</span> is true and 0 otherwise. For this choice of <span class="math inline">\(\mathrm{g}(\cdot)\)</span>, the expected value is <span class="math inline">\(\mathrm{E~} I(X \le x) = \Pr(X \le x) = F(x)\)</span>, the distribution function evaluated at a fixed point <em>x</em>. Using the analog principle, we define the nonparametric estimator of the distribution function <span class="math display">\[
\begin{aligned}
F_n(x)
&amp;=  \frac{1}{n} \sum_{i=1}^n I\left(X_i \le x\right) \\
&amp;=  \frac{\text{number of observations less than or equal to }x}{n} . 
\end{aligned}
\]</span> As a nonparametric estimator, <span class="math inline">\(F_n(\cdot)\)</span> is based on only observations and does not assume a parametric family for the distribution, it is also known as the <strong>empirical distribution function</strong>.</p>
<p><strong>Example 4.1.1. Toy Data Set</strong>. To illustrate, consider a fictitious, or “toy,” data set of <span class="math inline">\(n=10\)</span> observations. Determine the empirical distribution function.</p>
<p><span class="math display">\[
{\small
\begin{array}{c|cccccccccc}
\hline
i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\
X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\
\hline
\end{array}
}
\]</span></p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.1" href="javascript:toggleEX('toggleExampleSelect.1.1','displayTextExampleSelect.1.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.1" style="display: none">
<p>You should check that the sample mean is <span class="math inline">\(\bar{X} = 19.7\)</span> and that the sample variance is <span class="math inline">\(s^2 =34.45556\)</span>. The corresponding empirical distribution function is</p>
<p><span class="math display">\[\begin{aligned}
F_n(x) &amp;=
\left\{
\begin{array}{ll}
0 &amp; \text{ for }\ x&lt;10 \\
0.1 &amp; \text{ for }\ 10 \leq x&lt;15 \\
0.4 &amp; \text{ for }\ 15 \leq x&lt;20 \\
0.5 &amp; \text{ for }\ 20 \leq x&lt;23 \\
0.9 &amp; \text{ for }\ 23 \leq x&lt;30 \\
1 &amp; \text{ for }\ x \geq 30,
\end{array}
\right.\end{aligned}\]</span></p>
<p>which is shown in the following graph in Figure <a href="C-ModelSelection.html#fig:EDFToy">4.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:EDFToy"></span>
<img src="LossDataAnalytics_files/figure-html/EDFToy-1.png" alt="Empirical Distribution Function of a Toy Example" width="60%" />
<p class="caption">
Figure 4.1: Empirical Distribution Function of a Toy Example
</p>
</div>
<h5 style="text-align: center;">
<a id="displayTextToy.4f" href="javascript:togglecode('toggleToy','displayTextToy.4f');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleToy" style="display: none">
<pre><code>(xExample &lt;- c(10,rep(15,3),20,rep(23,4),30))
PercentilesxExample &lt;- ecdf(xExample)
plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;)</code></pre>
</div>
</div>
<hr />
</div>
<div id="S:MS:QuantileEstimator" class="section level4">
<h4><span class="header-section-number">4.1.1.3</span> Quartiles, Percentiles and Quantiles</h4>
<p>We have already seen the <em>median</em>, which is the number such that approximately half of a data set is below (or above) it. The <strong>first quartile</strong> is the number such that approximately 25% of the data is below it and the <em>third quartile</em> is the number such that approximately 75% of the data is below it. A <span class="math inline">\(100p\)</span> <strong>percentile</strong> is the number such that <span class="math inline">\(100 \times p\)</span> percent of the data is below it.</p>
<p>To generalize this concept, consider a distribution function <span class="math inline">\(F(\cdot)\)</span>, which may or may not be continuous, and let <span class="math inline">\(q\)</span> be a fraction so that <span class="math inline">\(0&lt;q&lt;1\)</span>. We want to define a quantile, say <span class="math inline">\(q_F\)</span>, to be a number such that <span class="math inline">\(F(q_F) \approx q\)</span>. Notice that when <span class="math inline">\(q = 0.5\)</span>, <span class="math inline">\(q_F\)</span> is the median; when <span class="math inline">\(q = 0.25\)</span>, <span class="math inline">\(q_F\)</span> is the first quartile, and so on. So, a quantile generalizes the concepts of median, quartiles, and percentiles.</p>
<p>To be precise, for a given <span class="math inline">\(0&lt;q&lt;1\)</span>, define the <span class="math inline">\(q\)</span><strong>th quantile</strong> <span class="math inline">\(q_F\)</span> to be <em>any</em> number that satisfies</p>
<span class="math display" id="eq:Quantile">\[\begin{equation} 
  F(q_F-) \le q \le F(q_F)
  \tag{4.1}
\end{equation}\]</span>
<p>Here, the notation <span class="math inline">\(F(x-)\)</span> means to evaluate the function <span class="math inline">\(F(\cdot)\)</span> as a left-hand limit.</p>
<p>To get a better understanding of this definition, let us look at a few special cases. First, consider the case where <span class="math inline">\(X\)</span> is a continuous random variable so that the distribution function <span class="math inline">\(F(\cdot)\)</span> has no jump points, as illustrated in Figure <a href="C-ModelSelection.html#fig:Quantile1">4.2</a>. In this figure, a few fractions, <span class="math inline">\(q_1\)</span>, <span class="math inline">\(q_2\)</span>, and <span class="math inline">\(q_3\)</span> are shown with their corresponding quantiles <span class="math inline">\(q_{F,1}\)</span>, <span class="math inline">\(q_{F,2}\)</span>, and <span class="math inline">\(q_{F,3}\)</span>. In each case, it can be seen that <span class="math inline">\(F(q_F-)= F(q_F)\)</span> so that there is a unique quantile. Because we can find a unique inverse of the distribution function at any <span class="math inline">\(0&lt;q&lt;1\)</span>, we can write <span class="math inline">\(q_F= F^{-1}(q)\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:Quantile1"></span>
<img src="LossDataAnalytics_files/figure-html/Quantile1-1.png" alt="Continuous Quantile Case" width="60%" />
<p class="caption">
Figure 4.2: Continuous Quantile Case
</p>
</div>
<p>Figure <a href="C-ModelSelection.html#fig:Quantile2">4.3</a> shows three cases for distribution functions. The left panel corresponds to the continuous case just discussed. The middle panel displays a jump point similar to those we already saw in the empirical distribution function of Figure <a href="C-ModelSelection.html#fig:EDFToy">4.1</a>. For the value of <span class="math inline">\(q\)</span> shown in this panel, we still have a unique value of the quantile <span class="math inline">\(q_F\)</span>. Even though there are many values of <span class="math inline">\(q\)</span> such that <span class="math inline">\(F(q_F-) \le q \le F(q_F)\)</span>, for a particular value of <span class="math inline">\(q\)</span>, there is only one solution to equation <a href="C-ModelSelection.html#eq:Quantile">(4.1)</a>. The right panel depicts a situation in which the quantile cannot be uniquely determined for the <span class="math inline">\(q\)</span> shown as there is a range of <span class="math inline">\(q_F\)</span>’s satisfying equation <a href="C-ModelSelection.html#eq:Quantile">(4.1)</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:Quantile2"></span>
<img src="LossDataAnalytics_files/figure-html/Quantile2-1.png" alt="Three Quantile Cases" width="90%" />
<p class="caption">
Figure 4.3: Three Quantile Cases
</p>
</div>
<hr />
<p><strong>Example 4.1.2. Toy Data Set: Continued.</strong> Determine quantiles corresponding to the 20th, 50th, and 95th percentiles.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.2" href="javascript:toggleEX('toggleExampleSelect.1.2','displayTextExampleSelect.1.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.2" style="display: none">
<p><strong>Solution</strong>. Consider Figure <a href="C-ModelSelection.html#fig:EDFToy">4.1</a>. The case of <span class="math inline">\(q=0.20\)</span> corresponds to the middle panel, so the 20th percentile is 15. The case of <span class="math inline">\(q=0.50\)</span> corresponds to the right panel, so the median is any number between 20 and 23 inclusive. Many software packages use the average 21.5 (e.g. <code>R</code>, as seen below). For the 95th percentile, the solution is 30. We can see from the graph that 30 also corresponds to the 99th and the 99.99th percentiles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(xExample, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>), <span class="dt">type=</span><span class="dv">6</span>)</code></pre></div>
<pre><code>##  20%  50%  95% 
## 15.0 21.5 30.0</code></pre>
</div>
<hr />
<p>By taking a weighted average between data observations, smoothed empirical quantiles can handle cases such as the right panel in Figure <a href="C-ModelSelection.html#fig:Quantile2">4.3</a>. The <span class="math inline">\(q\)</span>th <em>smoothed empirical quantile</em> is defined as <span class="math display">\[
\hat{\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}
\]</span> where <span class="math inline">\(j=\lfloor(n+1)q\rfloor\)</span>, <span class="math inline">\(h=(n+1)q-j\)</span>, and <span class="math inline">\(X_{(1)}, \ldots, X_{(n)}\)</span> are the ordered values (known as the <em>order statistics</em>) corresponding to <span class="math inline">\(X_1, \ldots, X_n\)</span>. Note that <span class="math inline">\(\hat{\pi}_q\)</span> is simply a linear interpolation between <span class="math inline">\(X_{(j)}\)</span> and <span class="math inline">\(X_{(j+1)}\)</span>.</p>
<p><strong>Example 4.1.3. Toy Data Set: Continued.</strong> Determine the 50th and 20th smoothed percentiles.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.3" href="javascript:toggleEX('toggleExampleSelect.1.3','displayTextExampleSelect.1.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.3" style="display: none">
<p><strong>Solution</strong> Take <span class="math inline">\(n=10\)</span> and <span class="math inline">\(q=0.5\)</span>. Then, <span class="math inline">\(j=\lfloor(11)0.5 \rfloor= \lfloor5.5 \rfloor=5\)</span> and <span class="math inline">\(h=(11)(0.5)-5=0.5\)</span>. Then the 0.5-th smoothed empirical quantile is <span class="math display">\[\hat{\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\]</span> Now take <span class="math inline">\(n=10\)</span> and <span class="math inline">\(q=0.2\)</span>. In this case, <span class="math inline">\(j=\lfloor(11)0.2\rfloor=\lfloor 2.2 \rfloor=2\)</span> and <span class="math inline">\(h=(11)(0.2)-2=0.2\)</span>. Then the 0.2-th smoothed empirical quantile is <span class="math display">\[\hat{\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.2 (15) + (0.8)(15) = 15.\]</span></p>
</div>
<hr />
</div>
<div id="density-estimators" class="section level4">
<h4><span class="header-section-number">4.1.1.4</span> Density Estimators</h4>
<p><strong>Discrete Variable.</strong> When the random variable is discrete, estimating the probability mass function <span class="math inline">\(f(x) = \Pr(X=x)\)</span> is straightforward. We simply use the sample average, defined to be <span class="math display">\[f_n(x) = \frac{1}{n} \sum_{i=1}^n I(X_i = x).\]</span></p>
<p><strong>Continuous Variable within a Group.</strong> For a continuous random variable, consider a discretized formulation in which the domain of <span class="math inline">\(F(\cdot)\)</span> is partitioned by constants <span class="math inline">\(\{c_0 &lt; c_1 &lt; \cdots &lt; c_k\}\)</span> into intervals of the form <span class="math inline">\([c_{j-1}, c_j)\)</span>, for <span class="math inline">\(j=1, \ldots, k\)</span>. The data observations are thus “grouped” by the intervals into which they fall. Then, we might use the basic definition of the empirical mass function, or a variation such as <span class="math display">\[f_n(x) = \frac{n_j}{n \times (c_j - c_{j-1})}  \ \ \ \ \ \ c_{j-1} \le x &lt; c_j,\]</span> where <span class="math inline">\(n_j\)</span> is the number of observations (<span class="math inline">\(X_i\)</span>) that fall into the interval <span class="math inline">\([c_{j-1}, c_j)\)</span>.</p>
<p><strong>Continuous Variable (not grouped).</strong> Extending this notion to instances where we observe individual data, note that we can always create arbitrary groupings and use this formula. More formally, let <span class="math inline">\(b&gt;0\)</span> be a small positive constant, known as a <strong>bandwidth</strong>, and define a density estimator to be</p>
<span class="math display" id="eq:KDF">\[\begin{equation} 
  f_n(x) = \frac{1}{2nb} \sum_{i=1}^n I(x-b &lt; X_i \le x + b)
  \tag{4.2}
\end{equation}\]</span>
<h5 style="text-align: center;">
<a id="displayTheory.1" href="javascript:toggleTheory('Theorykerneldensity','displayTheory.1');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="Theorykerneldensity" style="display: none">
<hr />
<p>The idea is that the estimator <span class="math inline">\(f_n(x)\)</span> in equation <a href="C-ModelSelection.html#eq:KDF">(4.2)</a> is the average over <span class="math inline">\(n\)</span> <a href="#" class="tooltip" style="color:green"><em>iid</em><span style="font-size:8pt"> identically and independently distributed</span></a> realizations of a random variable with mean</p>
<p><span class="math display">\[\begin{aligned}
\mathrm{E~ } \frac{1}{2b} I(x-b &lt; X \le x + b) &amp;=  \frac{1}{2b}\left(F(x+b)-F(x-b)\right) \\
&amp;=  \frac{1}{2b} \left( \left\{ F(x) + b F^{\prime}(x) + b^2 C_1\right\}
\left\{ F(x) - b F^{\prime}(x) + b^2 C_2\right\} \right) \\
&amp;=  F^{\prime}(x) + b \frac{C_1-C_2}{2} \rightarrow  F^{\prime}(x) = f(x),
\end{aligned}\]</span></p>
<p>as <span class="math inline">\(b\rightarrow 0\)</span>. That is, <span class="math inline">\(f_n(x)\)</span> is an asymptotically unbiased estimator of <span class="math inline">\(f(x)\)</span> (its expectation approaches the true value as sample size increases to infinity). This development assumes some smoothness of <span class="math inline">\(F(\cdot)\)</span>, in particular, twice differentiability at <span class="math inline">\(x\)</span>, but makes no assumptions on the form of the distribution function <span class="math inline">\(F\)</span>. Because of this, the density estimator <span class="math inline">\(f_n\)</span> is said to be <em>nonparametric</em>.</p>
<hr />
</div>
<p>More generally, define the <strong>kernel density estimator</strong> of the <a href="#" class="tooltip" style="color:green"><em>pdf</em><span style="font-size:8pt"> probability density function</span></a> at <em>x</em> as</p>
<span class="math display" id="eq:kernelDens">\[\begin{equation} 
  f_n(x) = \frac{1}{nb} \sum_{i=1}^n w\left(\frac{x-X_i}{b}\right) ,
  \tag{4.3}
\end{equation}\]</span>
<p>where <span class="math inline">\(w\)</span> is a probability density function centered about 0. Note that equation <a href="C-ModelSelection.html#eq:KDF">(4.2)</a> simply becomes the kernel density estimator where <span class="math inline">\(w(x) = \frac{1}{2}I(-1 &lt; x \le 1)\)</span>, also known as the <em>uniform kernel</em>. Other popular choices are shown in <a href="#tab:41">Table 4.1</a>.</p>
<p><a id=tab:41></a></p>
<p><span class="math display">\[
{\small
\begin{matrix}
\text{Table 4.1: Popular Choices for the Kernel Density Estimator}\\
\begin{array}{l|cc}
\hline
\text{Kernel} &amp;  w(x) \\
\hline 
\text{Uniform } &amp;  \frac{1}{2}I(-1 &lt; x \le 1) \\
\text{Triangle} &amp;  (1-|x|)\times I(|x| \le 1) \\
\text{Epanechnikov} &amp; \frac{3}{4}(1-x^2) \times I(|x| \le 1) \\
\text{Gaussian} &amp; \phi(x) \\
\hline
\end{array}\end{matrix}
}
\]</span></p>
<p>Here, <span class="math inline">\(\phi(\cdot)\)</span> is the standard normal density function. As we will see in the following example, the choice of bandwidth <span class="math inline">\(b\)</span> comes with a <em>bias-variance tradeoff</em> between matching local distributional features and reducing the volatility.</p>
<hr />
<p><strong>Example 4.1.4. Property Fund.</strong> Figure <a href="C-ModelSelection.html#fig:Density2">4.4</a> shows a histogram (with shaded gray rectangles) of logarithmic property claims from 2010. The (blue) thick curve represents a Gaussian kernel density where the bandwidth was selected automatically using an ad hoc rule based on the sample size and volatility of the data. For this dataset, the bandwidth turned out to be <span class="math inline">\(b=0.3255\)</span>. For comparison, the (red) dashed curve represents the density estimator with a bandwidth equal to 0.1 and the green smooth curve uses a bandwidth of 1. As anticipated, the smaller bandwidth (0.1) indicates taking local averages over less data so that we get a better idea of the local average, but at the price of higher volatility. In contrast, the larger bandwidth (1) smooths out local fluctuations, yielding a smoother curve that may miss perturbations in the local average. For actuarial applications, we mainly use the kernel density estimator to get a quick visual impression of the data. From this perspective, you can simply use the default ad hoc rule for bandwidth selection, knowing that you have the ability to change it depending on the situation at hand.</p>
<div class="figure" style="text-align: center"><span id="fig:Density2"></span>
<img src="LossDataAnalytics_files/figure-html/Density2-1.png" alt="Histogram of Logarithmic Property Claims with Superimposed Kernel Density Estimators" width="70%" />
<p class="caption">
Figure 4.4: Histogram of Logarithmic Property Claims with Superimposed Kernel Density Estimators
</p>
</div>
<h5 style="text-align: center;">
<a id="displaykpdf" href="javascript:togglecode('togglekpdf','displaykpdf');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="togglekpdf" style="display: none">
<pre><code>#Density Comparison
hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;)
lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5)
lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;)
lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3)
legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1),
            lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1)</code></pre>
</div>
<hr />
<p>Nonparametric density estimators, such as the kernel estimator, are regularly used in practice. The concept can also be extended to give smooth versions of an empirical distribution function. Given the definition of the kernel density estimator, the <em>kernel estimator of the distribution function</em> can be found as <span class="math display">\[\begin{aligned}
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n W\left(\frac{x-X_i}{b}\right).\end{aligned}\]</span></p>
<p>where <span class="math inline">\(W\)</span> is the distribution function associated with the kernel density <span class="math inline">\(w\)</span>. To illustrate, for the uniform kernel, we have <span class="math inline">\(w(y) = \frac{1}{2}I(-1 &lt; y \le 1)\)</span>, so <span class="math display">\[
\begin{aligned}
W(y) =
\begin{cases}
0 &amp;            y&lt;-1\\
\frac{y+1}{2}&amp; -1 \le y &lt; 1 \\
1 &amp; y \ge 1 \\
\end{cases}\end{aligned} .
\]</span></p>
<hr />
<p><strong>Example 4.1.5. Actuarial Exam Question.</strong> You study five lives to estimate the time from the onset of a disease to death. The times to death are:</p>
<p><span class="math display">\[\begin{array}{ccccc}
2 &amp; 3 &amp; 3 &amp; 3 &amp; 7  \\
\end{array}\]</span></p>
<p>Using a triangular kernel with bandwith <span class="math inline">\(2\)</span>, calculate the density function estimate at 2.5.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.5" href="javascript:toggleEX('toggleExampleSelect.1.5','displayTextExampleSelect.1.5');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.5" style="display: none">
<p><strong>Solution.</strong> For the kernel density estimate, we have <span class="math display">\[f_n(x) = \frac{1}{nb} \sum_{i=1}^n w\left(\frac{x-X_i}{b}\right),\]</span> where <span class="math inline">\(n=5\)</span>, <span class="math inline">\(b=2\)</span>, and <span class="math inline">\(x=2.5\)</span>. For the triangular kernel, <span class="math inline">\(w(x) = (1-|x|)\times I(|x| \le 1)\)</span>. Thus,</p>
<p><span class="math display">\[\begin{array}{c|c|c}
\hline
X_i &amp; \frac{x-X_i}{b} &amp; w\left(\frac{x-X_i}{b} \right) \\
\hline 
2 &amp; \frac{2.5-2}{2}=\frac{1}{4} &amp;  (1-\frac{1}{4})(1) = \frac{3}{4} \\
\hline
3 &amp; &amp; \\
3 &amp; \frac{2.5-3}{2}=\frac{-1}{4} &amp; \left(1-\left| \frac{-1}{4} \right| \right)(1) = \frac{3}{4} \\
3 &amp; &amp; \\
\hline
7 &amp; \frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\
\hline
\end{array}\]</span></p>
<p>Then the kernel density estimate is <span class="math display">\[f_n(x) = \frac{1}{5(2)}\left( \frac{3}{4} + (3) \frac{3}{4} + 0 \right) = \frac{3}{10}\]</span></p>
</div>
<hr />
</div>
</div>
<div id="S:MS:ToolsModelSelection" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Tools for Model Selection and Diagnostics</h3>
<p>The previous section introduced nonparametric estimators in which there was no parametric form assumed about the underlying distributions. However, in many actuarial applications, analysts seek to employ a parametric fit of a distribution for ease of explanation and the ability to readily extend it to more complex situations such as including explanatory variables in a regression setting. When fitting a parametric distribution, one analyst might try to use a gamma distribution to represent a set of loss data. However, another analyst may prefer to use a Pareto distribution. How does one know which model to <strong>select</strong>?</p>
<p>Nonparametric tools can be used to corroborate the selection of parametric models. Essentially, the approach is to compute selected summary measures under a fitted parametric model and to compare it to the corresponding quantity under the nonparametric model. As the nonparametric does not assume a specific distribution and is merely a function of the data, it is used as a benchmark to assess how well the parametric distribution/model represents the data. This comparison may alert the analyst to deficiencies in the parametric model and sometimes point ways to improving the parametric specification. Procedures geared towards assessing the validity of a model are known as <strong>model diagnostics</strong>.</p>
<div id="S:MS:GraphComparison" class="section level4">
<h4><span class="header-section-number">4.1.2.1</span> Graphical Comparison of Distributions</h4>
<p>We have already seen the technique of overlaying graphs for comparison purposes. To reinforce the application of this technique, Figure <a href="C-ModelSelection.html#fig:ComparisonCDFPDF">4.5</a> compares the empirical distribution to two parametric fitted distributions. The left panel shows the distribution functions of claims distributions. The dots forming an “S-shaped” curve represent the empirical distribution function at each observation. The thick blue curve gives corresponding values for the fitted gamma distribution and the light purple is for the fitted Pareto distribution. Because the Pareto is much closer to the empirical distribution function than the gamma, this provides evidence that the Pareto is the better model for this data set. The right panel gives similar information for the density function and provides a consistent message. Based (only) on these figures, the Pareto distribution is the clear choice for the analyst.</p>
<div class="figure" style="text-align: center"><span id="fig:ComparisonCDFPDF"></span>
<img src="LossDataAnalytics_files/figure-html/ComparisonCDFPDF-1.png" alt="Nonparametric Versus Fitted Parametric Distribution and Density Functions. The left-hand panel compares distribution functions, with the dots corresponding to the empirical distribution, the thick blue curve corresponding to the fitted gamma and the light purple curve corresponding to the fitted Pareto. The right hand panel compares these three distributions summarized using probability density functions." width="80%" />
<p class="caption">
Figure 4.5: Nonparametric Versus Fitted Parametric Distribution and Density Functions. The left-hand panel compares distribution functions, with the dots corresponding to the empirical distribution, the thick blue curve corresponding to the fitted gamma and the light purple curve corresponding to the fitted Pareto. The right hand panel compares these three distributions summarized using probability density functions.
</p>
</div>
<p>For another way to compare the appropriateness of two fitted models, consider the <strong>probability-probability (<span class="math inline">\(pp\)</span>) plot</strong>. A <span class="math inline">\(pp\)</span> plot compares cumulative probabilities under two models. For our purposes, these two models are the nonparametric empirical distribution function and the parametric fitted model. Figure <a href="C-ModelSelection.html#fig:PPPlot">4.6</a> shows <span class="math inline">\(pp\)</span> plots for the Property Fund data. The fitted gamma is on the left and the fitted Pareto is on the right, compared to the same empirical distribution function of the data. The straight line represents equality between the two distributions being compared, so points close to the line are desirable. As seen in earlier demonstrations, the Pareto is much closer to the empirical distribution than the gamma, providing additional evidence that the Pareto is the better model.</p>
<div class="figure" style="text-align: center"><span id="fig:PPPlot"></span>
<img src="LossDataAnalytics_files/figure-html/PPPlot-1.png" alt="Probability-Probability ($pp$) Plots. The horizontal axes gives the empirical distribution function at each observation. In the left-hand panel, the corresponding distribution function for the gamma is shown in the vertical axis. The right-hand panel shows the fitted Pareto distribution. Lines of $y=x$ are superimposed." width="80%" />
<p class="caption">
Figure 4.6: Probability-Probability (<span class="math inline">\(pp\)</span>) Plots. The horizontal axes gives the empirical distribution function at each observation. In the left-hand panel, the corresponding distribution function for the gamma is shown in the vertical axis. The right-hand panel shows the fitted Pareto distribution. Lines of <span class="math inline">\(y=x\)</span> are superimposed.
</p>
</div>
<p>A <span class="math inline">\(pp\)</span> plot is useful in part because no artificial scaling is required, such as with the overlaying of densities in Figure <a href="C-ModelSelection.html#fig:ComparisonCDFPDF">4.5</a>, in which we switched to the log scale to better visualize the data. The Chapter 4 <em>Technical Supplement A.1</em> introduces a variation of the <span class="math inline">\(pp\)</span> plot known as a <em>Lorenz curve</em>; this is an important tool for assessing income inequality. Furthermore, <span class="math inline">\(pp\)</span> plots are available in multivariate settings where more than one outcome variable is available. However, a limitation of the <span class="math inline">\(pp\)</span> plot is that, because it is a plot <em>cumulative</em> distribution functions, it can sometimes be difficult to detect <em>where</em> a fitted parametric distribution is deficient. As an alternative, it is common to use a <strong>quantile-quantile (<span class="math inline">\(qq\)</span>) plot</strong>, as demonstrated in Figure <a href="C-ModelSelection.html#fig:QQPlot">4.7</a>.</p>
<p>The <span class="math inline">\(qq\)</span> plot compares two fitted models through their quantiles. As with <span class="math inline">\(pp\)</span> plots, we compare the nonparametric to a parametric fitted model. Quantiles may be evaluated at each point of the data set, or on a grid (e.g., at <span class="math inline">\(0, 0.001, 0.002, \ldots, 0.999, 1.000\)</span>), depending on the application. In Figure <a href="C-ModelSelection.html#fig:QQPlot">4.7</a>, for each point on the aforementioned grid, the horizontal axis displays the empirical quantile and the vertical axis displays the corresponding fitted parametric quantile (gamma for the upper two panels, Pareto for the lower two). Quantiles are plotted on the original scale in the left panels and on the log scale in the right panels to allow us to see where a fitted distribution is deficient. The straight line represents equality between the empirical distribution and fitted distribution. From these plots, we again see that the Pareto is an overall better fit than the gamma. Furthermore, the lower-right panel suggests that the Pareto distribution does a good job with large observations, but provides a poorer fit for small observations.</p>
<div class="figure" style="text-align: center"><span id="fig:QQPlot"></span>
<img src="LossDataAnalytics_files/figure-html/QQPlot-1.png" alt="Quantile-Quantile ($qq$) Plots. The horizontal axes gives the empirical quantiles at each observation. The right-hand panels they are graphed on a logarithmic basis. The vertical axis gives the quantiles from the fitted distributions; gamma quantiles are in the upper panels, Pareto quantiles are in the lower panels." width="80%" />
<p class="caption">
Figure 4.7: Quantile-Quantile (<span class="math inline">\(qq\)</span>) Plots. The horizontal axes gives the empirical quantiles at each observation. The right-hand panels they are graphed on a logarithmic basis. The vertical axis gives the quantiles from the fitted distributions; gamma quantiles are in the upper panels, Pareto quantiles are in the lower panels.
</p>
</div>
<hr />
<p><strong>Example 4.1.6. Actuarial Exam Question.</strong> The graph below shows a <span class="math inline">\(pp\)</span> plot of a fitted distribution compared to a sample.</p>
<p><img src="LossDataAnalytics_files/figure-html/unnamed-chunk-2-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Comment on the two distributions with respect to left tail, right tail, and median probabilities.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.6" href="javascript:toggleEX('toggleExampleSelect.1.6','displayTextExampleSelect.1.6');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.6" style="display: none">
<p><strong>Solution.</strong> The tail of the fitted distribution is too thick on the left, too thin on the right, and the fitted distribution has less probability around the median than the sample. To see this, recall that the <span class="math inline">\(pp\)</span> plot graphs the cumulative distribution of two distributions on its axes (empirical on the x-axis and fitted on the y-axis in this case). For small values of <span class="math inline">\(x\)</span>, the fitted model assigns greater probability to being below that value than occurred in the sample (i.e. <span class="math inline">\(F(x) &gt; F_n(x)\)</span>). This indicates that the model has a heavier left tail than the data. For large values of <span class="math inline">\(x\)</span>, the model again assigns greater probability to being below that value and thus less probability to being above that value (i.e. <span class="math inline">\(S(x) &lt; S_n(x)\)</span>. This indicates that the model has a lighter right tail than the data. In addition, as we go from 0.4 to 0.6 on the horizontal axis (thus looking at the middle 20% of the data), the <span class="math inline">\(pp\)</span> plot increases from about 0.3 to 0.4. This indicates that the model puts only about 10% of the probability in this range.</p>
</div>
<hr />
</div>
<div id="S:MS:Tools:Stats" class="section level4">
<h4><span class="header-section-number">4.1.2.2</span> Statistical Comparison of Distributions</h4>
<p>When selecting a model, it is helpful to make the graphical displays presented. However, for reporting results, it can be effective to supplement the graphical displays with selected statistics that summarize model goodness of fit. <a href="#tab:42">Table 4.2</a> provides three commonly used goodness of fit statistics. In this table, <span class="math inline">\(F_n\)</span> is the empirical distribution, <span class="math inline">\(F\)</span> is the fitted or hypothesized distribution, and <span class="math inline">\(F_i = F(x_i)\)</span>.</p>
<p><a id=tab:42></a></p>
<p><span class="math display">\[
{\small
\begin{matrix}
\text{Table 4.2: Three Goodness of Fit Statistics} \\
\begin{array}{l|cc}
\hline
\text{Statistic} &amp; \text{Definition} &amp; \text{Computational Expression} \\
\hline 
\text{Kolmogorov-} &amp; \max_x |F_n(x) - F(x)| &amp; \max(D^+, D^-) \text{ where } \\
~~~\text{Smirnov} &amp;&amp; D^+ = \max_{i=1, \ldots, n} \left|\frac{i}{n} - F_i\right| \\
&amp;&amp; D^- = \max_{i=1, \ldots, n} \left| F_i - \frac{i-1}{n} \right| \\
\text{Cramer-von Mises} &amp; n \int (F_n(x) - F(x))^2 f(x) dx &amp; \frac{1}{12n} + \sum_{i=1}^n \left(F_i - (2i-1)/n\right)^2 \\
\text{Anderson-Darling} &amp; n \int \frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\frac{1}{n} \sum_{i=1}^n (2i-1) \log\left(F_i(1-F_{n+1-i})\right)^2 \\
\hline
\end{array} \\
\end{matrix}
}
\]</span></p>
<p>The <em>Kolmogorov-Smirnov statistic</em> is the maximum absolute difference between the fitted distribution function and the empirical distribution function. Instead of comparing differences between single points, the <em>Cramer-von Mises statistic</em> integrates the difference between the empirical and fitted distribution functions over the entire range of values. The <em>Anderson-Darling statistic</em> also integrates this difference over the range of values, although weighted by the inverse of the variance. It therefore places greater emphasis on the tails of the distribution (i.e when <span class="math inline">\(F(x)\)</span> or <span class="math inline">\(1-F(x)=S(x)\)</span> is small).</p>
<hr />
<p><strong>Exaxmple 4.1.7. Actuarial Exam Question (modified).</strong> A sample of claim payments is:</p>
<p><span class="math display">\[\begin{array}{ccccc}
29 &amp; 64 &amp; 90 &amp; 135 &amp; 182  \\
\end{array}\]</span></p>
<p>Compare the empirical claims distribution to an exponential distribution with mean <span class="math inline">\(100\)</span> by calculating the value of the Kolmogorov-Smirnov test statistic.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.7" href="javascript:toggleEX('toggleExampleSelect.1.7','displayTextExampleSelect.1.7');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.7" style="display: none">
<p><strong>Solution.</strong> For an exponential distribution with mean <span class="math inline">\(100\)</span>, the cumulative distribution function is <span class="math inline">\(F(x)=1-e^{-x/100}\)</span>. Thus,</p>
<p><span class="math display">\[\begin{array}{ccccc}
\hline
x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\
\hline 
29  &amp; 0.2517 &amp; 0.2 &amp; 0   &amp; \max(0.0517, 0.2517) = 0.2517 \\
64  &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \max(0.0727, 0.2727) = 0.2727 \\
90  &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \max(0.0066, 0.1934) = 0.1934 \\
135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \max(0.0592, 0.1408) = 0.1408 \\
182 &amp; 0.8380 &amp; 1   &amp; 0.8 &amp; \max(0.1620, 0.0380) = 0.1620 \\
\hline
\end{array}\]</span></p>
<p>The Kolmogorov-Smirnov test statistic is therefore <span class="math inline">\(KS = \max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727\)</span>.</p>
</div>
<hr />
</div>
</div>
<div id="starting-values" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Starting Values</h3>
<p>The method of moments and percentile matching are nonparametric estimation methods that provide alternatives to maximum likelihood. Generally, maximum likelihood is the preferred technique because it employs data more efficiently. (See Appendix Chapter <a href="C-AppC.html#C:AppC">17</a> for precise definitions of efficiency.) However, methods of moments and percentile matching are useful because they are easier to interpret and therefore allow the actuary or analyst to explain procedures to others. Additionally, the numerical estimation procedure (e.g. if performed in <code>R</code>) for the maximum likelihood is iterative and requires starting values to begin the recursive process. Although many problems are robust to the choice of the starting values, for some complex situations, it can be important to have a starting value that is close to the (unknown) optimal value. Method of moments and percentile matching are techniques that can produce desirable estimates without a serious computational investment and can thus be used as a <em>starting value</em> for computing maximum likelihood.</p>
<div id="method-of-moments" class="section level4">
<h4><span class="header-section-number">4.1.3.1</span> Method of Moments</h4>
<p>Under the <strong>method of moments</strong>, we approximate the moments of the parametric distribution using the empirical (nonparametric) moments described in Section <a href="C-ModelSelection.html#S:MS:MomentEstimator">4.1.1.1</a>. We can then algebraically solve for the parameter estimates.</p>
<hr />
<p><strong>Example 4.1.8. Property Fund.</strong> For the 2010 property fund, there are <span class="math inline">\(n=1,377\)</span> individual claims (in thousands of dollars) with</p>
<p><span class="math display">\[m_1 = \frac{1}{n} \sum_{i=1}^n X_i = 26.62259 \ \ \ \
\text{and} \ \ \ \
 m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = 136154.6 .\]</span> Fit the parameters of the gamma and Pareto distributions using the method of moments.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.8" href="javascript:toggleEX('toggleExampleSelect.1.8','displayTextExampleSelect.1.8');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.8" style="display: none">
<p><strong>Solution.</strong> To fit a gamma distribution, we have <span class="math inline">\(\mu_1 = \alpha \theta\)</span> and <span class="math inline">\(\mu_2^{\prime} = \alpha(\alpha+1) \theta^2\)</span>. Equating the two yields the method of moments estimators, easy algebra shows that</p>
<p><span class="math display">\[\alpha = \frac{\mu_1^2}{\mu_2^{\prime}-\mu_1^2}  \ \ \ \text{and} \ \ \  \theta = \frac{\mu_2^{\prime}-\mu_1^2}{\mu_1}.\]</span></p>
<p>Thus, the method of moment estimators are</p>
<p><span class="math display">\[\begin{aligned}
\hat{\alpha} &amp;=  \frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\
\hat{\theta} &amp;=  \frac{136154.6-26.62259^2}{26.62259} = 5,087.629.
\end{aligned}\]</span></p>
<p>For comparison, the maximum likelihood values turn out to be <span class="math inline">\(\hat{\alpha}_{MLE} = 0.2905959\)</span> and <span class="math inline">\(\hat{\theta}_{MLE} = 91.61378\)</span>, so there are big discrepancies between the two estimation procedures. This is one indication, as we have seen before, that the gamma model fits poorly.</p>
<p>In contrast, now assume a Pareto distribution so that <span class="math inline">\(\mu_1 = \theta/(\alpha -1)\)</span> and <span class="math inline">\(\mu_2^{\prime} = 2\theta^2/((\alpha-1)(\alpha-2) )\)</span>. Easy algebra shows</p>
<p><span class="math display">\[\alpha = 1+ \frac{\mu_2^{\prime}}{\mu_2^{\prime}-\mu_1^2} \ \ \ \
\text{and} \ \ \ \ \
 \theta = (\alpha-1)\mu_1.\]</span></p>
<p>Thus, the method of moment estimators are</p>
<p><span class="math display">\[\begin{aligned}
\hat{\alpha} &amp;=  1+ \frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\
\hat{\theta} &amp;=  (2.005233-1) \cdot 26.62259 = 26.7619
\end{aligned}\]</span></p>
<p>The maximum likelihood values turn out to be <span class="math inline">\(\hat{\alpha}_{MLE} = 0.9990936\)</span> and <span class="math inline">\(\hat{\theta}_{MLE} = 2.2821147\)</span>. It is interesting that <span class="math inline">\(\hat{\alpha}_{MLE}&lt;1\)</span>; for the Pareto distribution, recall that <span class="math inline">\(\alpha &lt;1\)</span> means that the mean is infinite. This is another indication that the property claims data set is a long tail distribution.</p>
</div>
<hr />
<p>As the above example suggests, there is flexibility with the method of moments. For example, we could have matched the second and third moments instead of the first and second, yielding different estimators. Furthermore, there is no guarantee that a solution will exist for each problem. You will also find that matching moments is possible for a few problems where the data are censored or truncated, but in general, this is a more difficult scenario. Finally, for distributions where the moments do not exist or are infinite, method of moments is not available. As an alternative, one can use the percentile matching technique.</p>
</div>
<div id="percentile-matching" class="section level4">
<h4><span class="header-section-number">4.1.3.2</span> Percentile Matching</h4>
<p>Under <strong>percentile matching</strong>, we approximate the quantiles or percentiles of the parametric distribution using the empirical (nonparametric) quantiles or percentiles described in Section <a href="C-ModelSelection.html#S:MS:QuantileEstimator">4.1.1.3</a>.</p>
<hr />
<p><strong>Example 4.1.9. Property Fund.</strong> For the 2010 property fund, we illustrate matching on quantiles. In particular, the Pareto distribution is intuitively pleasing because of the closed-form solution for the quantiles. Recall that the distribution function for the Pareto distribution is <span class="math display">\[F(x) = 1 - \left(\frac{\theta}{x+\theta}\right)^{\alpha}.\]</span> Easy algebra shows that we can express the quantile as <span class="math display">\[F^{-1}(q) = \theta \left( (1-q)^{-1/\alpha} -1 \right).\]</span> for a fraction <span class="math inline">\(q\)</span>, <span class="math inline">\(0&lt;q&lt;1\)</span>.</p>
<p>Determine estimates of the Pareto distribution parameters using the 25th and 95th empirical quantiles.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.9" href="javascript:toggleEX('toggleExampleSelect.1.9','displayTextExampleSelect.1.9');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.9" style="display: none">
<p><strong>Solution.</strong></p>
<p>The 25th percentile (the first quartile) turns out to be <span class="math inline">\(0.78853\)</span> and the 95th percentile is <span class="math inline">\(50.98293\)</span> (both in thousands of dollars). With two equations <span class="math display">\[0.78853 = \theta \left( 1- (1-.25)^{-1/\alpha} \right) \ \ \ \ \text{and} \ \ \ \ 50.98293 = \theta \left( 1- (1-.75)^{-1/\alpha} \right)\]</span> and two unknowns, the solution is <span class="math display">\[\hat{\alpha} = 0.9412076 \ \ \ \ \ \text{and} \ \ \ \
\hat{\theta} = 2.205617 .\]</span> We remark here that a numerical routine is required for these solutions as no analytic solution is available. Furthermore, recall that the maximum likelihood estimates are <span class="math inline">\(\hat{\alpha}_{MLE} = 0.9990936\)</span> and <span class="math inline">\(\hat{\theta}_{MLE} = 2.2821147\)</span>, so the percentile matching provides a better approximation for the Pareto distribution than the method of moments.</p>
</div>
<hr />
<p><strong>Example 4.1.10. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>Losses follow a loglogistic distribution with cumulative distribution function: <span class="math display">\[F(x) = \frac{\left(x/\theta\right)^{\gamma}}{1+\left(x/\theta\right)^{\gamma}}\]</span></li>
<li>The sample of losses is:</li>
</ol>
<p><span class="math display">\[\begin{array}{ccccccccccc}
10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\
\end{array}\]</span></p>
<p>Calculate the estimate of <span class="math inline">\(\theta\)</span> by percentile matching, using the 40th and 80th empirically smoothed percentile estimates.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.1.10" href="javascript:toggleEX('toggleExampleSelect.1.10','displayTextExampleSelect.1.10');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.1.10" style="display: none">
<p><strong>Solution.</strong> With 11 observations, we have <span class="math inline">\(j=\lfloor(n+1)q\rfloor = \lfloor 12(0.4) \rfloor = \lfloor 4.8\rfloor=4\)</span> and <span class="math inline">\(h=(n+1)q-j = 12(0.4)-4=0.8\)</span>. By interpolation, the 40th empirically smoothed percentile estimate is <span class="math inline">\(\hat{\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\)</span>.</p>
<p>Similarly, for the 80th empirically smoothed percentile estimate, we have <span class="math inline">\(12(0.8)=9.6\)</span> so the estimate is <span class="math inline">\(\hat{\pi}_{0.8} = 0.4(200)+0.6(210)=206\)</span>.</p>
<p>Using the loglogistic cumulative distribution, we need to solve the following two equations for parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\gamma\)</span>: <span class="math display">\[0.4=\frac{(89.2/\theta)^\gamma}{1+(89.2/\theta)^\gamma} \ \ \ \text{and} \ \ \ \   0.8=\frac{(206/\theta)^\gamma}{1+(206+\theta)^\gamma}\]</span></p>
<p>Solving for each parenthetical expression gives <span class="math inline">\(\frac{2}{3}=(89.2/\theta)^\gamma\)</span> and <span class="math inline">\(4=(206/\theta)^\gamma\)</span>. Taking the ratio of the second equation to the first gives <span class="math inline">\(6=(206/89.2)^\gamma \Rightarrow \gamma=\frac{\ln(6)}{\ln(206/89.2)} = 2.1407\)</span>. Then <span class="math inline">\(4^{1/2.1407}=206/\theta \Rightarrow \theta=107.8\)</span></p>
</div>
<hr />
<p>Like the method of moments, percentile matching is almost too flexible in the sense that many estimators can be based on percentile matches; for example, one actuary can base estimation on the 25th and 95th percentiles whereas another actuary uses the 20th and 80th percentiles. In general these estimators will differ and there is no compelling reason to prefer one over the other. Also as with the method of moments, percentile matching is appealing because it provides a technique that can be readily applied in selected situations and has an intuitive basis. Although most actuarial applications use maximum likelihood estimators, it can be convenient to have alternative approaches such as method of moments and percentile matching available.</p>
</div>
</div>
</div>
<div id="S:MS:ModelSelection" class="section level2">
<h2><span class="header-section-number">4.2</span> Model Selection</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Describe the iterative model selection specification process</li>
<li>Outline steps needed to select a parametric model</li>
<li>Describe pitfalls of model selection based purely on insample data when compared to the advantages of out-of-sample model validation</li>
</ul>
<hr />
<p>This section underscores the idea that model selection is an iterative process in which models are cyclically (re)formulated and tested for appropriateness before using them for inference. After an overview, we describe the model selection process based on:</p>
<ul>
<li>an in-sample or training dataset,</li>
<li>an out-of-sample or test dataset, and</li>
<li>a method that combines these approaches known as <strong>cross-validation</strong>.</li>
</ul>
<div id="iterative-model-selection" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Iterative Model Selection</h3>
<p>In our development, we examine the data graphically, hypothesize a model structure, and compare the data to a candidate model in order to formulate an improved model. <span class="citation">Box (<a href="#ref-box1980sampling">1980</a>)</span> describes this as an <em>iterative process</em> which is shown in Figure <a href="C-ModelSelection.html#fig:Iterative">4.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:Iterative"></span>
<img src="Figures/F5Iterative.png" alt="The iterative model specification process." width="80%" />
<p class="caption">
Figure 4.8: The iterative model specification process.
</p>
</div>
<p>This iterative process provides a useful recipe for structuring the task of specifying a model to represent a set of data.</p>
<ol style="list-style-type: decimal">
<li>The first step, the model formulation stage, is accomplished by examining the data graphically and using prior knowledge of relationships, such as from economic theory or industry practice.</li>
<li>The second step in the iteration is fitting based on the assumptions of the specified model. These assumptions must be consistent with the data to make valid use of the model.</li>
<li>The third step is <em>diagnostic checking</em>; the data and model must be consistent with one another before additional inferences can be made. Diagnostic checking is an important part of the model formulation; it can reveal mistakes made in previous steps and provide ways to correct these mistakes.</li>
</ol>
<p>The iterative process also emphasizes the skills you need to make analytics work. First, you need a willingness to summarize information numerically and portray this information graphically. Second, it is important to develop an understanding of model properties. You should understand how a probabilistic model behaves in order to match a set of data to it. Third, theoretical properties of the model are also important for inferring general relationships based on the behavior of the data.</p>
</div>
<div id="model-selection-based-on-a-training-dataset" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Model Selection Based on a Training Dataset</h3>
<p>It is common to refer to a dataset used for analysis as an <em>in-sample</em> or <em>training</em> dataset. Techniques available for selecting a model depend upon whether the outcomes <span class="math inline">\(X\)</span> are discrete, continuous, or a hybrid of the two, although the principles are the same.</p>
<p><strong>Graphical and other Basic Summary Measures.</strong> Begin by summarizing the data graphically and with statistics that do not rely on a specific parametric form, as summarized in Section <a href="C-ModelSelection.html#S:MS:NonParInf">4.1</a>. Specifically, you will want to graph both the empirical distribution and density functions. Particularly for loss data that contain many zeros and that can be skewed, deciding on the appropriate scale (e.g., logarithmic) may present some difficulties. For discrete data, tables are often preferred. Determine sample moments, such as the mean and variance, as well as selected quantiles, including the minimum, maximum, and the median. For discrete data, the mode (or most frequently occurring value) is usually helpful.</p>
<p>These summaries, as well as your familiarity of industry practice, will suggest one or more candidate parametric models. Generally, start with the simpler parametric models (for example, one parameter exponential before a two parameter gamma), gradually introducing more complexity into the modeling process.</p>
<p>Critique the candidate parametric model numerically and graphically. For the graphs, utilize the tools introduced in Section <a href="C-ModelSelection.html#S:MS:ToolsModelSelection">4.1.2</a> such as <span class="math inline">\(pp\)</span> and <span class="math inline">\(qq\)</span> plots. For the numerical assessments, examine the statistical significance of parameters and try to eliminate parameters that do not provide additional information.</p>
<p><strong>Likelihood Ratio Tests.</strong> For comparing model fits, if one model is a subset of another, then a likelihood ratio test may be employed; the general approach to likelihood ratio testing is described in Sections <a href="C-AppA.html#S:AppA:HT:LRT">15.4.3</a> and <a href="C-AppC.html#S:AppC:MLEModelVal">17.3.2</a>.</p>
<p><strong>Goodness of Fit Statistics.</strong> Generally, models are not proper subsets of one another so overall goodness of fit statistics are helpful for comparing models. <em>Information criteria</em> are one type of goodness of statistic. The most widely used examples are Akaike’s Information Criterion (<em>AIC</em>) and the (Schwarz) Bayesian Information Criterion (<em>BIC</em>); they are widely cited because they can be readily generalized to multivariate settings. Section <a href="C-AppA.html#S:AppA:HT:IC">15.4.4</a> provides a summary of these statistics.</p>
<p>For selecting the appropriate distribution, statistics that compare a parametric fit to a nonparametric alternative, summarized in Section <a href="C-ModelSelection.html#S:MS:Tools:Stats">4.1.2.2</a>, are useful for model comparison. For discrete data, a <em>goodness of fit</em> statistic (as described in Section <a href="C-Frequency-Modeling.html#S:goodness-of-fit">2.7</a>) is generally preferred as it is more intuitive and simpler to explain.</p>
</div>
<div id="model-selection-based-on-a-test-dataset" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Model Selection Based on a Test Dataset</h3>
<p><strong>Model validation</strong> is the process of confirming that the proposed model is appropriate, especially in light of the purposes of the investigation. An important limitation of the model selection process based only on insample data is that it can be susceptible to <em>data-snooping</em>, that is, fitting a great number of models to a single set of data. By looking at a large number of models, we may overfit the data and understate the natural variation in our representation.</p>
<p>Selecting a model based only on insample data also does not support the goal of <strong>predictive inference</strong>. Particularly in actuarial applications, our goal is to make statements about <em>new</em> experience rather than a dataset at hand. For example, we use claims experience from one year to develop a model that can be used to price insurance contracts for the following year. As an analogy, we can think about the training data set as experience from one year that is used to predict the behavior of the next year’s test data set.</p>
<p>We can respond to these criticisms by using a technique sometimes known as <strong>out-of-sample validation</strong>. The ideal situation is to have available two sets of data, one for training, or model development, and one for testing, or model validation. We initially develop one or several models on the first data set that we call our <em>candidate</em> models. Then, the relative performance of the candidate models can be measured on the second set of data. In this way, the data used to validate the model is unaffected by the procedures used to formulate the model.</p>
<p><strong>Random Split of the Data.</strong> Unfortunately, rarely will two sets of data be available to the investigator. However, we can implement the validation process by splitting the data set into <strong>training</strong> and <strong>test</strong> subsamples, respectively. Figure <a href="C-ModelSelection.html#fig:ModelValidation">4.9</a> illustrates this splitting of the data.</p>
<div class="figure" style="text-align: center"><span id="fig:ModelValidation"></span>
<img src="LossDataAnalytics_files/figure-html/ModelValidation-1.png" alt="Model Validation. A data set is randomly split into two subsamples." width="60%" />
<p class="caption">
Figure 4.9: Model Validation. A data set is randomly split into two subsamples.
</p>
</div>
<p>Various researchers recommend different proportions for the allocation. <span class="citation">Snee (<a href="#ref-snee1977validation">1977</a>)</span> suggests that data-splitting not be done unless the sample size is moderately large. The guidelines of <span class="citation">Picard and Berk (<a href="#ref-picard1990data">1990</a>)</span> show that the greater the number of parameters to be estimated, the greater the proportion of observations needed for the model development subsample.</p>
<p><strong>Model Validation Statistics.</strong> Much of the literature supporting the establishment of a model validation process is based on regression and classification models that you can think of as an <em>input-output</em> problem (<span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>). That is, we have several inputs <span class="math inline">\(x_1, \ldots, x_k\)</span> that are related to an output <span class="math inline">\(y\)</span> through a function such as <span class="math display">\[y = \mathrm{g}\left(x_1, \ldots, x_k\right).\]</span> One uses the training sample to develop an estimate of <span class="math inline">\(\mathrm{g}\)</span>, say, <span class="math inline">\(\hat{\mathrm{g}}\)</span>, and then calibrate the distance from the observed outcomes to the predictions using a criterion of the form</p>
<span class="math display" id="eq:OutSampleCriter">\[\begin{equation}
\sum_i \mathrm{d}(y_i,\hat{\mathrm{g}}\left(x_{i1}, \ldots, x_{ik}\right) ) .
\tag{4.4}
\end{equation}\]</span>
<p>Here, the sum <em>i</em> is over the test data. In many regression applications, it is common to use squared Euclidean distance of the form <span class="math inline">\(\mathrm{d}(y_i,\mathrm{g}) = (y_i-\mathrm{g})^2\)</span>. In actuarial applications, Euclidean distance <span class="math inline">\(\mathrm{d}(y_i,\mathrm{g}) = |y_i-\mathrm{g}|\)</span> is often preferred because of the skewed nature of the data (large outlying values of <span class="math inline">\(y\)</span> can have a large effect on the measure). The Chapter 4 <em>Technical Supplement A</em> describes another measure, the <em>Gini index</em>, that is useful in actuarial applications particularly when there is a large proportion of zeros in claims data (corresponding to no claims).</p>
<p><strong>Selecting a Distribution.</strong> Still, our focus so far has been to select a distribution for a data set that can be used for actuarial modeling without additional inputs <span class="math inline">\(x_1, \ldots, x_k\)</span>. Even in this more fundamental problem, the model validation approach is valuable. If we base all inference on only in-sample data, then there is a tendency to select more complicated models then needed. For example, we might select a four parameter GB2, generalized beta of the second kind, distribution when only a two parameter Pareto is needed. Information criteria such as <a href="#" class="tooltip" style="color:green"><em>AIC</em><span style="font-size:8pt"> Akaike’s information criterion</span></a> and <a href="#" class="tooltip" style="color:green"><em>BIC</em><span style="font-size:8pt"> Bayesian information criterion</span></a> included penalties for model complexity and so provide some protection but using a test sample is the best guarantee to achieve parsimonious models. From a quote often attributed to Albert Einstein, we want to “use the simplest model as possible but no simpler.”</p>
</div>
<div id="model-selection-based-on-cross-validation" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Model Selection Based on Cross-Validation</h3>
<p>Although out-of-sample validation is the gold standard in predictive modeling, it is not always practical to do so. The main reason is that we have limited sample sizes and the out-of-sample model selection criterion in equation <a href="C-ModelSelection.html#eq:OutSampleCriter">(4.4)</a> depends on a <em>random</em> split of the data. This means that different analysts, even when working the same data set and same approach to modeling, may select different models. This is likely in actuarial applications because we work with skewed data sets where there is a large chance of getting some very large outcomes and large outcomes may have a great influence on the parameter estimates.</p>
<p><strong>Cross-Validation Procedure.</strong> Alternatively, one may use <strong>cross-validation</strong>, as follows.</p>
<ul>
<li>The procedure begins by using a random mechanism to split the data into <em>K</em> subsets known as <em>folds</em>, where analysts typically use 5 to 10.</li>
<li>Next, one uses the first <em>K</em>-1 subsamples to estimate model parameters. Then, “predict” the outcomes for the <em>K</em>th subsample and use a measure such as in equation <a href="C-ModelSelection.html#eq:OutSampleCriter">(4.4)</a> to summarize the fit.</li>
<li>Now, repeat this by holding out each of the <em>K</em> sub-samples, summarizing with a cumulative out-of-sample statistic.</li>
</ul>
<p>Repeat these steps for several candidate models and choose the model with the lowest cumulative out-of-sample statistic.</p>
<p>Cross-validation is widely used because it retains the predictive flavor of the out-of-sample model validation process but, due to the re-use of the data, is more stable over random samples.</p>
</div>
</div>
<div id="S:MS:ModifiedData" class="section level2">
<h2><span class="header-section-number">4.3</span> Estimation using Modified Data</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Describe grouped, censored, and truncated data</li>
<li>Estimate parametric distributions based on grouped, censored, and truncated data</li>
<li>Estimate distributions nonparametrically based on grouped, censored, and truncated data</li>
</ul>
<hr />
<div id="parametric-estimation-using-modified-data" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Parametric Estimation using Modified Data</h3>
<p>Basic theory and many applications are based on <em>individual</em> observations that are “<em>complete</em>” and “<em>unmodified</em>,” as we have seen in the previous section. Section <a href="C-Severity.html#S:MaxLikeEstimation">3.5</a> introduced the concept of observations that are “<em>modified</em>” due to two common types of limitations: <strong>censoring</strong> and <strong>truncation</strong>. For example, it is common to think about an insurance deductible as producing data that are truncated (from the left) or policy limits as yielding data that are censored (from the right). This viewpoint is from the primary insurer (the seller of the insurance). However, as we will see in Chapter <a href="C-PortMgt.html#C:PortMgt">10</a>, a reinsurer (an insurer of an insurance company) may not observe claims smaller than an amount, only that a claim exists, an example of censoring from the left. So, in this section, we cover the full gamut of alternatives. Specifically, this section will address parametric estimation methods for three alternatives to individual, complete, and unmodified data: <strong>interval-censored</strong> data available only in groups, data that are limited or <strong>censored</strong>, and data that may not be observed due to <strong>truncation</strong>.</p>
<div id="S:MS:GroupedData" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> Parametric Estimation using Grouped Data</h4>
<p>Consider a sample of size <span class="math inline">\(n\)</span> observed from the distribution <span class="math inline">\(F(\cdot)\)</span>, but in groups so that we only know the group into which each observation fell, not the exact value. This is referred to as <strong>grouped</strong> or <strong>interval-censored</strong> data. For example, we may be looking at two successive years of annual employee records. People employed in the first year but not the second have left sometime during the year. With an exact departure date (individual data), we could compute the amount of time that they were with the firm. Without the departure date (grouped data), we only know that they departed sometime during a year-long interval.</p>
<p>Formalizing this idea, suppose there are <span class="math inline">\(k\)</span> groups or intervals delimited by boundaries <span class="math inline">\(c_0 &lt; c_1&lt; \cdots &lt; c_k\)</span>. For each observation, we only observe the interval into which it fell (e.g. <span class="math inline">\((c_{j-1}, c_j)\)</span>), not the exact value. Thus, we only know the number of observations in each interval. The constants <span class="math inline">\(\{c_0 &lt; c_1 &lt; \cdots &lt; c_k\}\)</span> form some partition of the domain of <span class="math inline">\(F(\cdot)\)</span>. Then the probability of an observation <span class="math inline">\(X_i\)</span> falling in the <span class="math inline">\(j\)</span>th interval is <span class="math display">\[\Pr\left(X
_i \in (c_{j-1}, c_j]\right) = F(c_j) - F(c_{j-1}).\]</span></p>
<p>The corresponding probability mass function for an observation is <span class="math display">\[\begin{aligned}
f(x) &amp;=
\begin{cases}
F(c_1) - F(c_{0}) &amp;   \text{if }\ x \in (c_{0}, c_1]\\
\vdots &amp; \vdots \\
F(c_k) - F(c_{k-1}) &amp;   \text{if }\ x \in (c_{k-1}, c_k]\\
\end{cases} \\
&amp;= \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{I(x \in (c_{j-1}, c_j])}
\end{aligned}\]</span></p>
<p>Now, define <span class="math inline">\(n_j\)</span> to be the number of observations that fall in the <span class="math inline">\(j\)</span>th interval, <span class="math inline">\((c_{j-1}, c_j]\)</span>. Thus, the likelihood function (with respect to the parameter(s) <span class="math inline">\(\theta\)</span>) is <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta) = \prod_{j=1}^n f(x_i) = \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{n_j}
\end{aligned}\]</span></p>
<p>And the log-likelihood function is <span class="math display">\[\begin{aligned}
L(\theta) = \ln \mathcal{L}(\theta) = \ln \prod_{j=1}^n f(x_i) = \sum_{j=1}^k n_j \ln \left\{F(c_j) - F(c_{j-1})\right\}
\end{aligned}\]</span></p>
<p>Maximizing the likelihood function (or equivalently, maximizing the log-likelihood function) would then produce the maximum likelihood estimates for grouped data.</p>
<p><strong>Example 4.3.1. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>Losses follow an exponential distribution with mean <span class="math inline">\(\theta\)</span>.</li>
<li>A random sample of 20 losses is distributed as follows:</li>
</ol>
<p><span class="math display">\[
{\small
\begin{array}{l|c}
\hline
\text{Loss Range} &amp; \text{Frequency} \\
\hline 
[0,1000] &amp; 7 \\
(1000,2000] &amp; 6 \\
(2000,\infty) &amp; 7 \\
\hline
\end{array}
}
\]</span></p>
<p>Calculate the maximum likelihood estimate of <span class="math inline">\(\theta\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.1" href="javascript:toggleEX('toggleExampleSelect.3.1','displayTextExampleSelect.3.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.1" style="display: none">
<p><strong>Solution.</strong> <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta) &amp;= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\
&amp;= (1-e^{-1000/\theta})^7(e^{-1000/\theta} - e^{-2000/\theta})^6(e^{-2000/\theta})^7 \\
&amp;= (1-p)^7(p-p^2)^6(p^2)^7 \\
&amp;= p^{20}(1-p)^{13}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(p=e^{-1000/\theta}\)</span>. Maximizing this expression with respect to <span class="math inline">\(p\)</span> is equivalent to maximizing the likelihood with respect to <span class="math inline">\(\theta\)</span>. The maximum occurs at <span class="math inline">\(p=\frac{20}{33}\)</span> and so <span class="math inline">\(\hat{\theta}=\frac{-1000}{\ln(20/33)}= 1996.90\)</span>.</p>
</div>
<hr />
</div>
<div id="censored-data" class="section level4">
<h4><span class="header-section-number">4.3.1.2</span> Censored Data</h4>
<p><strong>Censoring</strong> occurs when we record only a limited value of an observation. The most common form is <strong>right-censoring</strong>, in which we record the <em>smaller</em> of the “true” dependent variable and a censoring variable. Using notation, let <span class="math inline">\(X\)</span> represent an outcome of interest, such as the loss due to an insured event or time until an event. Let <span class="math inline">\(C_U\)</span> denote the censoring amount. With right-censored observations, we record <span class="math inline">\(X_U^{\ast}= \min(X, C_U) = X \wedge C_U\)</span>. We also record whether or not censoring has occurred. Let <span class="math inline">\(\delta_U= I(X \leq C_U)\)</span> be a binary variable that is 0 if censoring occurs and 1 if it does not.</p>
<p>For an example that we saw in Section <a href="C-Severity.html#S:PolicyLimits">3.4.2</a>, <span class="math inline">\(C_U\)</span> may represent the upper limit of coverage of an insurance policy (we used <span class="math inline">\(u\)</span> for the upper limit in that section). The loss may exceed the amount <span class="math inline">\(C_U\)</span>, but the insurer only has <span class="math inline">\(C_U\)</span> in its records as the amount paid out and does not have the amount of the actual loss <span class="math inline">\(X\)</span> in its records.</p>
<p>Similarly, with <strong>left-censoring</strong>, we record the <em>larger</em> of a variable of interest and a censoring variable. If <span class="math inline">\(C_L\)</span> is used to represent the censoring amount, we record <span class="math inline">\(X_L^{\ast}= \max(X, C_L)\)</span> along with the censoring indicator <span class="math inline">\(\delta_L= I(X \geq C_L)\)</span>.</p>
<p>As an example, you got a brief introduction to reinsurance, insurance for insurers, in Section <a href="C-Severity.html#S:Chap3Reinsurance">3.4.4</a> and will see more in Chapter <a href="C-PortMgt.html#C:PortMgt">10</a>. Suppose a reinsurer will cover insurer losses greater than <span class="math inline">\(C_L\)</span>; this means that the reinsurer is responsible for the excess of <span class="math inline">\(X_L^{\ast}\)</span> over <span class="math inline">\(C_L\)</span>. Using notation, this is <span class="math inline">\(Y = X_L^{\ast} - C_L\)</span>. To see this, first consider the case where the policyholder loss <span class="math inline">\(X &lt; C_L\)</span>. Then, the insurer will pay the entire claim and <span class="math inline">\(Y =C_L- C_L=0\)</span>, no loss for the reinsurer. For the second case, if the loss <span class="math inline">\(X \ge C_L\)</span>, then <span class="math inline">\(Y = X-C_L\)</span> represents the reinsurer’s retained claims. Put another way, if a loss occurs, the reinsurer records the actual amount if it exceeds the limit <span class="math inline">\(C_L\)</span> and otherwise it only records that it had a loss of <span class="math inline">\(0\)</span>.</p>
</div>
<div id="truncated-data" class="section level4">
<h4><span class="header-section-number">4.3.1.3</span> Truncated Data</h4>
<p>Censored observations are recorded for study, although in a limited form. In contrast, <strong>truncated</strong> outcomes are a type of missing data. An outcome is potentially truncated when the availability of an observation depends on the outcome.</p>
<p>In insurance, it is common for observations to be <strong>left-truncated</strong> at <span class="math inline">\(C_L\)</span> when the amount is <span class="math display">\[
\begin{aligned}
Y &amp;=
\left\{
\begin{array}{cl}
\text{we do not observe }X &amp; X &lt; C_L \\
X &amp; X \geq C_L
\end{array}
\right.\end{aligned} .
\]</span></p>
<p>In other words, if <span class="math inline">\(X\)</span> is less than the threshold <span class="math inline">\(C_L\)</span>, then it is not observed.</p>
<p>For an example we saw in Section <a href="C-Severity.html#S:PolicyDeduct">3.4.1</a>, <span class="math inline">\(C_L\)</span> may represent the deductible of an insurance policy (we used <span class="math inline">\(d\)</span> for the deductible in that section). If the insured loss is less than the deductible, then the insurer may not observe or record the loss at all. If the loss exceeds the deductible, then the excess <span class="math inline">\(X-C_L\)</span> is the claim that the insurer covers. In Section <a href="C-Severity.html#S:PolicyDeduct">3.4.1</a>, we defined the per payment loss to be <span class="math display">\[
Y^{P} = \left\{ \begin{matrix}
\text{Undefined} &amp; X \le d \\
X - d &amp; X &gt; d 
\end{matrix} \right. ,
\]</span> so that if a loss exceeds a deductible, we record the excess amount <span class="math inline">\(X-d\)</span>. This is very important when considering amounts that the insurer will pay. However, for estimation purposes of this section, it matters little if we substract a known constant such as <span class="math inline">\(C_L=d\)</span>. So, for our truncated variable <span class="math inline">\(Y\)</span>, we use the simpler convention and do not subract <span class="math inline">\(d\)</span>.</p>
<p>Similarly for <strong>right-truncated</strong> data, if <span class="math inline">\(X\)</span> exceeds a threshold <span class="math inline">\(C_U\)</span>, then it is not observed. In this case, the amount is <span class="math display">\[
\begin{aligned}
Y &amp;=
\left\{
\begin{array}{cl}
X &amp; X \leq C_U \\
\text{we do not observe }X &amp; X &gt; C_U.
\end{array}
\right.\end{aligned}
\]</span></p>
<p>Classic examples of truncation from the right include <span class="math inline">\(X\)</span> as a measure of distance to a star. When the distance exceeds a certain level <span class="math inline">\(C_U\)</span>, the star is no longer observable.</p>
<p>Figure <a href="C-ModelSelection.html#fig:CensorTrunc">4.10</a> compares truncated and censored observations. Values of <span class="math inline">\(X\)</span> that are greater than the “upper” censoring limit <span class="math inline">\(C_U\)</span> are not observed at all (right-censored), while values of <span class="math inline">\(X\)</span> that are smaller than the “lower” truncation limit <span class="math inline">\(C_L\)</span> are observed, but observed as <span class="math inline">\(C_L\)</span> rather than the actual value of <span class="math inline">\(X\)</span> (left-truncated).</p>
<div class="figure" style="text-align: center"><span id="fig:CensorTrunc"></span>
<img src="LossDataAnalytics_files/figure-html/CensorTrunc-1.png" alt="Censoring and Truncation" width="60%" />
<p class="caption">
Figure 4.10: Censoring and Truncation
</p>
</div>
<hr />
<h5 style="text-align: center;">
<a id="displayTextExampleMort.4f" href="javascript:toggleEX('toggleExampleMort','displayTextExampleMort.4f');"><i><strong>Show Mortality Study Example </strong></i></a>
</h5>
<div id="toggleExampleMort" style="display: none">
<p><strong>Example – Mortality Study.</strong> Suppose that you are conducting a two-year study of mortality of high-risk subjects, beginning January 1, 2010 and finishing January 1, 2012. Figure <a href="C-ModelSelection.html#fig:Mortality">4.11</a> graphically portrays the six types of subjects recruited. For each subject, the beginning of the arrow represents that the the subject was recruited and the arrow end represents the event time. Thus, the arrow represents exposure time.</p>
<div class="figure" style="text-align: center"><span id="fig:Mortality"></span>
<img src="LossDataAnalytics_files/figure-html/Mortality-1.png" alt="Timeline for Several Subjects on Test in a Mortality Study" width="60%" />
<p class="caption">
Figure 4.11: Timeline for Several Subjects on Test in a Mortality Study
</p>
</div>
<ul>
<li><strong>Type A - Right-censored.</strong> This subject is alive at the beginning and the end of the study. Because the time of death is not known by the end of the study, it is right-censored. Most subjects are Type A.</li>
<li><strong>Type B - Complete</strong> information is available for a type B subject. The subject is alive at the beginning of the study and the death occurs within the observation period.</li>
<li><strong>Type C - Right-censored and left-truncated.</strong> A type C subject is right-censored, in that death occurs after the observation period. However, the subject entered after the start of the study and is said to have a <em>delayed entry time</em>. Because the subject would not have been observed had death occurred before entry, it is left-truncated.</li>
<li><strong>Type D - Left-truncated.</strong> A type D subject also has delayed entry. Because death occurs within the observation period, this subject is not right censored.</li>
<li><strong>Type E - Left-truncated.</strong> A type E subject is not included in the study because death occurs prior to the observation period.</li>
<li><strong>Type F - Right-truncated.</strong> Similarly, a type F subject is not included because the entry time occurs after the observation period.</li>
</ul>
</div>
<hr />
<p>To summarize, for outcome <span class="math inline">\(X\)</span> and constants <span class="math inline">\(C_L\)</span> and <span class="math inline">\(C_U\)</span>,</p>
<table>
<thead>
<tr class="header">
<th align="center">Limitation Type</th>
<th align="center">Limited Variable</th>
<th align="center">Recording Information</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">right censoring</td>
<td align="center"><span class="math inline">\(X_U^{\ast}= \min(X, C_U)\)</span></td>
<td align="center"><span class="math inline">\(\delta_U= I(X \leq C_U)\)</span></td>
</tr>
<tr class="even">
<td align="center">left censoring</td>
<td align="center"><span class="math inline">\(X_L^{\ast}= \max(X, C_L)\)</span></td>
<td align="center"><span class="math inline">\(\delta_L= I(X \geq C_L)\)</span></td>
</tr>
<tr class="odd">
<td align="center">interval censoring</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">right truncation</td>
<td align="center"><span class="math inline">\(X\)</span></td>
<td align="center">observe <span class="math inline">\(X\)</span> if <span class="math inline">\(X \leq C_U\)</span></td>
</tr>
<tr class="odd">
<td align="center">left truncation</td>
<td align="center"><span class="math inline">\(X\)</span></td>
<td align="center">observe <span class="math inline">\(X\)</span> if <span class="math inline">\(X \geq C_L\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="parametric-estimation-using-censored-and-truncated-data" class="section level4">
<h4><span class="header-section-number">4.3.1.4</span> Parametric Estimation using Censored and Truncated Data</h4>
<p>For simplicity, we assume non-random censoring amounts and a continuous outcome <span class="math inline">\(X\)</span>. To begin, consider the case of right-censored data where we record <span class="math inline">\(X_U^{\ast}= \min(X, C_U)\)</span> and censoring indicator <span class="math inline">\(\delta= I(X \leq C_U)\)</span>. If censoring occurs so that <span class="math inline">\(\delta=0\)</span>, then <span class="math inline">\(X \geq C_U\)</span> and the likelihood is <span class="math inline">\(\Pr(X \geq C_U) = 1-F(C_U)\)</span>. If censoring does not occur so that <span class="math inline">\(\delta=1\)</span>, then <span class="math inline">\(X &lt; C_U\)</span> and the likelihood is <span class="math inline">\(f(x)\)</span>. Summarizing, we have the likelihood of a single observation as</p>
<p><span class="math display">\[
\begin{aligned}
\left\{
\begin{array}{ll}
1-F(C_U) &amp; \text{if }\delta=0 \\
f(x) &amp; \text{if } \delta = 1 
\end{array}
\right. = \left\{ f(x)\right\}^{\delta} \left\{1-F(C_U)\right\}^{1-\delta} .
\end{aligned}
\]</span></p>
<p>The right-hand expression allows us to present the likelihood more compactly. Now, for an <em>iid</em> sample of size <span class="math inline">\(n\)</span>, the likelihood is</p>
<p><span class="math display">\[
\mathcal{L} = 
\prod_{i=1}^n \left\{ f(x_i)\right\}^{\delta_i} \left\{1-F(C_{Ui})\right\}^{1-\delta_i} = \prod_{\delta_i=1} f(x_i) \prod_{\delta_i=0} \{1-F(C_{Ui})\},
\]</span></p>
<p>with potential censoring times <span class="math inline">\(\{ C_{U1}, \ldots,C_{Un} \}\)</span>. Here, the notation “<span class="math inline">\(\prod_{\delta_i=1}\)</span>” means to take the product over uncensored observations, and similarly for “<span class="math inline">\(\prod_{\delta_i=0}\)</span>.”</p>
<p>On the other hand, truncated data are handled in likelihood inference via conditional probabilities. Specifically, we adjust the likelihood contribution by dividing by the probability that the variable was observed. To summarize, we have the following contributions to the likelihood function for six types of outcomes:</p>
<p><span class="math display">\[
{\small
\begin{array}{lc}
\hline
\text{Outcome} &amp; \text{Likelihood Contribution} \\
\hline 
\text{exact value} &amp; f(x) \\
\text{right-censoring} &amp; 1-F(C_U) \\
\text{left-censoring} &amp; F(C_L) \\
\text{right-truncation} &amp; f(x)/F(C_U) \\
\text{left-truncation} &amp; f(x)/(1-F(C_L)) \\
\text{interval-censoring} &amp; F(C_U)-F(C_L) \\
\hline
\end{array}
}
\]</span></p>
<p>For known outcomes and censored data, the likelihood is <span class="math display">\[\mathcal{L}(\theta) = \prod_{E} f(x_i) \prod_{R} \{1-F(C_{Ui})\} \prod_{L}
F(C_{Li}) \prod_{I} (F(C_{Ui})-F(C_{Li})),\]</span> where “<span class="math inline">\(\prod_{E}\)</span>” is the product over observations with <em>E</em>xact values, and similarly for <em>R</em>ight-, <em>L</em>eft- and <em>I</em>nterval-censoring.</p>
<p>For right-censored and left-truncated data, the likelihood is <span class="math display">\[\mathcal{L} = \prod_{E} \frac{f(x_i)}{1-F(C_{Li})} \prod_{R} \frac{1-F(C_{Ui})}{1-F(C_{Li})},\]</span> and similarly for other combinations. To get further insights, consider the following.</p>
<hr />
<h5 style="text-align: center;">
<a id="displayTextExampleEXP.4f" href="javascript:toggleEX('toggleExampleEXP','displayTextExampleEXP.4f');"><i><strong>Show Special Case - Exponential Distribution</strong></i></a>
</h5>
<div id="toggleExampleEXP" style="display: none">
<p><strong>Special Case: Exponential Distribution.</strong> Consider data that are right-censored and left-truncated, with random variables <span class="math inline">\(X_i\)</span> that are exponentially distributed with mean <span class="math inline">\(\theta\)</span>. With these specifications, recall that <span class="math inline">\(f(x) = \theta^{-1} \exp(-x/\theta)\)</span> and <span class="math inline">\(F(x) = 1-\exp(-x/\theta)\)</span>.</p>
<p>For this special case, the log-likelihood is</p>
<p><span class="math display">\[
\begin{aligned}
L(\theta) &amp;= \sum_{E} \left\{ \ln f(x_i) - \ln (1-F(C_{Li})) \right\} + \sum_{R}\left\{ \ln (1-F(C_{Ui}))- \ln (1-\mathrm{F}(C_{Li})) \right\}\\
&amp;= \sum_{E} (-\ln \theta -(x_i-C_{Li})/\theta ) -\sum_{R} (C_{Ui}-C_{Li})/\theta .
\end{aligned}
\]</span></p>
<p>To simplify the notation, define <span class="math inline">\(\delta_i = I(X_i \geq C_{Ui})\)</span> to be a binary variable that indicates right-censoring. Let <span class="math inline">\(X_i^{\ast \ast} = \min(X_i, C_{Ui}) - C_{Li}\)</span> be the amount that the observed variable exceeds the lower truncation limit. With this, the log-likelihood is</p>
<span class="math display" id="eq:EXPloglik">\[\begin{equation} 
  L(\theta) =  - \sum_{i=1}^n ((1-\delta_i) \ln \theta + \frac{x_i^{\ast \ast}}{\theta})
  \tag{4.5}
\end{equation}\]</span>
<p>Taking derivatives with respect to the parameter <span class="math inline">\(\theta\)</span> and setting it equal to zero yields the maximum likelihood estimator</p>
<p><span class="math display">\[\widehat{\theta}  = \frac{1}{n_u} \sum_{i=1}^n  x_i^{\ast \ast},\]</span></p>
<p>where <span class="math inline">\(n_u = \sum_i (1-\delta_i)\)</span> is the number of uncensored observations.</p>
</div>
<hr />
<p><strong>Example 4.3.2. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>A sample of losses is: 600 700 900</li>
<li>No information is available about losses of 500 or less.</li>
<li>Losses are assumed to follow an exponential distribution with mean <span class="math inline">\(\theta\)</span>.</li>
</ol>
<p>Calculate the maximum likelihood estimate of <span class="math inline">\(\theta\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.2" href="javascript:toggleEX('toggleExampleSelect.3.2','displayTextExampleSelect.3.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.2" style="display: none">
<p><strong>Solution.</strong> These observations are truncated at 500. The contribution of each observation to the likelihood function is <span class="math display">\[\frac{f(x)}{1-F(500)} = \frac{\theta^{-1}e^{-x/\theta}}{e^{-500/\theta}}\]</span></p>
<p>Then the likelihood function is</p>
<p><span class="math display">\[\mathcal{L}(\theta)= \frac{\theta^{-1} e^{-600/\theta} \theta^{-1} e^{-700/\theta} \theta^{-1} e^{-900/\theta}}{(e^{-500/\theta})^3} = \theta^{-3}e^{-700/\theta}\]</span></p>
<p>The log-likelihood is</p>
<p><span class="math display">\[L(\theta) = \ln\mathcal{L}(\theta) = -3\ln \theta - 700\theta^{-1}\]</span></p>
<p>Maximizing this expression by setting the derivative with respect to <span class="math inline">\(\theta\)</span> equal to 0, we have</p>
<p><span class="math display">\[L&#39;(\theta) = -3\theta^{-1} + 700\theta^{-2} = 0 \ \Rightarrow \ \hat{\theta} = \frac{700}{3} = 233.33\]</span></p>
</div>
<hr />
<p><strong>Example 4.3.3. Actuarial Exam Question.</strong> You are given the following information about a random sample:</p>
<ol style="list-style-type: lower-roman">
<li>The sample size equals five.</li>
<li>The sample is from a Weibull distribution with <span class="math inline">\(\tau=2\)</span>.</li>
<li>Two of the sample observations are known to exceed 50, and the remaining three observations are 20, 30, and 45.</li>
</ol>
<p>Calculate the maximum likelihood estimate of <span class="math inline">\(\theta\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.3" href="javascript:toggleEX('toggleExampleSelect.3.3','displayTextExampleSelect.3.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.3" style="display: none">
<p><strong>Solution.</strong> The likelihood function is</p>
<p><span class="math display">\[
\begin{aligned} 
\mathcal{L}(\theta) &amp;= f(20) f(30) f(45) [1-F(50)]^2 \\
&amp;= \frac{2(20/\theta)^2 e^{-(20/\theta)^2}}{20} \frac{2(30/\theta)^2 e^{-(30/\theta)^2}}{30} \frac{2(45/\theta)^2 e^{-(45/\theta)^2}}{45}(e^{-(50/\theta)^2})^2 \\
&amp;\propto \frac{1}{\theta^6} e^{-8325/\theta^2}
\end{aligned}
\]</span></p>
<p>The natural logarithm of the above expression is <span class="math inline">\(-6\ln\theta - \frac{8325}{\theta^2}\)</span>. Maximizing this expression by setting its derivative to 0, we get</p>
<p><span class="math display">\[\frac{-6}{\theta} + \frac{16650}{\theta^3} = 0 \ \Rightarrow \ \hat{\theta} = \left(\frac{16650}{6}\right)^{\frac{1}{2}} = 52.6783\]</span></p>
</div>
<hr />
</div>
</div>
<div id="nonparametric-estimation-using-modified-data" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Nonparametric Estimation using Modified Data</h3>
<p>Nonparametric estimators provide useful benchmarks, so it is helpful to understand the estimation procedures for grouped, censored, and truncated data.</p>
<div id="grouped-data" class="section level4">
<h4><span class="header-section-number">4.3.2.1</span> Grouped Data</h4>
<p>As we have seen in Section <a href="C-ModelSelection.html#S:MS:GroupedData">4.3.1.1</a>, observations may be grouped (also referred to as interval censored) in the sense that we only observe them as belonging in one of <span class="math inline">\(k\)</span> intervals of the form <span class="math inline">\((c_{j-1}, c_j]\)</span>, for <span class="math inline">\(j=1, \ldots, k\)</span>. At the boundaries, the empirical distribution function is defined in the usual way: <span class="math display">\[
F_n(c_j) = \frac{\text{number of observations } \le c_j}{n}.
\]</span></p>
<p>For other values of <span class="math inline">\(x \in (c_{j-1}, c_j)\)</span>, we can estimate the distribution function with the <em>ogive</em> estimator, which linearly interpolates between <span class="math inline">\(F_n(c_{j-1})\)</span> and <span class="math inline">\(F_n(c_j)\)</span>, i.e. the values of the boundaries <span class="math inline">\(F_n(c_{j-1})\)</span> and <span class="math inline">\(F_n(c_j)\)</span> are connected with a straight line. This can formally be expressed as <span class="math display">\[F_n(x) = \frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \ \ \ \text{for } c_{j-1} \le x &lt; c_j\]</span></p>
<p>The corresponding density is <span class="math display">\[f_n(x) = F^{\prime}_n(x) = \frac{F_n(c_j)-F_n(c_{j-1})}{c_j - c_{j-1}} \ \ \  \text{for } c_{j-1} \le x &lt; c_j .\]</span></p>
<hr />
<p><strong>Example 4.3.4. Actuarial Exam Question.</strong> You are given the following information regarding claim sizes for 100 claims:</p>
<p><span class="math display">\[
{\small
\begin{array}{r|c}
\hline
\text{Claim Size} &amp;  \text{Number of Claims} \\
\hline
0 - 1,000 &amp; 16 \\
1,000 - 3,000 &amp; 22 \\
3,000 - 5,000 &amp; 25 \\
5,000 - 10,000 &amp; 18 \\
10,000 - 25,000 &amp; 10 \\
25,000 - 50,000 &amp; 5 \\
50,000 - 100,000 &amp; 3 \\
\text{over  } 100,000 &amp; 1 \\
\hline
\end{array}
}
\]</span></p>
<p>Using the ogive, calculate the estimate of the probability that a randomly chosen claim is between 2000 and 6000.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.4" href="javascript:toggleEX('toggleExampleSelect.3.4','displayTextExampleSelect.3.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.4" style="display: none">
<p><strong>Solution.</strong> At the boundaries, the empirical distribution function is defined in the usual way, so we have <span class="math display">\[F_{100}(1000) = 0.16, \ F_{100}(3000)=0.38, \ F_{100}(5000)=0.63, \ F_{100}(10000)=0.81\]</span> For other claim sizes, the ogive estimator linearly interpolates between these values: <span class="math display">\[F_{100}(2000) = 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27\]</span> <span class="math display">\[F_{100}(6000)=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666\]</span> Thus, the probability that a claim is between 2000 and 6000 is <span class="math inline">\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\)</span>.</p>
</div>
<hr />
</div>
<div id="right-censored-empirical-distribution-function" class="section level4">
<h4><span class="header-section-number">4.3.2.2</span> Right-Censored Empirical Distribution Function</h4>
<p>It can be useful to calibrate parametric estimators with nonparametric methods that do not rely on a parametric form of the distribution. The product-limit estimator due to <span class="citation">(Kaplan and Meier <a href="#ref-kaplan1958">1958</a>)</span> is a well-known estimator of the distribution function in the presence of censoring.</p>
<p><strong>Motivation for the Kaplan-Meier Product Limit Estimator.</strong> To explain why the product-limit works so well with censored observations, let us first return to the “usual” case without censoring. Here, the empirical distribution function <span class="math inline">\(F_n(x)\)</span> is an <em>unbiased</em> estimator of the distribution function <span class="math inline">\(F(x)\)</span>. This is because <span class="math inline">\(F_n(x)\)</span> is the average of indicator variables each of which are unbiased, that is, <span class="math inline">\(\mathrm{E~} I(X_i \le x) = \Pr(X_i \le x) = F(x)\)</span>.</p>
<p>Now suppose the the random outcome is censored on the right by a limiting amount, say, <span class="math inline">\(C_U\)</span>, so that we record the smaller of the two, <span class="math inline">\(X^* = \min(X, C_U)\)</span>. For values of <span class="math inline">\(x\)</span> that are smaller than <span class="math inline">\(C_U\)</span>, the indicator variable still provides an unbiased estimator of the distribution function before we reach the censoring limit. That is, <span class="math inline">\(\mathrm{E~} I(X^* \le x) = F(x)\)</span> because <span class="math inline">\(I(X^* \le x) = I(X \le x)\)</span> for <span class="math inline">\(x &lt; C_U\)</span>. In the same way, <span class="math inline">\(\mathrm{E~} I(X^* &gt; x) = 1 -F(x) = S(x)\)</span>. But, for <span class="math inline">\(x&gt;C_U\)</span>, <span class="math inline">\(I(X^* \le x)\)</span> is in general not an unbiased estimator of <span class="math inline">\(F(x)\)</span>.</p>
<p>As an alternative, consider <em>two</em> random variables that have different censoring limits. For illustration, suppose that we observe <span class="math inline">\(X_1^* = \min(X_1, 5)\)</span> and <span class="math inline">\(X_2^* = \min(X_2, 10)\)</span> where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent draws from the same distribution. For <span class="math inline">\(x \le 5\)</span>, the empirical distribution function <span class="math inline">\(F_2(x)\)</span> is an unbiased estimator of <span class="math inline">\(F(x)\)</span>. However, for <span class="math inline">\(5 &lt; x \le 10\)</span>, the first observation cannot be used for the distribution function because of the censoring limitation. Instead, the strategy developed by <span class="citation">(Kaplan and Meier <a href="#ref-kaplan1958">1958</a>)</span> is to use <span class="math inline">\(S_2(5)\)</span> as an estimator of <span class="math inline">\(S(5)\)</span> and then to use the second observation to estimate the survival function conditional on survival to time 5, <span class="math inline">\(\Pr(X &gt; x | X &gt;5) = \frac{S(x)}{S(5)}\)</span>. Specifically, for <span class="math inline">\(5 &lt; x \le 10\)</span>, the estimator of the survival function is <span class="math display">\[
\hat{S}(x) = S_2(5) \times I(X_2^* &gt; x ) .
\]</span></p>
<p><strong>Kaplan-Meier Product Limit Estimator.</strong> Extending this idea, for each observation <span class="math inline">\(i\)</span>, let <span class="math inline">\(u_i\)</span> be the upper censoring limit (<span class="math inline">\(=\infty\)</span> if no censoring). Thus, the recorded value is <span class="math inline">\(x_i\)</span> in the case of no censoring and <span class="math inline">\(u_i\)</span> if there is censoring. Let <span class="math inline">\(t_{1} &lt;\cdots&lt; t_{k}\)</span> be <span class="math inline">\(k\)</span> distinct points at which an uncensored loss occurs, and let <span class="math inline">\(s_j\)</span> be the number of uncensored losses <span class="math inline">\(x_i\)</span>’s at <span class="math inline">\(t_{j}\)</span>. The corresponding <strong>risk set</strong> is the number of observations that are active (not censored) at a value <em>less than</em> <span class="math inline">\(t_{j}\)</span>, denoted as <span class="math inline">\(R_j = \sum_{i=1}^n I(x_i \geq t_{j}) + \sum_{i=1}^n I(u_i \geq t_{j})\)</span>.</p>
<p>With this notation, the <strong>product-limit estimator</strong> of the distribution function is</p>
<span class="math display" id="eq:KaplanMeier">\[\begin{equation}
\hat{F}(x) =
\left\{
\begin{array}{ll}
0 &amp; x&lt;t_{1} \\
1-\prod_{j:t_{j} \leq x}\left( 1-\frac{s_j}{R_{j}}\right) &amp; x \geq t_{1} 
\end{array}
\right. .\tag{4.6}
\end{equation}\]</span>
<p>As usual, the corresponding estimate of the survival function is <span class="math inline">\(\hat{S}(x) = 1 - \hat{F}(x)\)</span>.</p>
<hr />
<p><strong>Example 4.3.5. Actuarial Exam Question.</strong> The following is a sample of 10 payments:</p>
<p><span class="math display">\[\begin{array}{cccccccccc}
4 &amp;4 &amp;5+ &amp;5+ &amp;5+ &amp;8 &amp;10+ &amp;10+ &amp;12 &amp;15 \\
\end{array}\]</span></p>
<p>where <span class="math inline">\(+\)</span> indicates that a loss has exceeded the policy limit.</p>
<p>Using the Kaplan-Meier product-limit estimator, calculate the probability that the loss on a policy exceeds 11, <span class="math inline">\(\hat{S}(11)\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.5" href="javascript:toggleEX('toggleExampleSelect.3.5','displayTextExampleSelect.3.5');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.5" style="display: none">
<p><strong>Solution.</strong> There are four event times (non-censored observations). For each time <span class="math inline">\(t_j\)</span>, we can calculate the number of events <span class="math inline">\(s_j\)</span> and the risk set <span class="math inline">\(R_j\)</span> as the following:</p>
<p><span class="math display">\[\begin{array}{cccc}
\hline
j &amp; t_j &amp; s_j &amp; R_j \\
\hline
1 &amp; 4 &amp; 2 &amp; 10 \\
2 &amp; 8 &amp; 1 &amp; 5 \\
3 &amp; 12 &amp; 1 &amp; 2 \\
4 &amp; 15 &amp; 1 &amp; 1 \\
\hline
\end{array}\]</span></p>
<p>Thus, the Kaplan-Meier estimate of <span class="math inline">\(S(11)\)</span> is <span class="math display">\[\begin{aligned}
\hat{S}(11) &amp;= \prod_{j:t_j\leq 11} \left( 1- \frac{s_j}{R_j} \right) =  \prod_{j=1}^{2} \left( 1- \frac{s_j}{R_j} \right)\\
&amp;= \left(1-\frac{2}{10} \right) \left(1-\frac{1}{5} \right) = (0.8)(0.8)= 0.64. \\
\end{aligned}\]</span></p>
</div>
<hr />
</div>
<div id="right-censored-left-truncated-empirical-distribution-function" class="section level4">
<h4><span class="header-section-number">4.3.2.3</span> Right-Censored, Left-Truncated Empirical Distribution Function</h4>
<p>In addition to right-censoring, we now extend the framework to allow for left-truncated data. As before, for each observation <span class="math inline">\(i\)</span>, let <span class="math inline">\(u_i\)</span> be the upper censoring limit (<span class="math inline">\(=\infty\)</span> if no censoring). Further, let <span class="math inline">\(d_i\)</span> be the lower truncation limit (0 if no truncation). Thus, the recorded value (if it is greater than <span class="math inline">\(d_i\)</span>) is <span class="math inline">\(x_i\)</span> in the case of no censoring and <span class="math inline">\(u_i\)</span> if there is censoring. Let <span class="math inline">\(t_{1} &lt;\cdots&lt; t_{k}\)</span> be <span class="math inline">\(k\)</span> distinct points at which an event of interest occurs, and let <span class="math inline">\(s_j\)</span> be the number of recorded events <span class="math inline">\(x_i\)</span>’s at time point <span class="math inline">\(t_{j}\)</span>. The corresponding risk set is <span class="math display">\[R_j = \sum_{i=1}^n I(x_i \geq t_{j}) + \sum_{i=1}^n I(u_i \geq t_{j}) - \sum_{i=1}^n I(d_i \geq t_{j}).\]</span></p>
<p>With this new definition of the risk set, the product-limit estimator of the distribution function is as in equation <a href="C-ModelSelection.html#eq:KaplanMeier">(4.6)</a>.</p>
<p><strong>Greenwood’s Formula</strong>. <span class="citation">(Greenwood <a href="#ref-greenwood1926">1926</a>)</span> derived the formula for the estimated variance of the product-limit estimator to be</p>
<p><span class="math display">\[\widehat{Var}(\hat{F}(x)) = (1-\hat{F}(x))^{2} \sum _{j:t_{j} \leq x} \dfrac{s_j}{R_{j}(R_{j}-s_j)}.\]</span></p>
<p><code>R</code>‘s <code>survfit</code> method takes a survival data object and creates a new object containing the Kaplan-Meier estimate of the survival function along with confidence intervals. The Kaplan-Meier method (<code>type='kaplan-meier'</code>) is used by default to construct an estimate of the survival curve. The resulting discrete survival function has point masses at the observed event times (discharge dates) <span class="math inline">\(t_j\)</span>, where the probability of an event given survival to that duration is estimated as the number of observed events at the duration <span class="math inline">\(s_j\)</span> divided by the number of subjects exposed or ’at-risk’ just prior to the event duration <span class="math inline">\(R_j\)</span>.</p>
<p>Two alternate types of estimation are also available for the <code>survfit</code> method. The alternative (<code>type='fh2'</code>) handles ties, in essence, by assuming that multiple events at the same duration occur in some arbitrary order. Another alternative (<code>type='fleming-harrington'</code>) uses the Nelson-Äalen (see <span class="citation">(Aalen <a href="#ref-aalen1978">1978</a>)</span>) estimate of the <strong>cumulative hazard function</strong> to obtain an estimate of the survival function. The estimated cumulative hazard <span class="math inline">\(\hat{H}(x)\)</span> starts at zero and is incremented at each observed event duration <span class="math inline">\(t_j\)</span> by the number of events <span class="math inline">\(s_j\)</span> divided by the number at risk <span class="math inline">\(R_j\)</span>. With the same notation as above, the <strong>Nelson-Äalen</strong> estimator of the distribution function is</p>
<p><span class="math display">\[
\begin{aligned}
\hat{F}_{NA}(x) &amp;=
\left\{
\begin{array}{ll}
0 &amp; x&lt;t_{1} \\
1- \exp \left(-\sum_{j:t_{j} \leq x}\frac{s_j}{R_j} \right) &amp; x \geq t_{1} 
\end{array}
\right. .\end{aligned}
\]</span></p>
<p>Note that the above expression is a result of the Nelson-Äalen estimator of the cumulative hazard function <span class="math display">\[\hat{H}(x)=\sum_{j:t_j\leq x}  \frac{s_j}{R_j} \]</span> and the relationship between the survival function and cumulative hazard function, <span class="math inline">\(\hat{S}_{NA}(x)=e^{-\hat{H}(x)}\)</span>.</p>
<hr />
<p><strong>Example 4.3.6. Actuarial Exam Question.</strong> For observation <span class="math inline">\(i\)</span> of a survival study:</p>
<ul>
<li><span class="math inline">\(d_i\)</span> is the left truncation point</li>
<li><span class="math inline">\(x_i\)</span> is the observed value if not right censored</li>
<li><span class="math inline">\(u_i\)</span> is the observed value if right censored</li>
</ul>
<p>You are given:</p>
<p><span class="math display">\[
{\small
\begin{array}{c|cccccccccc}
\hline
\text{Observation } (i) &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10\\ \hline
d_i &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.3 &amp; 1.5 &amp; 1.6\\
x_i &amp; 0.9 &amp; - &amp; 1.5 &amp; - &amp; - &amp; 1.7 &amp; - &amp; 2.1 &amp; 2.1 &amp; - \\
u_i &amp; - &amp; 1.2 &amp; - &amp; 1.5 &amp; 1.6 &amp; - &amp; 1.7 &amp; - &amp; - &amp; 2.3 \\
\hline
\end{array}
}
\]</span></p>
<p>Calculate the Kaplan-Meier product-limit estimate, <span class="math inline">\(\hat{S}(1.6)\)</span></p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.6" href="javascript:toggleEX('toggleExampleSelect.3.6','displayTextExampleSelect.3.6');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.6" style="display: none">
<p><strong>Solution.</strong> Recall the risk set <span class="math inline">\(R_j = \sum_{i=1}^n \left\{ I(x_i \geq t_{j}) + I(u_i \geq t_{j}) - I(d_i \geq t_{j}) \right\}\)</span>. Then</p>
<p><span class="math display">\[\begin{array}{ccccc}
\hline
j &amp; t_j &amp; s_j &amp; R_j &amp; \hat{S}(t_j) \\
\hline 
1  &amp; 0.9   &amp; 1   &amp; 10-3 = 7 &amp; 1-\frac{1}{7} = \frac{6}{7} \\
2  &amp; 1.5   &amp; 1   &amp; 8-2 = 6  &amp; \frac{6}{7}\left( 1 - \frac{1}{6} \right) = \frac{5}{7}\\
3  &amp; 1.7   &amp; 1   &amp; 5-0 = 5  &amp; \frac{5}{7}\left( 1 - \frac{1}{5} \right) = \frac{4}{7}\\
4  &amp; 2.1   &amp; 2   &amp; 3        &amp; \frac{4}{7}\left( 1 - \frac{2}{3}\right) = \frac{4}{21}\\
\hline
\end{array}\]</span></p>
<p>The Kaplan-Meier estimate is therefore <span class="math inline">\(\hat{S}(1.6) = \frac{5}{7}\)</span>.</p>
</div>
<hr />
<p><strong>Example 4.3.7. Actuarial Exam Question. - Continued.</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Using the Nelson-Äalen estimator, calculate the probability that the loss on a policy exceeds 11, <span class="math inline">\(\hat{S}_{NA}(11)\)</span>.</li>
<li>Calculate Greenwood’s approximation to the variance of the product-limit estimate <span class="math inline">\(\hat{S}(11)\)</span>.</li>
</ol>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.3.7" href="javascript:toggleEX('toggleExampleSelect.3.7','displayTextExampleSelect.3.7');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.3.7" style="display: none">
<p><strong>Solution.</strong> As before, there are four event times (non-censored observations). For each time <span class="math inline">\(t_j\)</span>, we can calculate the number of events <span class="math inline">\(s_j\)</span> and the risk set <span class="math inline">\(R_j\)</span> as the following:</p>
<p><span class="math display">\[\begin{array}{cccc}
\hline
j &amp; t_j &amp; s_j &amp; R_j \\
\hline
1 &amp; 4 &amp; 2 &amp; 10 \\
2 &amp; 8 &amp; 1 &amp; 5 \\
3 &amp; 12 &amp; 1 &amp; 2 \\
4 &amp; 15 &amp; 1 &amp; 1 \\
\hline
\end{array}\]</span></p>
<p>The Nelson-Äalen estimate of <span class="math inline">\(S(11)\)</span> is <span class="math inline">\(\hat{S}_{NA}(11)=e^{-\hat{H}(11)} = e^{-0.4} = 0.67\)</span>, since <span class="math display">\[\begin{aligned}
\hat{H}(11) &amp;= \sum_{j:t_j\leq 11} \frac{s_j}{R_j}  = \sum_{j=1}^{2} \frac{s_j}{R_j}  \\
&amp;= \frac{2}{10} + \frac{1}{5}  = 0.2 + 0.2 = 0.4 .\\
\end{aligned}\]</span></p>
<p>From earlier work, the Kaplan-Meier estimate of <span class="math inline">\(S(11)\)</span> is <span class="math inline">\(\hat{S}(11) = 0.64\)</span>. Then Greenwood’s estimate of the variance of the product-limit estimate of <span class="math inline">\(S(11)\)</span> is <span class="math display">\[\begin{aligned}
\widehat{Var}(\hat{S}(11)) &amp;= (\hat{S}(11))^2 \sum_{j:t_j\leq 11} \frac{s_j}{R_j(R_j-s_j)} 
&amp;= (0.64)^2 \left(\frac{2}{10(8)} + \frac{1}{5(4)} \right)  = 0.0307. \\
\end{aligned}\]</span></p>
</div>
<hr />
</div>
</div>
</div>
<div id="S:MS:BayesInference" class="section level2">
<h2><span class="header-section-number">4.4</span> Bayesian Inference</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Describe the Bayesian model as an alternative to the frequentist approach and summarize the five components of this modeling approach.</li>
<li>Summarize posterior distributions of parameters and use these posterior distributions to predict new outcomes.</li>
<li>Use conjugate distributions to determine posterior distributions of parameters.</li>
</ul>
<hr />
<div id="S:IntroBayes" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Introduction to Bayesian Inference</h3>
<p>Up to this point, our inferential methods have focused on the <strong>frequentist</strong> setting, in which samples are repeatedly drawn from a population. The vector of parameters <span class="math inline">\(\boldsymbol \theta\)</span> is fixed yet unknown, whereas the outcomes <span class="math inline">\(X\)</span> are realizations of random variables.</p>
<p>In contrast, under the <strong>Bayesian</strong> framework, we view both the model parameters and the data as random variables. We are uncertain about the parameters <span class="math inline">\(\boldsymbol \theta\)</span> and use probability tools to reflect this uncertainty.</p>
<p>To get a sense of the Bayesian framework, begin by recalling Bayes’ rule</p>
<p><span class="math display">\[
\Pr(parameters|data) = \frac{\Pr(data|parameters) \times \Pr(parameters)}{\Pr(data)}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\Pr(parameters)\)</span> is the distribution of the parameters, known as the <em>prior</em> distribution.</li>
<li><span class="math inline">\(\Pr(data | parameters)\)</span> is the sampling distribution. In a frequentist context, it is used for making inferences about the parameters and is known as the <em>likelihood</em>.</li>
<li><span class="math inline">\(\Pr(parameters | data)\)</span> is the distribution of the parameters having observed the data, known as the <em>posterior</em> distribution.</li>
<li><span class="math inline">\(\Pr(data)\)</span> is the marginal distribution of the data. It is generally obtained by integrating (or summing) the joint distribution of data and parameters over parameter values.</li>
</ul>
<p><strong>Why Bayes?</strong> There are several advantages of the Bayesian approach. First, we can describe the entire distribution of parameters conditional on the data. This allows us, for example, to provide probability statements regarding the likelihood of parameters. Second, the Bayesian approach provides a unified approach for estimating parameters. Some non-Bayesian methods, such as least squares, require a separate approach to estimate variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. This is convenient for explaining results to consumers of the data analysis. Third, this approach allows analysts to blend prior information known from other sources with the data in a coherent manner. This topic is developed in detail in the credibility chapter. Fourth, Bayesian analysis is particularly useful for forecasting future responses.</p>
<p><strong>Poisson - Gamma Special Case.</strong> To develop intuition, we consider the Poisson-Gamma case that holds a prominent position in actuarial applications. The idea is to consider a set of random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> where each <span class="math inline">\(X_i\)</span> could represent the number of claims for the <em>i</em>th policyholder. Assume that <span class="math inline">\(X_i\)</span> has a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>, analogous to the likelihood that we first saw in Chapter <a href="C-Frequency-Modeling.html#C:Frequency-Modeling">2</a>. In a non-Bayesian (or frequentist) context, the parameter <span class="math inline">\(\lambda\)</span> is viewed as an unknown quantity that is not random (it is said to be “fixed”). In the Bayesian context, the unknown parameter <span class="math inline">\(\lambda\)</span> is viewed as uncertain and is modeled as a random variable. In this special case, we use the gamma distribution to reflect this uncertainty, the prior distribution.</p>
<p>Think of the following two-stage sampling scheme to motivate our probabilistic set-up.</p>
<ol style="list-style-type: decimal">
<li>In the first stage, the parameter <span class="math inline">\(\lambda\)</span> is drawn from a gamma distribution.</li>
<li>In the second stage, for that value of <span class="math inline">\(\lambda\)</span>, there are <span class="math inline">\(n\)</span> draws from the same (identical) Poisson distribution that are independent, conditional on <span class="math inline">\(\lambda\)</span>.</li>
</ol>
<p>From this simple set-up, some important conclusions emerge.</p>
<ul>
<li>The distribution of <span class="math inline">\(X_i\)</span> is no longer Poisson. For a special case, it turns out to be a negative binomial distribution (see the following “Snippet of Theory”).</li>
<li>The random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> are not independent. This is because they share the common random variable <span class="math inline">\(\lambda\)</span>.</li>
<li>As in the frequentist context, the goal is to make statments about likely values of parameters such as <span class="math inline">\(\lambda\)</span> given the observed data <span class="math inline">\(X_1, \ldots, X_n\)</span>. However, because now both the parameter and the data are random variables, we can use the language of conditional probability to make such statements. As we will see in Section <a href="C-ModelSelection.html#S:ConjugateDistributions">4.4.4</a>, it turns out that the distribution of <span class="math inline">\(\lambda\)</span> given the data <span class="math inline">\(X_1, \ldots, X_n\)</span> is also gamma (with updated parameters), a result that simplifies the task of inferring likely values of the parameter <span class="math inline">\(\lambda\)</span>.</li>
</ul>
<h5 style="text-align: center;">
<a id="displayTheory.2" href="javascript:toggleTheory('TheoryBayesPoisson','displayTheory.2');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="TheoryBayesPoisson" style="display: none">
<hr />
<p>Let us demonstrate that the distribution of <span class="math inline">\(X\)</span> is negative binomial. We assume that the distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(\lambda\)</span> is Poisson, so that <span class="math display">\[
\Pr(X = x|\lambda) = \frac{\lambda^x}{\Gamma(x+1)} e^{-\lambda} ,
\]</span> using notation <span class="math inline">\(\Gamma(x+1) = x!\)</span> for integer <span class="math inline">\(x\)</span>. Assume that <span class="math inline">\(\lambda\)</span> is a draw from a gamma distribution with fixed parameters, say, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\theta\)</span>, so this has <a href="#" class="tooltip" style="color:green"><em>pdf</em><span style="font-size:8pt"> probability density function</span></a> <span class="math display">\[
f(\lambda) = \frac{\lambda^{\alpha-1}}{\theta^{\alpha}\Gamma(\alpha)}\exp(-\lambda/\theta).
\]</span> We know that a <em>pdf</em> integrates to one. To make the development easier, define the reciprocal parameter <span class="math inline">\(\theta_r = 1/\theta\)</span> and so we have <span class="math display">\[
\int_0^{\infty} f(\lambda) ~d\lambda =1 ~~~ ==&gt; ~~~ \theta_r^{-\alpha} \Gamma(\alpha) = \int_0^{\infty} \lambda^{\alpha-1} \exp\left(-\lambda\theta_r\right) ~
d\lambda .
\]</span> From Appendix Chapter <a href="C-AppB.html#C:AppB">16</a> on iterated expectations, we have that the <a href="#" class="tooltip" style="color:green"><em>pmf</em><span style="font-size:8pt"> probability mass function</span></a> of <span class="math inline">\(X\)</span> can be computed in an iterated fashion as</p>
<p><span class="math display">\[
\begin{aligned}
\Pr(X = x) 
&amp;=  \mathrm{E} \left\{\Pr(X = x|\lambda)\right\}\\
&amp;=  \int_0^{\infty} \Pr(X = x|\lambda) f(\lambda) ~d\lambda \\
&amp;=  \int_0^{\infty} \frac{\lambda^x}{\Gamma(x+1)} e^{-\lambda} \frac{\lambda^{\alpha-1}}{\theta^{\alpha}\Gamma(\alpha)}\exp(-\lambda/\theta) ~d\lambda\\
&amp;=  \frac{1}{\theta^{\alpha}\Gamma(x+1)\Gamma(\alpha)} \int_0^{\infty} \lambda^{x+\alpha-1} \exp\left(-\lambda(1+\frac{1}{\theta})\right) ~d\lambda \\
&amp;=  \frac{1}{\theta^{\alpha}\Gamma(x+1)\Gamma(\alpha)} \Gamma(x+\alpha)\left(1+\frac{1}{\theta}\right)^{-(x+\alpha)} \\
&amp;=  \frac{\Gamma(x+\alpha)}{\Gamma(x+1)\Gamma(\alpha)}\left(\frac{1}{1+\theta}\right)^{\alpha} \left(\frac{\theta}{1+\theta}\right)^{x} .\\
\end{aligned} 
\]</span> Here, we used the gamma distribution equality with the substitution <span class="math inline">\(\theta_r = 1 + 1/\theta\)</span>. As can be seen from Section <a href="C-Frequency-Modeling.html#S:negative-binomial-distribution">2.2.3.3</a>, this is a negative binomial distribution with parameter <span class="math inline">\(r = \alpha\)</span> and <span class="math inline">\(\beta = \theta\)</span>.</p>
<hr />
</div>
<p>In this chapter, we use small examples that can be done by hand in order to focus on the foundations. For practical implementation, analysts rely heavily on simulation methods using modern computational methods such as Markov Chain Monte Carlo (<em>MCMC</em>) simulation. We will get an exposure to simulation techniques in Chapter <a href="C-Simulation.html#C:Simulation">6</a> but more intensive techniques such as <em>MCMC</em> requires yet more background. See <span class="citation">Hartman (<a href="#ref-hartman2016">2016</a>)</span> for an introduction to computational Bayesian methods from an actuarial perspective.</p>
</div>
<div id="bayesian-model" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Bayesian Model</h3>
<p>With the intuition developed in the preceding Section <a href="C-ModelSelection.html#S:IntroBayes">4.4.1</a>, we now restate the Bayesian model with a bit more precision using mathematical notation. For simplicity, this summary assumes both the outcomes and parameters are continuous random variables. In the examples, we sometimes ask the viewer to apply these same principles to discrete versions. Conceptually both the continuous and discrete cases are the same; mechanically, one replaces a <a href="#" class="tooltip" style="color:green"><em>pdf</em><span style="font-size:8pt"> probability density function</span></a> by a <a href="#" class="tooltip" style="color:green"><em>pmf</em><span style="font-size:8pt"> probability mass function</span></a> and an integral by a sum.</p>
<p>As stated earlier, under the Bayesian perspective, the model parameters and data are both viewed as random. Our uncertainty about the parameters of the underlying data generating process is reflected in the use of probability tools.</p>
<p><strong>Prior Distribution.</strong> Specifically, think about parameters <span class="math inline">\(\boldsymbol \theta\)</span> as a random vector and let <span class="math inline">\(\pi(\boldsymbol \theta)\)</span> denote the distribution of possible outcomes. This is knowledge that we have before outcomes are observed and is called the prior distribution. Typically, the prior distribution is a regular distribution and so integrates or sums to one, depending on whether <span class="math inline">\(\boldsymbol \theta\)</span> is continuous or discrete. However, we may be very uncertain (or have no clue) about the distribution of <span class="math inline">\(\boldsymbol \theta\)</span>; the Bayesian machinery allows the following situation <span class="math display">\[\int \pi(\theta) d\theta = \infty,\]</span> in which case <span class="math inline">\(\pi(\cdot)\)</span> is called an <strong>improper prior</strong>.</p>
<p><strong>Model Distribution.</strong> The distribution of outcomes given an assumed value of <span class="math inline">\(\boldsymbol \theta\)</span> is known as the model distribution and denoted as <span class="math inline">\(f(x | \boldsymbol \theta) = f_{X|\boldsymbol \theta} (x|\boldsymbol \theta )\)</span>. This is the usual frequentist mass or density function. This is simply the likelihood in the frequentist context and so it is also convenient to use this as a descriptor for the model distribution.</p>
<p><strong>Joint Distribution.</strong> The distribution of outcomes and model parameters is, unsurprisingly, known as the joint distribution and denoted as <span class="math inline">\(f(x , \boldsymbol \theta) = f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)\)</span>.</p>
<p><strong>Marginal Outcome Distribution.</strong> The distribution of outcomes can be expressed as <span class="math display">\[f(x) = \int f(x | \boldsymbol \theta)\pi(\boldsymbol \theta) ~d \boldsymbol \theta.\]</span> This is analogous to a frequentist mixture distribution. In the mixture distribution, we combined (or “mixed”) different subpopulations. In the Bayesian context, the marginal distribution is a combination of different realizations of parameters (in some literatures, you can think about this as combining different “states of nature”).</p>
<p><strong>Posterior Distribution of Parameters.</strong> After outcomes have been observed (hence the terminology “posterior”), one can use Bayes theorem to write the distribution as <span class="math display">\[\pi(\boldsymbol \theta | x) =\frac{f(x , \boldsymbol \theta)}{f(x)} =\frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)}\]</span> The idea is to update your knowledge of the distribution of <span class="math inline">\(\boldsymbol \theta\)</span> (<span class="math inline">\(\pi(\boldsymbol \theta)\)</span>) with the data <span class="math inline">\(x\)</span>. Making statements about potential values of parameters is an important aspect of statistical inference.</p>
</div>
<div id="bayesian-inference" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Bayesian Inference</h3>
<div id="summarizing-the-posterior-distribution-of-parameters" class="section level4">
<h4><span class="header-section-number">4.4.3.1</span> Summarizing the Posterior Distribution of Parameters</h4>
<p>One way to summarize a distribution is to use a confidence interval type statement. To summarize the <em>posterior</em> distribution of parameters, the interval <span class="math inline">\([a,b]\)</span> is said to be a <span class="math inline">\(100(1-\alpha)\%\)</span> <em>credibility</em> interval for <span class="math inline">\(\boldsymbol \theta\)</span> if <span class="math display">\[\Pr (a \le \theta \le b | \mathbf{x}) \ge 1- \alpha.\]</span></p>
<p>For another approach to summarization, we can look to classical decision analysis. In this set-up, the loss function <span class="math inline">\(l(\hat{\theta}, \theta)\)</span> determines the penalty paid for using the estimate <span class="math inline">\(\hat{\theta}\)</span> instead of the true <span class="math inline">\(\theta\)</span>. The <strong>Bayes estimate</strong> is the value that minimizes the expected loss <span class="math inline">\(\mathrm{E~}[ l(\hat{\theta}, \theta)]\)</span>. Some important special cases include:</p>
<p><span class="math display">\[
{\small
\begin{array}{cll}
\hline
\text{Loss function } l(\hat{\theta}, \theta) &amp; \text{Descriptor} &amp; \text{Bayes Estimate} \\
\hline 
(\hat{\theta}- \theta)^2 &amp; \text{squared error loss} &amp; \mathrm{E}(\theta|X) \\
|\hat{\theta}- \theta| &amp; \text{absolute deviation loss} &amp; \text{median of } \pi(\theta|x) \\
I(\hat{\theta} =\theta) &amp; \text{zero-one loss (for discrete probabilities)} &amp; \text{mode of } \pi(\theta|x) \\
\hline
\end{array}
}
\]</span></p>
<p>Minimizing expected loss is a rigorous method for providing a single “best guess” about a likely value of a parameter, comparable to a frequentist estimator of the unknown (fixed) parameter.</p>
<hr />
<p><strong>Example 4.4.1. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>In a portfolio of risks, each policyholder can have at most one claim per year.</li>
<li>The probability of a claim for a policyholder during a year is <span class="math inline">\(q\)</span>.</li>
<li>The prior density is <span class="math display">\[\pi(q) = q^3/0.07, \ \ \ 0.6 &lt; q &lt; 0.8\]</span></li>
</ol>
<p>A randomly selected policyholder has one claim in Year 1 and zero claims in Year 2. For this policyholder, calculate the posterior probability that <span class="math inline">\(0.7 &lt; q &lt; 0.8\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.1" href="javascript:toggleEX('toggleExampleSelect.4.1','displayTextExampleSelect.4.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.1" style="display: none">
<p><strong>Solution.</strong> The posterior density is proportional to the product of the likelihood function and prior density. Thus, <span class="math display">\[\pi(q|1,0) \propto f(1|q)\ f(0|q)\ \pi(q) \propto q(1-q)q^3 = q^4-q^5\]</span></p>
<p>To get the exact posterior density, we integrate the above function over its range <span class="math inline">\((0.6, 0.8)\)</span></p>
<p><span class="math display">\[\int_{0.6}^{0.8} q^4-q^5 dq = \frac{q^5}{5} - \left. \frac{q^6}{6} \right|_{0.6}^{0.8} = 0.014069 \ \Rightarrow \ \pi(q|1,0)=\frac{q^4-q^5}{0.014069}\]</span></p>
<p>Then <span class="math display">\[\Pr(0.7&lt;q&lt;0.8|1,0)= \int_{0.7}^{0.8} \frac{q^4-q^5}{0.014069}dq = 0.5572\]</span></p>
</div>
<hr />
<p><strong>Example 4.4.2. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>The prior distribution of the parameter <span class="math inline">\(\Theta\)</span> has probability density function: <span class="math display">\[\pi(\theta) = \frac{1}{\theta^2}, \ \ 1 &lt; \theta &lt; \infty\]</span></li>
<li>Given <span class="math inline">\(\Theta = \theta\)</span>, claim sizes follow a Pareto distribution with parameters <span class="math inline">\(\alpha=2\)</span> and <span class="math inline">\(\theta\)</span>.</li>
</ol>
<p>A claim of 3 is observed. Calculate the posterior probability that <span class="math inline">\(\Theta\)</span> exceeds 2.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.2" href="javascript:toggleEX('toggleExampleSelect.4.2','displayTextExampleSelect.4.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.2" style="display: none">
<p><em>Solution:</em> The posterior density, given an observation of 3 is</p>
<p><span class="math display">\[\pi(\theta|3) =  \frac{f(3|\theta)\pi(\theta)}{\int_1^\infty f(3|\theta)\pi(\theta)d\theta} = 
\frac{\frac{2\theta^2}{(3+\theta)^3}\frac{1}{\theta^2}}{\int_1^\infty 2(3+\theta)^{-3} d\theta} = 
\frac{2(3+\theta)^{-3}}{\left. -(3+\theta)^{-2}\right|_1^\infty} = 32(3+\theta)^{-3}, \ \ \theta &gt; 1\]</span></p>
<p>Then</p>
<p><span class="math display">\[\Pr(\Theta&gt;2|3) = \int_2^\infty 32(3+\theta)^{-3}d\theta = \left. -16(3+\theta)^{-2} \right|_2^\infty = \frac{16}{25} = 0.64\]</span></p>
</div>
<hr />
</div>
<div id="bayesian-predictive-distribution" class="section level4">
<h4><span class="header-section-number">4.4.3.2</span> Bayesian Predictive Distribution</h4>
<p>For another type of statistical inference, it is often of interest to “predict” the value of a random outcome that is yet to be observed. Specifically, for new data <span class="math inline">\(y\)</span>, the <strong>predictive distribution</strong> is <span class="math display">\[f(y|x) = \int f(y|\theta) \pi(\theta|x) d\theta .\]</span> It is also sometimes called a “posterior” distribution as the distribution of the new data is conditional on a base set of data.</p>
<p>Using squared error loss for the loss function, the <strong>Bayesian prediction</strong> of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{E}(Y|X) &amp;=  \int ~y f(y|X) dy = \int y \left(\int f(y|\theta) \pi(\theta|X) d\theta \right) dy \\
&amp;=  \int  \mathrm{E}(Y|\theta) \pi(\theta|X) ~d\theta .
\end{aligned}
\]</span> As noted earlier, for some situations the distribution of parameters is discrete, not continuous. Having a discrete set of possible parameters allow us to think of them as alternative “states of nature,” a helpful interpretation.</p>
<hr />
<p><strong>Example 4.4.3. Actuarial Exam Question.</strong> For a particular policy, the conditional probability of the annual number of claims given <span class="math inline">\(\Theta = \theta\)</span>, and the probability distribution of <span class="math inline">\(\Theta\)</span> are as follows:</p>
<p><span class="math display">\[
{\small
\begin{array}{l|ccc}
\hline
\text{Number of Claims} &amp; 0 &amp; 1 &amp; 2 \\
\text{Probability} &amp; 2\theta &amp; \theta &amp; 1-3\theta \\
\hline
\end{array}
}
\]</span></p>
<p><span class="math display">\[
{\small
\begin{array}{l|cc}
\hline
\theta &amp; 0.05 &amp; 0.30 \\
\text{Probability} &amp; 0.80 &amp; 0.20 \\
\hline
\end{array}
}
\]</span></p>
<p>Two claims are observed in Year 1. Calculate the Bayesian prediction of the number of claims in Year 2.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.3" href="javascript:toggleEX('toggleExampleSelect.4.3','displayTextExampleSelect.4.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.3" style="display: none">
<p><strong>Solution.</strong> Start with the posterior distribution of the parameter <span class="math display">\[
\Pr(\theta|X) = \frac{\Pr(X|\theta)\Pr(\theta)}{\sum_{\theta}\Pr(X|\theta)\Pr(\theta)}
\]</span> so <span class="math display">\[
\begin{aligned} 
\Pr(\theta=0.05|X=2) &amp;= \frac{\Pr(X=2|\theta=0.05)\Pr(\theta=0.05)}
{\Pr(X=2|\theta=0.05)\Pr(\theta=0.05)+\Pr(X=2|\theta=0.3)\Pr(\theta=0.3)}\\
&amp;=\frac{(1-3\times 0.05)(0.8)}{(1-3\times 0.05)(0.8)+(1-3\times 0.3)(0.2)}= \frac{68}{70}.
\end{aligned} 
\]</span></p>
<p>Thus, <span class="math inline">\(\Pr(\theta=0.3|X=1)= 1 - \Pr(\theta=0.05|X=1) = \frac{2}{70}\)</span>.</p>
<p>From the model distribution, we have <span class="math display">\[
E(X|\theta) = 0 \times 2\theta + 1 \times \theta + 2 \times (1-3\theta) = 2 - 5 \theta.
\]</span> Thus,</p>
<p><span class="math display">\[
\begin{aligned}
E(Y|X)
&amp;=   \sum_{\theta}  \mathrm{E}(Y|\theta) \pi(\theta|X) \\
&amp;= \mathrm{E}(Y|\theta=0.05) \pi(\theta=0.05|X)+\mathrm{E}(Y|\theta=0.3) \pi(\theta=0.3|X)\\
&amp;= ( 2 - 5 (0.05))\frac{68}{70} + ( 2 - 5 (0.3))\frac{2}{70} = 1.714.
\end{aligned}
\]</span></p>
</div>
<hr />
<p><strong>Example 4.4.4. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>Losses on a company’s insurance policies follow a Pareto distribution with probability density function: <span class="math display">\[
f(x|\theta) = \frac{\theta}{(x+\theta)^2}, \ \ 0 &lt; x &lt; \infty
\]</span></li>
<li>For half of the company’s policies <span class="math inline">\(\theta=1\)</span> , while for the other half <span class="math inline">\(\theta=3\)</span>.</li>
</ol>
<p>For a randomly selected policy, losses in Year 1 were 5. Calculate the posterior probability that losses for this policy in Year 2 will exceed 8.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.4" href="javascript:toggleEX('toggleExampleSelect.4.4','displayTextExampleSelect.4.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.4" style="display: none">
<p><strong>Solution.</strong> We are given the prior distribution of <span class="math inline">\(\theta\)</span> as <span class="math inline">\(\Pr(\theta=1)=\Pr(\theta=3)=\frac{1}{2}\)</span>, the conditional distribution <span class="math inline">\(f(x|\theta)\)</span>, and the fact that we observed <span class="math inline">\(X_1=5\)</span>. The goal is to find the predictive probability <span class="math inline">\(\Pr(X_2&gt;8|X_1=5)\)</span>.</p>
<p>The posterior probabilities are</p>
<p><span class="math display">\[
\begin{aligned}
\Pr(\theta=1|X_1=5) &amp;= \frac{f(5|\theta=1)\Pr(\theta=1)}{f(5|\theta=1)\Pr(\theta=1) + f(5|\theta=3)\Pr(\theta=3)} \\
&amp;= \frac{\frac{1}{36}(\frac{1}{2})}{\frac{1}{36}(\frac{1}{2})+\frac{3}{64}(\frac{1}{2})} = \frac{\frac{1}{72}}{\frac{1}{72} + \frac{3}{128}} = \frac{16}{43}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\Pr(\theta=3|X_1=5) &amp;= \frac{f(5|\theta=3)\Pr(\theta=3)}{f(5|\theta=1)\Pr(\theta=1) + f(5|\theta=3)\Pr(\theta=3)} \\
&amp;= 1-\Pr(\theta=1|X_1=5) = \frac{27}{43}
\end{aligned}
\]</span></p>
<p>Note that the conditional probability that losses exceed 8 is</p>
<p><span class="math display">\[
\begin{aligned}
\Pr(X_2&gt;8|\theta) &amp;= \int_8^\infty f(x|\theta)dx \\
&amp;= \int_8^\infty \frac{\theta}{(x+\theta)^2}dx = \left. -\frac{\theta}{x+\theta} \right|_8^\infty = \frac{\theta}{8 + \theta}
\end{aligned}
\]</span></p>
<p>The predictive probability is therefore</p>
<p><span class="math display">\[
\begin{aligned}
\Pr(X_2&gt;8|X_1=5) &amp;= \Pr(X_2&gt;8|\theta=1) \Pr(\theta=1|X_1=5) + \Pr(X_2&gt;8|\theta=3) \Pr(\theta=3 | X_1=5) \\
&amp;= \frac{1}{8+1}\left( \frac{16}{43}\right) + \frac{3}{8+3} \left( \frac{27}{43}\right) = 0.2126
\end{aligned}
\]</span></p>
</div>
<hr />
<p><strong>Example 4.4.5. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>The probability that an insured will have at least one loss during any year is <span class="math inline">\(p\)</span>.</li>
<li>The prior distribution for <span class="math inline">\(p\)</span> is uniform on <span class="math inline">\([0, 0.5]\)</span>.</li>
<li>An insured is observed for 8 years and has at least one loss every year.</li>
</ol>
<p>Calculate the posterior probability that the insured will have at least one loss during Year 9.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.5" href="javascript:toggleEX('toggleExampleSelect.4.5','displayTextExampleSelect.4.5');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.5" style="display: none">
<p><strong>Solution.</strong> The posterior probability density is <span class="math display">\[\begin{aligned}
\pi(p|1,1,1,1,1,1,1,1) &amp;\propto \Pr(1,1,1,1,1,1,1,1|p)\ \pi(p) = p^8(2) \propto p^8 \\ 
\Rightarrow \pi(p|1,1,1,1,1,1,1,1) &amp;= \frac{p^8}{\int_0^5 p^8 dp} = \frac{p^8}{(0.5^9)/9} = 9(0.5^{-9})p^8
\end{aligned}\]</span></p>
<p>Thus, the posterior probability that the insured will have at least one loss during Year 9 is</p>
<p><span class="math display">\[\begin{aligned}
\Pr(X_9=1|1,1,1,1,1,1,1,1) &amp;= \int_0^5 \Pr(X_9=1|p) \pi(p|1,1,1,1,1,1,1,1) dp \\
&amp;= \int_0^5 p(9)(0.5^{-9})p^8 dp = 9(0.5^{-9})(0.5^{10})/10 = 0.45
\end{aligned}\]</span></p>
</div>
<hr />
<p><strong>Example 4.4.6. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>Each risk has at most one claim each year. <span class="math display">\[
{\small
\begin{array}{ccc}
\hline
\text{Type of Risk} &amp; \text{Prior Probability} &amp; \text{Annual Claim Probability} \\
\hline
\text{I} &amp; 0.7 &amp; 0.1 \\
\text{II} &amp; 0.2 &amp; 0.2 \\
\text{III} &amp; 0.1 &amp; 0.4 \\ 
\hline
\end{array}
}
\]</span></li>
</ol>
<p>One randomly chosen risk has three claims during Years 1-6. Calculate the posterior probability of a claim for this risk in Year 7.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.6" href="javascript:toggleEX('toggleExampleSelect.4.6','displayTextExampleSelect.4.6');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.6" style="display: none">
<p><strong>Solution.</strong> The probabilities are from a binomial distribution with 6 trials in which 3 successes were observed.</p>
<p><span class="math display">\[\begin{aligned} 
\Pr(3|\text{I}) &amp;= {6 \choose 3} (0.1^3)(0.9^3) = 0.01458 \\
\Pr(3|\text{II}) &amp;= {6 \choose 3} (0.2^3)(0.8^3) = 0.08192 \\
\Pr(3|\text{III}) &amp;= {6 \choose 3} (0.4^3)(0.6^3) = 0.27648
\end{aligned}\]</span></p>
<p>The probability of observing three successes is <span class="math display">\[\begin{aligned} \Pr(3) &amp;= \Pr(3|\text{I})\Pr(\text{I}) + \Pr(3|\text{II})\Pr(\text{II}) + \Pr(3|\text{III})\Pr(\text{III}) \\
&amp;=  0.7(0.01458) + 0.2(0.08192) + 0.1(0.27648) = 0.054238
\end{aligned}\]</span></p>
<p>The three posterior probabilities are <span class="math display">\[\begin{aligned}
\Pr(\text{I}|3) &amp;= \frac{\Pr(3|\text{I})\Pr(\text{I})}{\Pr(3)} = \frac{0.7(0.01458)}{0.054238} = 0.18817 \\
\Pr(\text{II}|3) &amp;= \frac{\Pr(3|\text{II})\Pr(\text{II})}{\Pr(3)} = \frac{0.2(0.08192)}{0.054238} = 0.30208 \\
\Pr(\text{III}|3) &amp;= \frac{\Pr(3|\text{III})\Pr(\text{III})}{\Pr(3)} = \frac{0.1(0.27648)}{0.054238} = 0.50975 
\end{aligned}\]</span></p>
<p>The posterior probability of a claim is then <span class="math display">\[\begin{aligned} 
\Pr(\text{claim} | 3) &amp;= \Pr(\text{claim}|\text{I})\Pr(\text{I} | 3) + \Pr(\text{claim} | \text{II})\Pr(\text{II} | 3) + \Pr(\text{claim} | \text{III}) \Pr(\text{III} | 3) \\ 
&amp;= 0.1(0.18817) + 0.2(0.30208) + 0.4(0.50975) = 0.28313
\end{aligned}\]</span></p>
</div>
<hr />
</div>
</div>
<div id="S:ConjugateDistributions" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Conjugate Distributions</h3>
<p>In the Bayesian framework, the key to statistical inference is understanding the posterior distribution of the parameters. As described in Section <a href="C-ModelSelection.html#S:IntroBayes">4.4.1</a>, modern data analysis using Bayesian methods utilize computationally intensive techniques such as <a href="#" class="tooltip" style="color:green"><em>MCMC</em><span style="font-size:8pt"> Markov Chain Monte Carlo</span></a> simulation. Another approach for computing posterior distributions are based on <strong>conjugate distributions.</strong> Although this approach is available only for a limited number of distributions, it has the appeal that it provides closed-form expressions for the distributions, allowing for easy interpretations of results.</p>
<p>To relate the prior and posterior distributions of the parameters, we have the relationship</p>
<p><span class="math display">\[\begin{array}{ccc}
\pi(\boldsymbol \theta | x) &amp; = &amp; \frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)}  \\
 &amp; \propto  &amp; f(x|\boldsymbol \theta ) \pi(\boldsymbol \theta) \\
\text{Posterior} &amp; \text{is proportional to} &amp; \text{likelihood} \times \text{prior} .
\end{array}\]</span></p>
<p>For conjugate distributions, the posterior and the prior come from the same family of distributions. The following illustration looks at the Poisson-gamma special case, the most well-known in actuarial applications.</p>
<p><strong>Special Case – Poisson-Gamma - Continued.</strong> Assume a Poisson(<span class="math inline">\(\lambda\)</span>) model distribution and that <span class="math inline">\(\lambda\)</span> follows a gamma(<span class="math inline">\(\alpha, \theta\)</span>) prior distribution. Then, the posterior distribution of <span class="math inline">\(\lambda\)</span> given the data follows a gamma distribution with new parameters <span class="math inline">\(\alpha_{post} = \sum_i x_i + \alpha\)</span> and <span class="math inline">\(\theta_{post} = 1/(n + 1/\theta)\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleConj.4f" href="javascript:toggleEX('toggleExampleConj','displayTextExampleConj.4f');"><i><strong>Show Special Case Details</strong></i></a>
</h5>
<div id="toggleExampleConj" style="display: none">
<p><strong>Special Case – Poisson-Gamma - Continued.</strong> The model distribution is <span class="math display">\[f(\mathbf{x} | \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} .\]</span> The prior distribution is <span class="math display">\[\pi(\lambda) = \frac{\left(\lambda/\theta\right)^{\alpha} \exp(-\lambda/\theta)}{\lambda \Gamma(\alpha)}.\]</span> Thus, the posterior distribution is proportional to <span class="math display">\[\begin{aligned}
\pi(\lambda | \mathbf{x}) &amp;\propto f(\mathbf{x}|\theta ) \pi(\lambda) \\
&amp;= C \lambda^{\sum_i x_i + \alpha -1} \exp(-\lambda(n+1/\theta))
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a constant. We recognize this to be a gamma distribution with new parameters <span class="math inline">\(\alpha_{post} = \sum_i x_i + \alpha\)</span> and <span class="math inline">\(\theta_{post} = 1/(n + 1/\theta)\)</span>. Thus, the gamma distribution is a conjugate prior for the Poisson model distribution.</p>
</div>
<hr />
<p><strong>Example 4.4.7. Actuarial Exam Question.</strong> You are given:</p>
<ol style="list-style-type: lower-roman">
<li>The conditional distribution of the number of claims per policyholder is Poisson with mean <span class="math inline">\(\lambda\)</span>.</li>
<li>The variable <span class="math inline">\(\lambda\)</span> has a gamma distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\theta\)</span>.</li>
<li>For policyholders with 1 claim in Year 1, the Bayes prediction for the number of claims in Year 2 is 0.15.</li>
<li>For policyholders with an average of 2 claims per year in Year 1 and Year 2, the Bayes prediction for the number of claims in Year 3 is 0.20.</li>
</ol>
<p>Calculate <span class="math inline">\(\theta\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTextExampleSelect.4.7" href="javascript:toggleEX('toggleExampleSelect.4.7','displayTextExampleSelect.4.7');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleSelect.4.7" style="display: none">
<p><strong>Solution.</strong> Since the conditional distribution of the number of claims per policyholder, <span class="math inline">\(E(X|\lambda)=Var(X|\lambda)=\lambda\)</span>, the Bayes prediction is <span class="math display">\[\begin{aligned}
\mathrm{E}(X_2|X_1)
&amp;= \int \mathrm{E}(X_2|\lambda) \pi(\lambda|X_1) d\lambda = \alpha_{new} \theta_{new}
\end{aligned}\]</span> because the posterior distribution is gamma with parameters <span class="math inline">\(\alpha_{new}\)</span> and <span class="math inline">\(\theta_{new}\)</span>.</p>
<p>For year 1, we have <span class="math display">\[
0.15 = (X_1 + \alpha) \times \frac{1}{n+1/\theta} = (1 + \alpha) \times \frac{1}{1+1/\theta},
\]</span> so <span class="math inline">\(0.15(1+1/\theta)= 1 + \alpha.\)</span> For year 2, we have <span class="math display">\[
0.2 = (X_1+X_2 + \alpha) \times \frac{1}{n+1/\theta} = (4 + \alpha) \times \frac{1}{2+1/\theta},
\]</span> so <span class="math inline">\(0.2(2+1/\theta)= 4 + \alpha.\)</span> Equating these yields <span class="math display">\[
0.2(2+1/\theta)=3 + 0.15(1+1/\theta)
\]</span> resulting in <span class="math inline">\(\theta = 1/55 = 0.018182\)</span>.</p>
</div>
<hr />
<p>Closed-form expressions means that results can be readily interpreted and easily computed; hence, conjugate distributions are useful in actuarial practice. Two other special cases used extensively are:</p>
<ul>
<li>The uncertainty of parameters is summarized using a beta distribution and the outcomes have a (conditional on the parameter) binomial distribution.</li>
<li>The uncertainty of parameters is summarized using a normal distribution and the outcomes are conditionally normally distributed.</li>
</ul>
<p>Additional results on conjugate distributions are summarized in the Appendix Chapter <a href="C-AppB.html#C:AppB">16</a>.</p>
</div>
</div>
<div id="MS:further-reading-and-resources" class="section level2">
<h2><span class="header-section-number">4.5</span> Further Resources and Contributors</h2>
<div id="exercises-2" class="section level4 unnumbered">
<h4>Exercises</h4>
<p>Here are a set of exercises that guide the viewer through some of the theoretical foundations of <strong>Loss Data Analytics</strong>. Each tutorial is based on one or more questions from the professional actuarial examinations, typically the Society of Actuaries Exam C.</p>
<p style="text-align: center;">
<a href="http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-model-selection/">Model Selection Guided Tutorials</a>
</p>
</div>
<div id="contributors-3" class="section level4 unnumbered">
<h4>Contributors</h4>
<ul>
<li><strong>Edward W. (Jed) Frees</strong> and <strong>Lisa Gao</strong>, University of Wisconsin-Madison, are the principal authors of the initial version of this chapter. Email: <a href="mailto:jfrees@bus.wisc.edu">jfrees@bus.wisc.edu</a> for chapter comments and suggested improvements.</li>
</ul>

</div>
</div>
<div id="technical-supplement-a.-gini-statistic" class="section level2 unnumbered">
<h2>Technical Supplement A. Gini Statistic</h2>
<div id="ts-a.1.-the-classic-lorenz-curve" class="section level3 unnumbered">
<h3>TS A.1. The Classic Lorenz Curve</h3>
<p>In welfare economics, it is common to compare distributions via the <strong>Lorenz curve</strong>, developed by Max Otto Lorenz <span class="citation">(Lorenz <a href="#ref-lorenz1905methods">1905</a>)</span>. A Lorenz curve is a graph of the proportion of a population on the horizontal axis and a distribution function of interest on the vertical axis. It is typically used to represent income distributions. When the income distribution is perfectly aligned with the population distribution, the Lorenz curve results in a 45 degree line that is known as the <em>line of equality</em>. Because the graph compares two distribution functions, one can also think of a Lorenz curve as a type of <span class="math inline">\(pp\)</span> plot that was introduced in Section <a href="C-ModelSelection.html#S:MS:GraphComparison">4.1.2.1</a>. The area between the Lorenz curve and the line of equality is a measure of the discrepancy between the income and population distributions. Two times this area is known as the <strong>Gini index</strong>, introduced by Corrado Gini in 1912.</p>
<p><strong>Example – Classic Lorenz Curve.</strong> For an insurance example, Figure <a href="C-ModelSelection.html#fig:ClassicLorenz">4.12</a> shows a distribution of insurance losses. This figure is based on a random sample of 2000 losses. The left-hand panel shows a right-skewed histogram of losses. The right-hand panel provides the corresponding Lorenz curve, showing again a skewed distribution. For example, the arrow marks the point where 60 percent of the policyholders have 30 percent of losses. The 45 degree line is the line of equality; if each policyholder has the same loss, then the loss distribution would be at this line. The Gini index, twice the area between the Lorenz curve and the 45 degree line, is 37.6 percent for this data set.</p>
<div class="figure" style="text-align: center"><span id="fig:ClassicLorenz"></span>
<img src="LossDataAnalytics_files/figure-html/ClassicLorenz-1.png" alt="Distribution of insurance losses." width="90%" />
<p class="caption">
Figure 4.12: Distribution of insurance losses.
</p>
</div>
</div>
<div id="ts-a.2.-ordered-lorenz-curve-and-the-gini-index" class="section level3 unnumbered">
<h3>TS A.2. Ordered Lorenz Curve and the Gini Index</h3>
<p>We now introduce a modification of the classic Lorenz curve and Gini statistic that is useful in insurance applications. Specifically, we introduce an <em>ordered</em> Lorenz curve which is a graph of the distribution of losses versus premiums, where both losses and premiums are ordered by relativities. Intuitively, the relativities point towards aspects of the comparison where there is a mismatch between losses and premiums. To make the ideas concrete, we first provide some notation. We will consider <span class="math inline">\(i=1, \ldots, n\)</span> policies. For the <span class="math inline">\(i\)</span>th policy, let</p>
<ul>
<li><span class="math inline">\(y_i\)</span> denote the insurance loss,</li>
<li><span class="math inline">\(\mathbf{x}_i\)</span> be the set of policyholder characteristics known to the analyst,</li>
<li><span class="math inline">\(P_i=P(\mathbf{x}_i)\)</span> be the associated premium that is a function of <span class="math inline">\(\mathbf{x}_i\)</span>,</li>
<li><span class="math inline">\(S_i = S(\mathbf{x}_i)\)</span> be an insurance score under consideration for rate changes, and</li>
<li><span class="math inline">\(R_i = R(\mathbf{x}_i)=S(\mathbf{x}_i)/P(\mathbf{x}_i)\)</span> is the relativity, or relative premium.</li>
</ul>
<p>Thus, the set of information used to calculate the ordered Lorenz curve for the <span class="math inline">\(i\)</span>th policy is <span class="math inline">\((y_i, P_i, S_i, R_i)\)</span>.</p>
<div id="ordered-lorenz-curve" class="section level4 unnumbered">
<h4>Ordered Lorenz Curve</h4>
We now sort the set of policies based on relativities (from smallest to largest) and compute the premium and loss distributions. Using notation, the premium distribution is
<span class="math display" id="eq:EmpPremDF">\[\begin{equation}
\hat{F}_P(s) =  \frac{ \sum_{i=1}^n
P(\mathbf{x}_i) \mathrm{I}(R_i \leq s) }{\sum_{i=1}^n P(\mathbf{x}_i)} ,\tag{4.7}
\end{equation}\]</span>
and the loss distribution is
<span class="math display" id="eq:EmpLossDF">\[\begin{equation}
\hat{F}_{L}(s) =  \frac{ \sum_{i=1}^n y_i \mathrm{I}(R_i
\leq s) }{\sum_{i=1}^n y_i} ,\tag{4.8}
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathrm{I}(\cdot)\)</span> is the indicator function, returning a 1 if the event is true and zero otherwise. The graph <span class="math inline">\(\left(\hat{F}_P(s),\hat{F}_{L}(s) \right)\)</span> is an <strong>ordered Lorenz curve</strong>.</p>
<p>The classic Lorenz curve shows the proportion of policyholders on the horizontal axis and the loss distribution function on the vertical axis. The ordered Lorenz curve extends the classical Lorenz curve in two ways, (1) through the ordering of risks and prices by relativities and (2) by allowing prices to vary by observation. We summarize the ordered Lorenz curve in the same way as the classic Lorenz curve using a Gini index, defined as twice the area between the curve and a 45 degree line. The analyst seeks ordered Lorenz curves that approach passing through the southeast corner (1,0); these have greater separation between the loss and premium distributions and therefore larger Gini indices.</p>
<p><strong>Example – Loss Distribution.</strong></p>
<p>Suppose we have <span class="math inline">\(n=5\)</span> policyholders with experience as:</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="center"><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>Sum</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Loss</td>
<td align="center"><span class="math inline">\(y_i\)</span></td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>6</td>
<td>25</td>
</tr>
<tr class="even">
<td align="left">Premium</td>
<td align="center"><span class="math inline">\(P(\mathbf{x}_i)\)</span></td>
<td>4</td>
<td>2</td>
<td>6</td>
<td>5</td>
<td>8</td>
<td>25</td>
</tr>
<tr class="odd">
<td align="left">Relativity</td>
<td align="center"><span class="math inline">\(R(\mathbf{x}_i)\)</span></td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td></td>
</tr>
</tbody>
</table>
<p>Determine the Lorenz curve and the ordered Lorenz curve.</p>
<h5 style="text-align: center;">
<a id="displayLorenz.1" href="javascript:toggleEX('toggleLorenz','displayLorenz.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleLorenz" style="display: none">
<hr />
<div class="figure" style="text-align: center"><span id="fig:LorenzVsOrdered"></span>
<img src="LossDataAnalytics_files/figure-html/LorenzVsOrdered-1.png" alt="Lorenz versus Ordered Lorenz Curve" width="90%" />
<p class="caption">
Figure 4.13: Lorenz versus Ordered Lorenz Curve
</p>
</div>
<p>Figure <a href="C-ModelSelection.html#fig:LorenzVsOrdered">4.13</a> compares the Lorenz curve to the ordered version based on this data. The left-hand panel shows the Lorenz curve. The horizontal axis is the cumulative proportion of policyholders (0, 0.2, 0.4, and so forth) and the vertical axis is the cumulative proportion of losses (0, 4/25, 9/25, and so forth). This figure shows little separation between the distributions of losses and policyholders.</p>
<p>The right-hand panel shows the ordered Lorenz curve. Because observations are sorted by relativities, the first point after the origin (reading from left to right) is (8/25, 6/25). The second point is (13/25, 10/25), with the pattern continuing. For the ordered Lorenz curve, the horizontal axis uses premium weights, the vertical axis uses loss weights, and both axes are ordered by relativities. From the figure, we see that there is greater separation between losses and premiums when viewed through this relativity.</p>
<hr />
</div>
</div>
<div id="gini-index" class="section level4 unnumbered">
<h4>Gini Index</h4>
Specifically, the Gini index can be calculated as follows. Suppose that the empirical ordered Lorenz curve is given by <span class="math inline">\(\{ (a_0=0, b_0=0), (a_1, b_1), \ldots,\)</span> <span class="math inline">\((a_n=1, b_n=1) \}\)</span> for a sample of <span class="math inline">\(n\)</span> observations. Here, we use <span class="math inline">\(a_j = \hat{F}_P(R_j)\)</span> and <span class="math inline">\(b_j = \hat{F}_{L}(R_j)\)</span>. Then, the empirical Gini index is
<span class="math display" id="eq:GiniDefn">\[\begin{eqnarray}
\widehat{Gini} &amp;=&amp;  2\sum_{j=0}^{n-1} (a_{j+1} - a_j) \left \{
\frac{a_{j+1}+a_j}{2} - \frac{b_{j+1}+b_j}{2} \right\} \nonumber \\
&amp;=&amp; 1 - \sum_{j=0}^{n-1} (a_{j+1} - a_j) (b_{j+1}+b_j) .\tag{4.9}
\end{eqnarray}\]</span>
<p><strong>Example – Loss Distribution: Continued.</strong> In the figure, the Gini index for the left-hand panel is 5.6%. In contrast, the Gini index for the right-hand panel is 14.9%. <span class="math inline">\(~~\Box\)</span></p>
</div>
</div>
<div id="ts-a.3.-out-of-sample-validation" class="section level3 unnumbered">
<h3>TS A.3. Out-of-Sample Validation</h3>
<p>The Gini statistics based on an ordered Lorenz curve can be used for out-of-sample validation. The procedure follows:</p>
<ol style="list-style-type: decimal">
<li>Use an in-sample data set to estimate several competing models.</li>
<li>Designate an out-of-sample, or validation, data set of the form <span class="math inline">\(\{(y_i, \mathbf{x}_i), i=1,\ldots,n\}\)</span>.</li>
<li>Establish one of the models as the base model. Use this estimated model and explanatory variables from the validation sample to form premiums of the form <span class="math inline">\(P(\mathbf{x}_i)\)</span>.</li>
<li>Pick one of the competing models. Use this estimated model and explanatory variables from the validation sample to form scores of the form <span class="math inline">\(S(\mathbf{x}_i)\)</span>.</li>
<li>From the premiums and scores, develop relativities <span class="math inline">\(R_i =S(\mathbf{x}_i)/P(\mathbf{x}_i)\)</span>.</li>
<li>Use the validation sample outcomes <span class="math inline">\(y_i\)</span> to compute the Gini statistic.</li>
</ol>
<p><strong>Example – Out-of-Sample Validation</strong></p>
<p>Suppose that we have experience from 25 states and that, for each state, we have available 500 observations that can be used to predict future losses. For simplicity, assume that the analyst knows that these losses were generated by a gamma distribution with a common shape parameter equal to 5. Unknown to the analyst, the scale parameters vary by state, from a low of 20 to 66. To compute base premiums, the analyst assumes a scale parameter that is common to all states that is to be estimated from the data. As an alternative, the analyst allows the scale parameters to vary by state and will again use the data to estimate these parameters.</p>
<p>An out of sample validation set of 200 from each state is available. Determine the ordered Lorenz curve and the corresponding Gini statistic to compare the two rate procedures.</p>
<h5 style="text-align: center;">
<a id="displayLorenzeExample" href="javascript:toggleEX('toggleExampleLor','displayLorenzeExample');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExampleLor" style="display: none">
<hr />
<p>Recall for the gamma distribution that the mean equals the shape times the scale or, 5 times the scale parameter, for our example. So, you can check that the maximum likelihood estimates are simple the average experience.</p>
<p>For our base premium, we assume a common distribution among all states. For these simulated data, the average is <strong>P</strong>=219.96. You can think of this common premium as based on a <em>community rating</em> principle.</p>
<p>As an alternative, we use averages that are state-specific; these averages form our scores <strong>S</strong>. Because this illustration uses means that vary by states, we anticipate this alternative rating procedure to be preferred to the community rating procedure.</p>
<p>Out of sample claims were generated from the same gamma distribution, with 200 observations for each state. The following <code>R</code> code shows how to calculate the ordered Lorenz curve.</p>
<h5 style="text-align: center;">
<a id="displayGiniCode.4" href="javascript:togglecode('toggleGiniCode.4','displayGiniCode.4');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleGiniCode.4" style="display: none">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GiniCalc &lt;-<span class="st"> </span><span class="cf">function</span>(Claims,PIx,Sx){
   y   &lt;-<span class="st"> </span>Claims<span class="op">/</span><span class="kw">mean</span>(Claims)
   PIx &lt;-<span class="st"> </span>PIx<span class="op">/</span><span class="kw">mean</span>(PIx)
   Sx  &lt;-<span class="st"> </span>Sx<span class="op">/</span><span class="kw">mean</span>(Sx)
   Rx  &lt;-<span class="st"> </span>Sx<span class="op">/</span>PIx       <span class="co">#Relativity</span>
   n   &lt;-<span class="st"> </span><span class="kw">length</span>(PIx)
   origorder &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">:</span>n)
   PSRmat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(PIx,Sx,Rx,y,origorder))
   PSRmatOrder &lt;-<span class="st"> </span>PSRmat[<span class="kw">order</span>(Rx),]  <span class="co">#  Sort by relativity</span>
<span class="co">#  PREMIUM, LOSS DFs</span>
   DFPrem &lt;-<span class="st"> </span><span class="kw">cumsum</span>(PSRmatOrder<span class="op">$</span>PIx)<span class="op">/</span>n
   DFLoss &lt;-<span class="st"> </span><span class="kw">cumsum</span>(PSRmatOrder<span class="op">$</span>y)<span class="op">/</span>n
<span class="co">#  GINI CALC</span>
   DFPremdiff &lt;-<span class="st"> </span>DFPrem[<span class="dv">2</span><span class="op">:</span>n]<span class="op">-</span>DFPrem[<span class="dv">1</span><span class="op">:</span>(n<span class="op">-</span><span class="dv">1</span>)]
   DFPremavg  &lt;-<span class="st"> </span>(DFPrem[<span class="dv">2</span><span class="op">:</span>n]<span class="op">+</span>DFPrem[<span class="dv">1</span><span class="op">:</span>(n<span class="op">-</span><span class="dv">1</span>)])<span class="op">/</span><span class="dv">2</span>
   DFLossavg  &lt;-<span class="st"> </span>(DFLoss[<span class="dv">2</span><span class="op">:</span>n]<span class="op">+</span>DFLoss[<span class="dv">1</span><span class="op">:</span>(n<span class="op">-</span><span class="dv">1</span>)])<span class="op">/</span><span class="dv">2</span>
   (Gini &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">crossprod</span>(DFPremdiff,DFPremavg<span class="op">-</span>DFLossavg)) 
<span class="co">#  STANDARD ERROR CALC</span>
   h1 &lt;-<span class="st"> </span><span class="fl">0.5</span><span class="op">*</span><span class="st"> </span>(PSRmatOrder<span class="op">$</span>PIx<span class="op">*</span>DFLoss <span class="op">+</span><span class="st"> </span>PSRmatOrder<span class="op">$</span>y<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>DFPrem) ) <span class="co">#  PROJECTION CALC</span>
   h1bar   &lt;-<span class="st"> </span><span class="kw">mean</span>(h1) 
   sigmah  &lt;-<span class="st"> </span><span class="kw">var</span>(h1)
   sigmahy &lt;-<span class="st"> </span><span class="kw">cov</span>(h1,PSRmatOrder<span class="op">$</span>y)
   sigmahpi &lt;-<span class="st"> </span><span class="kw">cov</span>(h1,PSRmatOrder<span class="op">$</span>PIx)
   sigmay  &lt;-<span class="st"> </span><span class="kw">var</span>(y)
   sigmapi &lt;-<span class="st"> </span><span class="kw">var</span>(PIx)
   sigmaypi &lt;-<span class="st"> </span><span class="kw">cov</span>(PSRmatOrder<span class="op">$</span>y,PSRmatOrder<span class="op">$</span>PIx)
   temp1=<span class="st"> </span><span class="dv">4</span><span class="op">*</span>sigmah <span class="op">+</span><span class="st"> </span>h1bar<span class="op">^</span><span class="dv">2</span><span class="op">*</span>sigmay <span class="op">+</span><span class="st"> </span>h1bar<span class="op">^</span><span class="dv">2</span><span class="op">*</span>sigmapi <span class="op">-</span>
<span class="st">           </span><span class="dv">4</span><span class="op">*</span>h1bar<span class="op">*</span>sigmahy <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>h1bar<span class="op">*</span>sigmahpi <span class="op">+</span>
<span class="st">           </span><span class="dv">2</span><span class="op">*</span>h1bar<span class="op">^</span><span class="dv">2</span><span class="op">*</span>sigmaypi
   sigmaGini  &lt;-<span class="st"> </span><span class="dv">4</span><span class="op">*</span>temp1
   stderrGini &lt;-<span class="st"> </span><span class="kw">sqrt</span>(sigmaGini<span class="op">/</span>n) 
   check &lt;-<span class="st"> </span><span class="kw">var</span>(PIx<span class="op">-</span>Sx)
   Gini  &lt;-<span class="st"> </span>Gini<span class="op">*</span>(check <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>)
   stderrGini &lt;-<span class="st"> </span>stderrGini<span class="op">*</span>(check <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>)
   Retmat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(DFPrem,DFLoss)) 
   RetmatGini&lt;-<span class="kw">list</span>(Retmat,Gini,stderrGini)
     <span class="kw">return</span>(RetmatGini)
}
temp=<span class="kw">GiniCalc</span>(<span class="dt">Claims=</span>Outy,<span class="dt">PIx=</span>Flatpred,<span class="dt">Sx=</span>Predvec)
Results=temp[[<span class="dv">1</span>]]
Gini &lt;-<span class="st"> </span>temp[[<span class="dv">2</span>]];<span class="co">#Gini</span>
stderrorGini &lt;-<span class="st"> </span>temp[[<span class="dv">3</span>]];<span class="co">#Standard Error</span></code></pre></div>
</div>
<p><img src="LossDataAnalytics_files/figure-html/unnamed-chunk-6-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>For these data, the Gini index is 0.187 with a standard error equal to 0.00381. This suggests that the state-specific alternative procedure is strongly preferred to the base community rating procedure.</p>
<hr />
</div>
<div id="discussion" class="section level4 unnumbered">
<h4>Discussion</h4>
<p>In insurance claims modeling, standard out-of-sample validation measures are not the most informative due to the high proportions of zeros (corresponding to no claim) and the skewed fat-tailed distribution of the positive values. In contrast, the Gini index works well with many zeros (see the demonstration in <span class="citation">(Frees, Meyers, and Cummings <a href="#ref-frees2014insurance">2014</a>)</span>). Moreover, the Gini index can be motivated by the economics of insurance. Intuitively, the Gini index measures the negative covariance between a policy’s “profit” (<span class="math inline">\(P-y\)</span>, premium minus loss) and the rank of the relativity (<strong>R</strong>, score divided by premium). That is, the close approximation</p>
<p><span class="math display">\[\widehat{Gini} \approx - \frac{2}{n} \widehat{Cov} \left\{ (P-y), rank(R) \right\} .\]</span></p>
<p>This observation leads an insurer to seek a score and resulting relativity that produces to a large Gini index. In this way, the Gini index and associated ordered Lorenz curve are useful for identifying profitable blocks of insurance business.</p>
<p>Unlike classical measures of association, the Gini index assumes that a premium base <strong>P</strong> is currently in place and seeks to assess vulnerabilities of this structure. This approach is more akin to hypothesis testing (when compared to goodness of fit) where one identifies a “null hypothesis” as the current state of the world and uses decision-making criteria/statistics to compare this with an “alternative hypothesis.”</p>
<p>Properties of the insurance version of the Gini statistic were developed by <span class="citation">(Frees, Meyers, and Cummings <a href="#ref-frees2011summarizing">2011</a>)</span> and <span class="citation">(Frees, Meyers, and Cummings <a href="#ref-frees2014insurance">2014</a>)</span>. In these articles you can find formulas for the standard errors and other additional background information.</p>

</div>
</div>
</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references">
<div id="ref-box1980sampling">
<p>Box, George EP. 1980. “Sampling and Bayes’ Inference in Scientific Modelling and Robustness.” <em>Journal of the Royal Statistical Society. Series A (General)</em>. JSTOR, 383–430.</p>
</div>
<div id="ref-snee1977validation">
<p>Snee, Ronald D. 1977. “Validation of Regression Models: Methods and Examples.” <em>Technometrics</em> 19 (4). Taylor &amp; Francis Group: 415–28.</p>
</div>
<div id="ref-picard1990data">
<p>Picard, Richard R., and Kenneth N. Berk. 1990. “Data Splitting.” <em>The American Statistician</em> 44 (2). Taylor &amp; Francis: 140–47.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div id="ref-kaplan1958">
<p>Kaplan, Edward L., and Paul Meier. 1958. “Nonparametric Estimation from Incomplete Observations.” <em>Journal of the American Statistical Association</em> 53 (282). Taylor &amp; Francis: 457–81.</p>
</div>
<div id="ref-greenwood1926">
<p>Greenwood, Major. 1926. “The Errors of Sampling of the Survivorship Tables.” In <em>Reports on Public Health and Statistical Subjects</em>. Vol. 33. London: Her Majesty’s Stationary Office.</p>
</div>
<div id="ref-aalen1978">
<p>Aalen, Odd. 1978. “Nonparametric Inference for a Family of Counting Processes.” <em>The Annals of Statistics</em> 6 (4). Institute of Mathematical Statistics: 701–26.</p>
</div>
<div id="ref-hartman2016">
<p>Hartman, Brian. 2016. “Bayesian Computational Methods.” <em>Predictive Modeling Applications in Actuarial Science: Volume 2, Case Studies in Insurance</em>. Cambridge University Press.</p>
</div>
<div id="ref-lorenz1905methods">
<p>Lorenz, Max O. 1905. “Methods of Measuring the Concentration of Wealth.” <em>Publications of the American Statistical Association</em> 9 (70). Taylor &amp; Francis: 209–19.</p>
</div>
<div id="ref-frees2014insurance">
<p>Frees, Edward W., Glenn Meyers, and A. David Cummings. 2011. “Summarizing Insurance Scores Using a Gini Index.” <em>Journal of the American Statistical Association</em> 106 (495). Taylor &amp; Francis: 1085–98.</p> 2014. “Insurance Ratemaking and a Gini Index.” <em>Journal of Risk and Insurance</em> 81 (2). Wiley Online Library: 335–66.</p>
</div>
<div id="ref-frees2011summarizing">
<p>Frees, Edward W., Glenn Meyers, and A. David Cummings. 2011. “Summarizing Insurance Scores Using a Gini Index.” <em>Journal of the American Statistical Association</em> 106 (495). Taylor &amp; Francis: 1085–98.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C-Severity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C-AggLossModels.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["LossDataAnalytics.pdf", "LossDataAnalytics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
