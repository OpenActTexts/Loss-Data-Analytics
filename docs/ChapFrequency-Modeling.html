<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Frequency Modeling | Loss Data Analytics</title>
  <meta name="description" content="Chapter 2 Frequency Modeling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Frequency Modeling | Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 2 Frequency Modeling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/openacttexts/Loss-Data-Analytics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Frequency Modeling | Loss Data Analytics" />
  
  <meta name="twitter:description" content="Chapter 2 Frequency Modeling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ChapIntro.html"/>
<link rel="next" href="ChapSeverity.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the markdown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
        MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};

// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}


// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};

Survey.StylesManager.applyTheme("modern");

</script>  
<!-- This completes the code for the quizzes -->

<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#other-collaborators"><i class="fa fa-check"></i>Other Collaborators</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version"><i class="fa fa-check"></i>Version</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ChapIntro.html"><a href="ChapIntro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics to Insurance Activities</a><ul>
<li class="chapter" data-level="1.1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.2</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="ChapIntro.html"><a href="ChapIntro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="ChapIntro.html"><a href="ChapIntro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="ChapIntro.html"><a href="ChapIntro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.3.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="ChapIntro.html"><a href="ChapIntro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ChapIntro.html"><a href="ChapIntro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html"><i class="fa fa-check"></i><b>2</b> Frequency Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>2.1</b> Frequency Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Basic Frequency Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>2.2.1</b> Foundations</a></li>
<li class="chapter" data-level="2.2.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>2.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> The (<em>a</em>, <em>b</em>, 0) Class</a></li>
<li class="chapter" data-level="2.4" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimating Frequency Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>2.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Frequency Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="2.7" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="2.8" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>2.9</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="2.9.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>2.9.1</b> TS 2.A. R Code for Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ChapSeverity.html"><a href="ChapSeverity.html"><i class="fa fa-check"></i><b>3</b> Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Basic Distributional Quantities</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>3.1.1</b> Moments</a></li>
<li class="chapter" data-level="3.1.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LS:Quantiles"><i class="fa fa-check"></i><b>3.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#moment-generating-function"><i class="fa fa-check"></i><b>3.1.3</b> Moment Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions for Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Loss:Gamma"><i class="fa fa-check"></i><b>3.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#pareto-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LS:Weibull"><i class="fa fa-check"></i><b>3.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>3.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Methods of Creating New Distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="3.3.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>3.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="3.3.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LossSev:Raising"><i class="fa fa-check"></i><b>3.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="3.3.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#exponentiation"><i class="fa fa-check"></i><b>3.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="3.3.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#finite-mixtures"><i class="fa fa-check"></i><b>3.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="3.3.6" data-path="ChapSeverity.html"><a href="ChapSeverity.html#continuous-mixtures"><i class="fa fa-check"></i><b>3.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>3.4</b> Coverage Modifications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="3.4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="3.4.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#coinsurance-and-inflation"><i class="fa fa-check"></i><b>3.4.3</b> Coinsurance and Inflation</a></li>
<li class="chapter" data-level="3.4.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>3.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="3.5.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Loss:MLEModified"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Likelihood Estimators using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ChapSeverity.html"><a href="ChapSeverity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>3.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html"><i class="fa fa-check"></i><b>4</b> Model Selection and Estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:NonParEst"><i class="fa fa-check"></i><b>4.1.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.1.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>4.1.2</b> Tools for Model Selection and Diagnostics</a></li>
<li class="chapter" data-level="4.1.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#starting-values"><i class="fa fa-check"></i><b>4.1.3</b> Starting Values</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>4.2</b> Model Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#iterative-model-selection"><i class="fa fa-check"></i><b>4.2.1</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="4.2.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#model-selection-based-on-a-training-dataset"><i class="fa fa-check"></i><b>4.2.2</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="4.2.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>4.2.3</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="4.2.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Cross-Validation"><i class="fa fa-check"></i><b>4.2.4</b> Model Selection Based on Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>4.3</b> Estimation using Modified Data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModifiedData1"><i class="fa fa-check"></i><b>4.3.1</b> Parametric Estimation using Modified Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.2</b> Nonparametric Estimation using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:BayesInference"><i class="fa fa-check"></i><b>4.4</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.4.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:IntroBayes"><i class="fa fa-check"></i><b>4.4.1</b> Introduction to Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#bayesian-model"><i class="fa fa-check"></i><b>4.4.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#bayesian-inference"><i class="fa fa-check"></i><b>4.4.3</b> Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>4.4.4</b> Conjugate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html"><i class="fa fa-check"></i><b>5</b> Aggregate Loss Models</a><ul>
<li class="chapter" data-level="5.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>5.2</b> Individual Risk Model</a></li>
<li class="chapter" data-level="5.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:AggLoss:CRM"><i class="fa fa-check"></i><b>5.3</b> Collective Risk Model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>5.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>5.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="5.3.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#analytic-results"><i class="fa fa-check"></i><b>5.3.3</b> Analytic Results</a></li>
<li class="chapter" data-level="5.3.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:AggLoss:Tweedie"><i class="fa fa-check"></i><b>5.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>5.4</b> Computing the Aggregate Claims Distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>5.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="5.4.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#simulation"><i class="fa fa-check"></i><b>5.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>5.5</b> Effects of Coverage Modifications</a><ul>
<li class="chapter" data-level="5.5.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>5.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="5.5.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>5.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="5.5.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>5.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>5.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-5.a.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS 5.A.1. Individual Risk Model Properties</a></li>
<li><a href="ChapAggLossModels.html#ts-5.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it">TS 5.A.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li><a href="ChapAggLossModels.html#ts-5.a.3.-example-5.3.8-moment-generating-function-of-aggregate-loss-s_n">TS 5.A.3. Example 5.3.8 Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ChapSimulation.html"><a href="ChapSimulation.html"><i class="fa fa-check"></i><b>6</b> Simulation and Resampling</a><ul>
<li class="chapter" data-level="6.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:SimulationFundamentals"><i class="fa fa-check"></i><b>6.1</b> Simulation Fundamentals</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="6.1.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:InverseTransform"><i class="fa fa-check"></i><b>6.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="6.1.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#simulation-precision"><i class="fa fa-check"></i><b>6.1.3</b> Simulation Precision</a></li>
<li class="chapter" data-level="6.1.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:SimulationStatInference"><i class="fa fa-check"></i><b>6.1.4</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Bootstrap"><i class="fa fa-check"></i><b>6.2</b> Bootstrapping and Resampling</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#bootstrap-foundations"><i class="fa fa-check"></i><b>6.2.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="6.2.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sim:Precision"><i class="fa fa-check"></i><b>6.2.2</b> Bootstrap Precision: Bias, Standard Deviation, and Mean Square Error</a></li>
<li class="chapter" data-level="6.2.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#confidence-intervals"><i class="fa fa-check"></i><b>6.2.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.2.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:ParametricBootStrap"><i class="fa fa-check"></i><b>6.2.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:CrossValidation"><i class="fa fa-check"></i><b>6.3</b> Cross-Validation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.3.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="6.3.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="6.3.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#cross-validation-and-bootstrap"><i class="fa fa-check"></i><b>6.3.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:ImportanceSampling"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.5" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:MCMC"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo Markov Chain (MCMC)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#metropolis-hastings"><i class="fa fa-check"></i><b>6.5.1</b> Metropolis Hastings</a></li>
<li class="chapter" data-level="6.5.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#gibbs-sampler"><i class="fa fa-check"></i><b>6.5.2</b> Gibbs Sampler</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ChapSimulation.html"><a href="ChapSimulation.html#Simulation:further-reading-and-resources"><i class="fa fa-check"></i><b>6.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="6.6.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#ts-6.a.-bootstrap-applications-in-predictive-modeling"><i class="fa fa-check"></i><b>6.6.1</b> TS 6.A. Bootstrap Applications in Predictive Modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html"><i class="fa fa-check"></i><b>7</b> Premium Foundations</a><ul>
<li class="chapter" data-level="7.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:IntroductionRatemaking"><i class="fa fa-check"></i><b>7.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="7.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:AggRateMaking"><i class="fa fa-check"></i><b>7.2</b> Aggregate Ratemaking Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:PurePremium"><i class="fa fa-check"></i><b>7.2.1</b> Pure Premium Method</a></li>
<li class="chapter" data-level="7.2.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:LossRatio"><i class="fa fa-check"></i><b>7.2.2</b> Loss Ratio Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:PricingPrinciples"><i class="fa fa-check"></i><b>7.3</b> Pricing Principles</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#premium-principles"><i class="fa fa-check"></i><b>7.3.1</b> Premium Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#properties-of-premium-principles"><i class="fa fa-check"></i><b>7.3.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:HeterogeneousRisks"><i class="fa fa-check"></i><b>7.4</b> Heterogeneous Risks</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:ExposureToRisk"><i class="fa fa-check"></i><b>7.4.1</b> Exposure to Risk</a></li>
<li class="chapter" data-level="7.4.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:RatingFactors"><i class="fa fa-check"></i><b>7.4.2</b> Rating Factors</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:TrendDevelopment"><i class="fa fa-check"></i><b>7.5</b> Development and Trending</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#exposures-and-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Exposures and Premiums</a></li>
<li class="chapter" data-level="7.5.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#losses-claims-and-payments"><i class="fa fa-check"></i><b>7.5.2</b> Losses, Claims, and Payments</a></li>
<li class="chapter" data-level="7.5.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:CompareMethods"><i class="fa fa-check"></i><b>7.5.3</b> Comparing Pure Premium and Loss Ratio Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:GiniStatistic"><i class="fa fa-check"></i><b>7.6</b> Selecting a Premium</a><ul>
<li class="chapter" data-level="7.6.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#classic-lorenz-curve"><i class="fa fa-check"></i><b>7.6.1</b> Classic Lorenz Curve</a></li>
<li class="chapter" data-level="7.6.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#performance-curve-and-a-gini-statistic"><i class="fa fa-check"></i><b>7.6.2</b> Performance Curve and a Gini Statistic</a></li>
<li class="chapter" data-level="7.6.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#out-of-sample-validation"><i class="fa fa-check"></i><b>7.6.3</b> Out-of-Sample Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>7.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#ts-7.a.-rate-regulation"><i class="fa fa-check"></i>TS 7.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html"><i class="fa fa-check"></i><b>8</b> Risk Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>8.2</b> Poisson Regression Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>8.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>8.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>8.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="8.2.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#exercises-3"><i class="fa fa-check"></i><b>8.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>8.3</b> Categorical Variables and Multiplicative Tariff</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>8.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="8.3.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>8.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="8.3.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>8.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="8.3.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>8.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>8.4</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-8.a.-estimating-poisson-regression-models"><i class="fa fa-check"></i>TS 8.A. Estimating Poisson Regression Models</a></li>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-8.b.-selecting-rating-factors"><i class="fa fa-check"></i>TS 8.B. Selecting Rating Factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ChapCredibility.html"><a href="ChapCredibility.html"><i class="fa fa-check"></i><b>9</b> Experience Rating Using Credibility Theory</a><ul>
<li class="chapter" data-level="9.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>9.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="9.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.2</b> Limited Fluctuation Credibility</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:frequency"><i class="fa fa-check"></i><b>9.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="9.2.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>9.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="9.2.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>9.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="9.2.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#partial-credibility"><i class="fa fa-check"></i><b>9.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Cred:Buhlmann"><i class="fa fa-check"></i><b>9.3</b> Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.3.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>9.3.1</b> Credibility <em>Z</em>, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#bühlmann-straub-credibility"><i class="fa fa-check"></i><b>9.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="9.5" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Cred:BayesInf"><i class="fa fa-check"></i><b>9.5</b> Bayesian Inference and Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.5.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#Sec:Cred:gammaPoisson"><i class="fa fa-check"></i><b>9.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="9.5.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#beta-binomial-model"><i class="fa fa-check"></i><b>9.5.2</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="9.5.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#exact-credibility"><i class="fa fa-check"></i><b>9.5.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="ChapCredibility.html"><a href="ChapCredibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>9.6</b> Estimating Credibility Parameters</a><ul>
<li class="chapter" data-level="9.6.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="9.6.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#nonparametric-estimation-for-bühlmann-and-bühlmann-straub-models"><i class="fa fa-check"></i><b>9.6.2</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#semiparametric-estimation-for-bühlmann-and-bühlmann-straub-models"><i class="fa fa-check"></i><b>9.6.3</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>9.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="ChapCredibility.html"><a href="ChapCredibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html"><i class="fa fa-check"></i><b>10</b> Insurance Portfolio Management including Reinsurance</a><ul>
<li class="chapter" data-level="10.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#introduction-to-insurance-portfolios"><i class="fa fa-check"></i><b>10.1</b> Introduction to Insurance Portfolios</a></li>
<li class="chapter" data-level="10.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Tails"><i class="fa fa-check"></i><b>10.2</b> Tails of Distributions</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>10.2.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="10.2.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>10.2.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>10.3</b> Risk Measures</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>10.3.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="10.3.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>10.3.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="10.3.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>10.3.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>10.4</b> Reinsurance</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>10.4.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.4.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>10.4.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.4.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>10.4.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#further-resources-and-contributors-1"><i class="fa fa-check"></i><b>10.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html"><i class="fa fa-check"></i><b>11</b> Loss Reserving</a><ul>
<li class="chapter" data-level="11.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:motivation"><i class="fa fa-check"></i><b>11.1</b> Motivation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:claim-types"><i class="fa fa-check"></i><b>11.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="11.1.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#why-reserving"><i class="fa fa-check"></i><b>11.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Data"><i class="fa fa-check"></i><b>11.2</b> Loss Reserve Data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#from-micro-to-macro"><i class="fa fa-check"></i><b>11.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="11.2.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#run-off-triangles"><i class="fa fa-check"></i><b>11.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="11.2.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#loss-reserve-notation"><i class="fa fa-check"></i><b>11.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="11.2.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Rcode"><i class="fa fa-check"></i><b>11.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Chain-ladder"><i class="fa fa-check"></i><b>11.3</b> The Chain-Ladder Method</a><ul>
<li class="chapter" data-level="11.3.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:DeterministicCL"><i class="fa fa-check"></i><b>11.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="11.3.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#macks-distribution-free-chain-ladder-model"><i class="fa fa-check"></i><b>11.3.2</b> Mack’s Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="11.3.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#r-code-for-chain-ladder-predictions"><i class="fa fa-check"></i><b>11.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:GLMs"><i class="fa fa-check"></i><b>11.4</b> GLMs and Bootstrap for Loss Reserves</a><ul>
<li class="chapter" data-level="11.4.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-specification"><i class="fa fa-check"></i><b>11.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="11.4.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>11.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="11.4.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#bootstrap"><i class="fa fa-check"></i><b>11.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#LossRe:further-reading-and-resources"><i class="fa fa-check"></i><b>11.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html"><i class="fa fa-check"></i><b>12</b> Experience Rating using Bonus-Malus</a><ul>
<li class="chapter" data-level="12.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:Intro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:NCD"><i class="fa fa-check"></i><b>12.2</b> <em>NCD</em> System in Several Countries</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#ncd-system-in-malaysia"><i class="fa fa-check"></i><b>12.2.1</b> <em>NCD</em> System in Malaysia</a></li>
<li class="chapter" data-level="12.2.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#ncd-systems-in-other-countries"><i class="fa fa-check"></i><b>12.2.2</b> NCD Systems in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:BMS"><i class="fa fa-check"></i><b>12.3</b> <em>BMS</em> and Markov Chain Model</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>12.3.1</b> Transition Probability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:StatDist"><i class="fa fa-check"></i><b>12.4</b> <em>BMS</em> and Stationary Distribution</a><ul>
<li class="chapter" data-level="12.4.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>12.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-code-for-a-stationary-distribution"><i class="fa fa-check"></i><b>12.4.2</b> <code>R</code> Code for a Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>12.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="12.4.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-program-for-premium-evolution"><i class="fa fa-check"></i><b>12.4.4</b> <code>R</code> Program for Premium Evolution</a></li>
<li class="chapter" data-level="12.4.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>12.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="12.4.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-program-for-convergence-rate"><i class="fa fa-check"></i><b>12.4.6</b> <code>R</code> Program for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:PremRtg"><i class="fa fa-check"></i><b>12.5</b> <em>BMS</em> and Premium Rating</a><ul>
<li class="chapter" data-level="12.5.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>12.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="12.5.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#a-priori-risk-classification"><i class="fa fa-check"></i><b>12.5.2</b> A Priori Risk Classification</a></li>
<li class="chapter" data-level="12.5.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#modelling-of-residual-heterogeneity"><i class="fa fa-check"></i><b>12.5.3</b> Modelling of Residual Heterogeneity</a></li>
<li class="chapter" data-level="12.5.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution-allowing-for-residual-heterogeneity"><i class="fa fa-check"></i><b>12.5.4</b> Stationary Distribution Allowing for Residual Heterogeneity</a></li>
<li class="chapter" data-level="12.5.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#determination-of-optimal-relativities"><i class="fa fa-check"></i><b>12.5.5</b> Determination of Optimal Relativities</a></li>
<li class="chapter" data-level="12.5.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#numerical-illustrations"><i class="fa fa-check"></i><b>12.5.6</b> Numerical Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Further"><i class="fa fa-check"></i><b>12.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="12.6.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>12.6.1</b> Further Reading and References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html"><i class="fa fa-check"></i><b>13</b> Data and Systems</a><ul>
<li class="chapter" data-level="13.1" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data"><i class="fa fa-check"></i><b>13.1</b> Data</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-types-and-sources"><i class="fa fa-check"></i><b>13.1.1</b> Data Types and Sources</a></li>
<li class="chapter" data-level="13.1.2" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-structures-and-storage"><i class="fa fa-check"></i><b>13.1.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="13.1.3" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-quality"><i class="fa fa-check"></i><b>13.1.3</b> Data Quality</a></li>
<li class="chapter" data-level="13.1.4" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-cleaning"><i class="fa fa-check"></i><b>13.1.4</b> Data Cleaning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-analysis-preliminaries"><i class="fa fa-check"></i><b>13.2</b> Data Analysis Preliminaries</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#S:process"><i class="fa fa-check"></i><b>13.2.1</b> Data Analysis Process</a></li>
<li class="chapter" data-level="13.2.2" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#exploratory-versus-confirmatory"><i class="fa fa-check"></i><b>13.2.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="13.2.3" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>13.2.3</b> Supervised versus Unsupervised</a></li>
<li class="chapter" data-level="13.2.4" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#parametric-versus-nonparametric"><i class="fa fa-check"></i><b>13.2.4</b> Parametric versus Nonparametric</a></li>
<li class="chapter" data-level="13.2.5" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#S:expred"><i class="fa fa-check"></i><b>13.2.5</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="13.2.6" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-modeling-versus-algorithmic-modeling"><i class="fa fa-check"></i><b>13.2.6</b> Data Modeling versus Algorithmic Modeling</a></li>
<li class="chapter" data-level="13.2.7" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#big-data-analysis"><i class="fa fa-check"></i><b>13.2.7</b> Big Data Analysis</a></li>
<li class="chapter" data-level="13.2.8" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#reproducible-analysis"><i class="fa fa-check"></i><b>13.2.8</b> Reproducible Analysis</a></li>
<li class="chapter" data-level="13.2.9" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#ethical-issues"><i class="fa fa-check"></i><b>13.2.9</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#data-analysis-techniques"><i class="fa fa-check"></i><b>13.3</b> Data Analysis Techniques</a><ul>
<li class="chapter" data-level="13.3.1" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#exploratory-techniques"><i class="fa fa-check"></i><b>13.3.1</b> Exploratory Techniques</a></li>
<li class="chapter" data-level="13.3.2" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#confirmatory-techniques"><i class="fa fa-check"></i><b>13.3.2</b> Confirmatory Techniques</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#some-r-functions"><i class="fa fa-check"></i><b>13.4</b> Some R Functions</a></li>
<li class="chapter" data-level="13.5" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="ChapDataSystems.html"><a href="ChapDataSystems.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html"><i class="fa fa-check"></i><b>14</b> Dependence Modeling</a><ul>
<li class="chapter" data-level="14.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:VarTypes"><i class="fa fa-check"></i><b>14.1</b> Variable Types</a><ul>
<li class="chapter" data-level="14.1.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:QuaVar"><i class="fa fa-check"></i><b>14.1.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:QuanVar"><i class="fa fa-check"></i><b>14.1.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>14.1.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>14.2</b> Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>14.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>14.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>14.2.3</b> Nominal Variables</a></li>
<li class="chapter" data-level="14.2.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ordinal-variables"><i class="fa fa-check"></i><b>14.2.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="14.2.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#interval-variables"><i class="fa fa-check"></i><b>14.2.5</b> Interval Variables</a></li>
<li class="chapter" data-level="14.2.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#discrete-and-continuous-variables"><i class="fa fa-check"></i><b>14.2.6</b> Discrete and Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>14.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="14.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>14.4</b> Application Using Copulas</a><ul>
<li class="chapter" data-level="14.4.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#data-description"><i class="fa fa-check"></i><b>14.4.1</b> Data Description</a></li>
<li class="chapter" data-level="14.4.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>14.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="14.4.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>14.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="14.4.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>14.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>14.5</b> Types of Copulas</a><ul>
<li class="chapter" data-level="14.5.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#normal-gaussian-copulas"><i class="fa fa-check"></i><b>14.5.1</b> Normal (Gaussian) Copulas</a></li>
<li class="chapter" data-level="14.5.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#t--and-elliptical-copulas"><i class="fa fa-check"></i><b>14.5.2</b> <em>t</em>- and Elliptical Copulas</a></li>
<li class="chapter" data-level="14.5.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#archimedean-copulas"><i class="fa fa-check"></i><b>14.5.3</b> Archimedean Copulas</a></li>
<li class="chapter" data-level="14.5.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>14.5.4</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>14.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="14.7" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>14.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ts-14.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 14.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="CAppA.html"><a href="CAppA.html"><i class="fa fa-check"></i><b>15</b> Appendix A: Review of Statistical Inference</a><ul>
<li class="chapter" data-level="15.1" data-path="CAppA.html"><a href="CAppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>15.1</b> Basic Concepts</a><ul>
<li class="chapter" data-level="15.1.1" data-path="CAppA.html"><a href="CAppA.html#random-sampling"><i class="fa fa-check"></i><b>15.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="15.1.2" data-path="CAppA.html"><a href="CAppA.html#sampling-distribution"><i class="fa fa-check"></i><b>15.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="15.1.3" data-path="CAppA.html"><a href="CAppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>15.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="CAppA.html"><a href="CAppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>15.2</b> Point Estimation and Properties</a><ul>
<li class="chapter" data-level="15.2.1" data-path="CAppA.html"><a href="CAppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>15.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="15.2.2" data-path="CAppA.html"><a href="CAppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>15.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="CAppA.html"><a href="CAppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>15.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="15.3.1" data-path="CAppA.html"><a href="CAppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>15.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="15.3.2" data-path="CAppA.html"><a href="CAppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>15.3.2</b> Large-sample Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="15.3.3" data-path="CAppA.html"><a href="CAppA.html#confidence-interval"><i class="fa fa-check"></i><b>15.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>15.4</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="15.4.1" data-path="CAppA.html"><a href="CAppA.html#basic-concepts"><i class="fa fa-check"></i><b>15.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="15.4.2" data-path="CAppA.html"><a href="CAppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>15.4.2</b> Student-<em>t</em> test based on <em>mle</em></a></li>
<li class="chapter" data-level="15.4.3" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>15.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="15.4.4" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>15.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="CAppB.html"><a href="CAppB.html"><i class="fa fa-check"></i><b>16</b> Appendix B: Iterated Expectations</a><ul>
<li class="chapter" data-level="16.1" data-path="CAppB.html"><a href="CAppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>16.1</b> Conditional Distribution and Conditional Expectation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="CAppB.html"><a href="CAppB.html#conditional-distribution"><i class="fa fa-check"></i><b>16.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="16.1.2" data-path="CAppB.html"><a href="CAppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>16.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="CAppB.html"><a href="CAppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>16.2</b> Iterated Expectations and Total Variance</a><ul>
<li class="chapter" data-level="16.2.1" data-path="CAppB.html"><a href="CAppB.html#S:AppB:LIE"><i class="fa fa-check"></i><b>16.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="16.2.2" data-path="CAppB.html"><a href="CAppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>16.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="CAppB.html"><a href="CAppB.html#application"><i class="fa fa-check"></i><b>16.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="CAppB.html"><a href="CAppB.html#S:AppConjugateDistributions"><i class="fa fa-check"></i><b>16.3</b> Conjugate Distributions</a><ul>
<li class="chapter" data-level="16.3.1" data-path="CAppB.html"><a href="CAppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>16.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="16.3.2" data-path="CAppB.html"><a href="CAppB.html#S:IterExp:Conjugate"><i class="fa fa-check"></i><b>16.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="CAppC.html"><a href="CAppC.html"><i class="fa fa-check"></i><b>17</b> Appendix C: Maximum Likelihood Theory</a><ul>
<li class="chapter" data-level="17.1" data-path="CAppC.html"><a href="CAppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>17.1</b> Likelihood Function</a><ul>
<li class="chapter" data-level="17.1.1" data-path="CAppC.html"><a href="CAppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="17.1.2" data-path="CAppC.html"><a href="CAppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>17.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="CAppC.html"><a href="CAppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>17.2</b> Maximum Likelihood Estimators</a><ul>
<li class="chapter" data-level="17.2.1" data-path="CAppC.html"><a href="CAppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>17.2.1</b> Definition and Derivation of <em>MLE</em></a></li>
<li class="chapter" data-level="17.2.2" data-path="CAppC.html"><a href="CAppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotic Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="17.2.3" data-path="CAppC.html"><a href="CAppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>17.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="CAppC.html"><a href="CAppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>17.3</b> Statistical Inference Based on Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="17.3.1" data-path="CAppC.html"><a href="CAppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>17.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="17.3.2" data-path="CAppC.html"><a href="CAppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>17.3.2</b> <em>MLE</em> and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html"><i class="fa fa-check"></i><b>18</b> Appendix D: Summary of Distributions</a><ul>
<li class="chapter" data-level="18.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:DiscreteDistributions"><i class="fa fa-check"></i><b>18.1</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>18.1.1</b> The <em>(a,b,0)</em> Class</a></li>
<li class="chapter" data-level="18.1.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>18.1.2</b> The <em>(a,b,1)</em> Class</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:ContinuousDistributions"><i class="fa fa-check"></i><b>18.2</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>18.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>18.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>18.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.4" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>18.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="18.2.5" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>18.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="18.2.6" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>18.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>18.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html"><i class="fa fa-check"></i><b>19</b> Appendix E: Conventions for Notation</a><ul>
<li class="chapter" data-level="19.1" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:General"><i class="fa fa-check"></i><b>19.1</b> General Conventions</a></li>
<li class="chapter" data-level="19.2" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Abbreviations"><i class="fa fa-check"></i><b>19.2</b> Abbreviations</a></li>
<li class="chapter" data-level="19.3" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:StatSymbols"><i class="fa fa-check"></i><b>19.3</b> Common Statistical Symbols and Operators</a></li>
<li class="chapter" data-level="19.4" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Symbols"><i class="fa fa-check"></i><b>19.4</b> Common Mathematical Symbols and Functions</a></li>
<li class="chapter" data-level="19.5" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#further-readings"><i class="fa fa-check"></i><b>19.5</b> Further Readings</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ChapFrequency-Modeling" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Frequency Modeling</h1>
<p><em>Chapter Preview.</em> A primary focus for insurers is estimating the magnitude of aggregate claims it must bear under its insurance contracts. <a href="#" class="tooltip" style="color:green"><em>Aggregate claims</em><span style="font-size:8pt">The sum of all claims observed in a period of time</span></a> are affected by both the frequency and the severity of the insured event. Decomposing aggregate claims into these two components, each of which warrant significant attention, is essential for analysis and pricing. This chapter discusses frequency distributions, summary measures, and parameter estimation techniques.</p>
<p>In Section <a href="ChapFrequency-Modeling.html#S:frequency-distributions">2.1</a>, we present terminology and discuss reasons why we study frequency and severity separately. The foundations of frequency distributions and measures are presented in Section <a href="ChapFrequency-Modeling.html#S:basic-frequency-distributions">2.2</a> along with three principal distributions: the binomial, the Poisson, and the negative binomial. These three distributions are members of what is known as the <span class="math inline">\((a,b,0)\)</span> class of distributions, a distinguishing, identifying feature which allows for efficient calculation of probabilities, further discussed in Section <a href="ChapFrequency-Modeling.html#S:the-a-b-0-class">2.3</a>. When fitting a dataset with a distribution, parameter values need to be estimated and in Section <a href="ChapFrequency-Modeling.html#S:estimating-frequency-distributions">2.4</a>, the procedure for maximum likelihood estimation is explained.</p>
<p>For insurance datasets, the observation at zero denotes no occurrence of a particular event; this often deserves additional attention. As explained further in Section <a href="ChapFrequency-Modeling.html#S:other-frequency-distributions">2.5</a>, for some datasets it may be impossible to have zero of the studied event or zero events may follow a different model than other event counts.
In either case, direct fitting of typical count models could lead to improper estimates. Zero truncation or modification techniques allow for more appropriate distribution fit.</p>
<p>Noting that our insurance portfolio could consist of different sub-groups, each with its own set of individual characteristics, Section <a href="ChapFrequency-Modeling.html#S:mixture-distributions">2.6</a> introduces mixture distributions and methodology to allow for this heterogeneity within a portfolio. Section <a href="ChapFrequency-Modeling.html#S:goodness-of-fit">2.7</a> describes goodness of fit which measures the reasonableness of the parameter estimates. Exercises are presented in Section <a href="ChapFrequency-Modeling.html#S:exercises">2.8</a> and Section <a href="ChapFrequency-Modeling.html#S:rcode">2.9.1</a> concludes the chapter with <code>R</code> Code for plots depicted in Section <a href="ChapFrequency-Modeling.html#S:estimating-frequency-distributions">2.4</a>.</p>
<div id="S:frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Frequency Distributions</h2>
<hr />
<p>In this section, you learn how to summarize the importance of frequency modeling in terms of</p>
<ul>
<li>contractual,</li>
<li>behavioral,</li>
<li>database, and</li>
<li>regulatory/administrative motivations.</li>
</ul>
<hr />
<div id="S:how-frequency-augments-severity-information" class="section level3">
<h3><span class="header-section-number">2.1.1</span> How Frequency Augments Severity Information</h3>
<div id="S:basic-terminology" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Basic Terminology</h4>
<p>In this chapter, <strong>loss</strong>, also referred to as ground-up loss, denotes the amount of financial loss suffered by the insured. We use <strong>claim</strong> to denote the indemnification upon the occurrence of an insured event, thus the amount paid by the insurer. While some texts use <strong>loss</strong> and <strong>claim</strong> interchangeably, we wish to make a distinction here to recognize how insurance contractual provisions, such as deductibles and limits, affect the size of the claim stemming from a loss. <a href="#" class="tooltip" style="color:green"><em>Frequency</em><span style="font-size:8pt">Count random variables that represent the number of claims</span></a> represents how often an insured event occurs, typically within a policy contract. Here, we focus on count random variables that represent the number of claims, that is, how frequently an event occurs. <a href="#" class="tooltip" style="color:green"><em>Severity</em><span style="font-size:8pt">The amount, or size, of each payment for an insured event</span></a> denotes the amount, or size, of each payment for an insured event. In future chapters, the aggregate model, which combines frequency models with severity models, is examined.</p>
</div>
<div id="S:the-importance-of-frequency" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> The Importance of Frequency</h4>
<p>Recall from Section <a href="ChapIntro.html#S:PredModApps">1.2</a> that setting the price of an insurance good can be a complex problem. In manufacturing, the cost of a good is (relatively) known. In other financial service areas, market prices are available. In insurance, we can generalize the price setting as follows. Start with an expected cost, then add “margins” to account for the product’s riskiness, expenses incurred in servicing the product, and a profit/surplus allowance for the insurer.</p>
<p>The expected cost for insurance can be determined as the expected number of claims times the amount per claim, that is, expected value of <em>frequency times severity</em>. The focus on claim count allows the insurer to consider those factors which directly affect the occurrence of a loss, thereby potentially generating a claim.</p>
<!-- The frequency process can then be modeled. -->
</div>
<div id="S:why-examine-frequency-information" class="section level4">
<h4><span class="header-section-number">2.1.1.3</span> Why Examine Frequency Information?</h4>
<p>Insurers and other stakeholders, including governmental organizations, have various motivations for gathering and maintaining frequency datasets.</p>
<ul>
<li><p><strong>Contractual.</strong> In insurance contracts, it is common for particular deductibles and policy limits to be listed and invoked for each occurrence of an insured event. Correspondingly, the claim count data generated would indicate the number of claims which meet these criteria, offering a unique claim frequency measure. Extending this, models of total insured losses would need to account for deductibles and policy limits for each insured event.</p></li>
<li><p><strong>Behavioral.</strong> In considering factors that influence loss frequency, the risk-taking and risk-reducing behavior of individuals and companies should be considered. Explanatory (rating) variables can have different effects on models of how often an event occurs in contrast to the size of the event.</p>
<ul>
<li>In healthcare, the decision to utilize healthcare by individuals, and minimize such healthcare utilization through preventive care and wellness measures, is related primarily to his or her personal characteristics. The cost per user is determined by the patient’s medical condition, potential treatment measures, and decisions made by the healthcare provider (such as the physician) and the patient. While there is overlap in those factors and how they affect total healthcare costs, attention can be focused on those separate drivers of healthcare visit frequency and healthcare cost severity.</li>
<li>In personal lines, prior claims history is an important underwriting factor. It is common to use an indicator of whether or not the insured had a claim within a certain time period prior to the contract. Also, the number of claims incurred by the insured in previous periods has predictive power.</li>
<li>In homeowners insurance, in modeling potential loss frequency, the insurer could consider loss prevention measures that the homeowner has adopted, such as visible security systems. Separately, when modeling loss severity, the insurer would examine those factors that affect repair and replacement costs.</li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li><p><strong>Databases</strong>. Insurers may hold separate data files that suggest developing separate frequency and severity models. For example, a policyholder file is established when a policy is written. This file records much underwriting information about the insured(s), such as age, gender, and prior claims experience, policy information such as coverage, deductibles and limitations, as well as any insurance claims event. A separate file, known as the “claims” file, records details of the claim against the insurer, including the amount. (There may also be a “payments” file that records the timing of the payments although we shall not deal with that here.) This recording process could then extend to insurers modeling the frequency and severity as separate processes.</p></li>
<li><p><strong>Regulatory and Administrative.</strong> Insurance is a highly regulated and monitored industry, given its importance in providing financial security to individuals and companies facing risk. As part of their duties, regulators routinely require the reporting of claims numbers as well as amounts. This may be due to the fact that there can be alternative definitions of an “amount,” e.g., paid versus incurred, and there is less potential error when reporting claim numbers. This continual monitoring helps ensure financial stability of these insurance companies.</p></li>
</ul>
<div id="surveyElement21">

</div>
<div id="surveyResult21">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz21.1" href="javascript:toggleQuiz
('display.Quiz21.2','display.Quiz21.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz21.2" style="display: none">
<p id="Quiz21Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz21.js">
</script>
</div>
</div>
</div>
<div id="S:basic-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.2</span> Basic Frequency Distributions</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Determine quantities that summarize a distribution such as the distribution and survival function, as well as moments such as the mean and variance</li>
<li>Define and compute the moment and probability generating functions</li>
<li>Describe and understand relationships among three important frequency distributions, the binomial, Poisson, and negative binomial distributions</li>
</ul>
<hr />
<p>In this section, we introduce the distributions that are commonly used in actuarial practice to model count data. The claim count random variable is denoted by <span class="math inline">\(N\)</span>; by its very nature it assumes only non-negative integer values. Hence the distributions below are all discrete distributions supported on the set of non-negative integers <span class="math inline">\(\{0, 1, \ldots \}\)</span>.</p>
<div id="S:foundations" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Foundations</h3>
<p>Since <span class="math inline">\(N\)</span> is a discrete random variable taking values in <span class="math inline">\(\{0, 1, \ldots \}\)</span>, the most natural full description of its distribution is through the specification of the probabilities with which it assumes each of the non-negative integer values. This leads us to the concept of the <a href="#" class="tooltip" style="color:green"><em>probability mass function (pmf)</em><span style="font-size:8pt">A function that gives the probability that a discrete random variable is exactly equal to some value</span></a> of <span class="math inline">\(N\)</span>, denoted as <span class="math inline">\(p_N(\cdot)\)</span> and defined as follows:</p>
<p><span class="math display">\[
p_N(k)=\Pr(N=k), \quad \hbox{for } k=0,1,\ldots
\]</span></p>
<p>We note that there are alternate complete descriptions, or characterizations, of the distribution of <span class="math inline">\(N\)</span>; for example, the <a href="#" class="tooltip" style="color:green"><em>distribution function</em><span style="font-size:8pt">The chance that the random variable is less than or equal to x, as a function of x</span></a> of <span class="math inline">\(N\)</span> defined by <span class="math inline">\(F_N(x) = \Pr(N \le x)\)</span> and determined as:</p>
<p><span class="math display">\[
F_N(x)=\begin{cases}
\sum\limits_{k=0}^{\lfloor x \rfloor } \Pr(N=k), &amp;x\geq 0;\\
0, &amp; \hbox{otherwise}.
\end{cases}
\]</span></p>
<p>In the above, <span class="math inline">\(\lfloor \cdot \rfloor\)</span> denotes the floor function; <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the greatest integer less than or equal to <span class="math inline">\(x\)</span>. This expression also suggests the descriptor <em>cumulative distribution function,</em> a commonly used alternative way of expressing the distribution function. We also note that the <a href="#" class="tooltip" style="color:green"><em>survival function</em><span style="font-size:8pt">The probability that the random variable takes on a value greater than a number x</span></a> of <span class="math inline">\(N\)</span>, denoted by <span class="math inline">\(S_N(\cdot)\)</span>, is defined as the ones’-complement of <span class="math inline">\(F_N(\cdot)\)</span>, <em>i.e.</em> <span class="math inline">\(S_N(\cdot)=1-F_N(\cdot)\)</span>. Clearly, the latter is another characterization of the distribution of <span class="math inline">\(N\)</span>.</p>
<p>Often one is interested in quantifying a certain aspect of the distribution and not in its complete description. This is particularly useful when comparing distributions. A <em>center of location</em> of the distribution is one such aspect, and there are many different measures that are commonly used to quantify it. Of these, the <a href="#" class="tooltip" style="color:green"><em>mean</em><span style="font-size:8pt">Average</span></a> is the most popular; the mean of <span class="math inline">\(N\)</span>, denoted by <span class="math inline">\(\mu_N\)</span>,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> is defined as</p>
<p><span class="math display">\[
\mu_N=\sum_{k=0}^\infty k~p_N(k).
\]</span></p>
<p>We note that <span class="math inline">\(\mu_N\)</span> is the expected value of the random variable <span class="math inline">\(N\)</span>, <em>i.e.</em> <span class="math inline">\(\mu_N=\mathrm{E}[N]\)</span>. This leads to a general class of measures, the <a href="#" class="tooltip" style="color:green"><em>moments</em><span style="font-size:8pt">The rth moment of a list is the average value of the random variable raised to the rth power</span></a> of the distribution; the <span class="math inline">\(r\)</span>-th raw moment of <span class="math inline">\(N\)</span>, for <span class="math inline">\(r&gt; 0\)</span>, is defined as <span class="math inline">\(\mathrm{E}{[N^r]}\)</span> and denoted by <span class="math inline">\(\mu_N&#39;(r)\)</span>. We remark that the prime <span class="math inline">\(\prime\)</span> here does <em>not</em> denote differentiation. Rather, it is commonly used notation to distinguish a raw from a central moment, as will be introduction in Section <a href="ChapSeverity.html#S:Chap3Moments">3.1.1</a>. For <span class="math inline">\(r&gt;0\)</span>, we have</p>
<p><span class="math display">\[
\mu_N&#39;(r)= \mathrm{E}{[N^r]}= \sum_{k=0}^\infty k^r~p_N(k).
\]</span></p>
<p>We note that <span class="math inline">\(\mu_N&#39;(\cdot)\)</span> is a well-defined non-decreasing function taking values in <span class="math inline">\([0,\infty]\)</span>, as <span class="math inline">\(\Pr(N\in\{0, 1, \ldots \})=1\)</span>; also, note that <span class="math inline">\(\mu_N=\mu_N&#39;(1)\)</span>. In the following, when we refer to a moment it will be implicit that it is finite unless mentioned otherwise.</p>
<p>Another basic aspect of a distribution is its dispersion, and of the various measures of dispersion studied in the literature, the <a href="#" class="tooltip" style="color:green"><em>standard deviation</em><span style="font-size:8pt">The square-root of variance</span></a> is the most popular. Towards defining it, we first define the <a href="#" class="tooltip" style="color:green"><em>variance</em><span style="font-size:8pt">Second central moment of a random variable x, measuring the expected squared deviation of between the variable and its mean</span></a> of <span class="math inline">\(N\)</span>, denoted by <span class="math inline">\(\mathrm{Var}[N]\)</span>, as <span class="math inline">\(\mathrm{Var}[N]=\mathrm{E}{[(N-\mu_N)^2]}\)</span> when <span class="math inline">\(\mu_N\)</span> is finite. By basic properties of the expected value of a random variable, we see that <span class="math inline">\(\mathrm{Var}[N]=\mathrm{E}[N^2]-[\mathrm{E}(N)]^2\)</span>. The standard deviation of <span class="math inline">\(N\)</span>, denoted by <span class="math inline">\(\sigma_N\)</span>, is defined as the square root of <span class="math inline">\(\mathrm{Var}~N\)</span>. Note that the latter is well-defined as <span class="math inline">\(\mathrm{Var}[N]\)</span>, by its definition as the average squared deviation from the mean, is non-negative; <span class="math inline">\(\mathrm{Var}[N]\)</span> is denoted by <span class="math inline">\(\sigma_N^2\)</span>. Note that these two measures take values in <span class="math inline">\([0,\infty]\)</span>.</p>
</div>
<div id="S:generating-functions" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Moment and Probability Generating Functions</h3>
<p>Now we introduce two generating functions that are found to be useful when working with count variables. Recall that for a discrete random variable, the <a href="#" class="tooltip" style="color:green"><em>moment generating function (mgf)</em><span style="font-size:8pt">The mgf of random variable n is defined the expectation of exp(tn), as a function of t</span></a> of <span class="math inline">\(N\)</span>, denoted as <span class="math inline">\(M_N(\cdot)\)</span>, is defined as
<span class="math display">\[
M_N(t) = \mathrm{E}~{[e^{tN}]} = \sum^{\infty}_{k=0} ~e^{tk}~ p_N(k), \quad t\in \mathbb{R}.
\]</span>
We note that while <span class="math inline">\(M_N(\cdot)\)</span> is well defined as it is the expectation of a non-negative random variable (<span class="math inline">\(e^{tN}\)</span>), though it can assume the value <span class="math inline">\(\infty\)</span>. Note that for a count random variable, <span class="math inline">\(M_N(\cdot)\)</span> is finite valued on <span class="math inline">\((-\infty,0]\)</span> with <span class="math inline">\(M_N(0)=1\)</span>. The following theorem, whose proof can be found in <span class="citation">Billingsley (<a href="#ref-billingsley" role="doc-biblioref">2008</a>)</span> (pages 285-6), encapsulates the reason for its name.</p>
<hr />
<p><a id=thm:2.1></a></p>
<p><strong>Theorem 2.1.</strong><br />
Let <span class="math inline">\(N\)</span> be a count random variable such that <span class="math inline">\(\mathrm{E}~{[e^{t^\ast N}]}\)</span> is finite for some <span class="math inline">\(t^\ast&gt;0\)</span>. We have the following:</p>
<ol style="list-style-type: lower-alpha">
<li>All moments of <span class="math inline">\(N\)</span> are finite, <em>i.e.</em></li>
</ol>
<p><span class="math display">\[
\mathrm{E}{[N^r]}&lt;\infty, \quad r &gt; 0.
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>The <em>mgf</em> can be used to <em>generate</em> its moments as follows:
<span class="math display">\[
\left.\frac{{\rm d}^m}{{\rm d}t^m} M_N(t)\right\vert_{t=0}=\mathrm{E}[N^m], \quad m\geq 1.
\]</span></li>
<li>The <em>mgf</em> <span class="math inline">\(M_N(\cdot)\)</span> characterizes the distribution; in other words it uniquely specifies the distribution.</li>
</ol>
<hr />
<p>Another reason that the <em>mgf</em> is very useful as a tool is that for two independent random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, with their mgfs existing in a neighborhood of <span class="math inline">\(0\)</span>, the <em>mgf</em> of <span class="math inline">\(X+Y\)</span> is the product of their respective mgfs, that is, <span class="math inline">\(M_{X+Y}(t) = M_{X}(t)M_{Y}(t)\)</span>, for small <span class="math inline">\(t\)</span>.</p>
<p>A related generating function to the <em>mgf</em> is the <a href="#" class="tooltip" style="color:green"><em>probability generating function (pgf)</em><span style="font-size:8pt">For a random variable n, its pgf is defined as the expectation of s^n, as a function of s</span></a>, and is a useful tool for random variables taking values in the non-negative integers. For a random variable <span class="math inline">\(N\)</span>, by <span class="math inline">\(P_N(\cdot)\)</span> we denote its <em>pgf</em> and we define it as follows<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>:</p>
<p><span class="math display">\[
P_N(s)=\mathrm{E}~{[s^N]}, \quad s\geq 0.
\]</span></p>
<p>It is straightforward to see that if the <em>mgf</em> <span class="math inline">\(M_N(\cdot)\)</span> exists on <span class="math inline">\((-\infty,t^\ast)\)</span> then
<span class="math display">\[
P_N(s)=M_N(\log(s)), \quad s&lt;e^{t^\ast}.
\]</span>
Moreover, if the <em>pgf</em> exists on an interval <span class="math inline">\([0,s^\ast)\)</span> with <span class="math inline">\(s^\ast&gt;1\)</span>, then the <em>mgf</em> <span class="math inline">\(M_N(\cdot)\)</span> exists on <span class="math inline">\((-\infty,\log(s^\ast))\)</span>, and hence uniquely specifies the distribution of <span class="math inline">\(N\)</span> by <a href="#thm:2.1">Theorem 2.1</a>. (As a reminder, throughout this text we use <em>log</em> as the natural logarithm, not the base ten (common) logarithm or other version.) The following result for <em>pgf</em> is an analog of <a href="#thm:2.1">Theorem 2.1</a>, and in particular justifies its name.</p>
<hr />
<p><a id=thm:2.2></a></p>
<p><strong>Theorem 2.2.</strong>
Let <span class="math inline">\(N\)</span> be a count random variable such that <span class="math inline">\(\mathrm{E}~{(s^{\ast})^N}\)</span> is finite for some <span class="math inline">\(s^\ast&gt;1\)</span>. We have the following:</p>
<ol style="list-style-type: lower-alpha">
<li>All moments of <span class="math inline">\(N\)</span> are finite, <em>i.e.</em>
<span class="math display">\[
\mathrm{E}~{N^r}&lt;\infty, \quad r\geq 0.
\]</span></li>
<li>The <span class="math inline">\(pmf\)</span> of <span class="math inline">\(N\)</span> can be derived from the <em>pgf</em> as follows:
<span class="math display">\[
p_N(m)=\begin{cases} 
P_N(0), &amp;m=0;\cr
&amp;\cr
\left(\frac{1}{m!}\right) \left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=0}\;, &amp;m\geq 1.\cr
\end{cases}
\]</span></li>
<li>The factorial moments of <span class="math inline">\(N\)</span> can be derived as follows:
<span class="math display">\[
\left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=1}=\mathrm{E}~{\prod\limits_{i=0}^{m-1} (N-i)}, \quad m\geq 1.
\]</span></li>
<li>The <em>pgf</em> <span class="math inline">\(P_N(\cdot)\)</span> characterizes the distribution; in other words it uniquely specifies the distribution.</li>
</ol>
</div>
<div id="S:important-frequency-distributions" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Important Frequency Distributions</h3>
<p>In this sub-section we study three important frequency distributions used in statistics, namely the binomial, the Poisson, and the negative binomial distributions. In the following, a risk denotes a unit covered by insurance. A risk could be an individual, a building, a company, or some other identifier for which insurance coverage is provided. For context, imagine an insurance data set containing the number of claims by risk or stratified in some other manner. The above mentioned distributions also happen to be the most commonly used in insurance practice for reasons, some of which we mention below.</p>
<ul>
<li>These distributions can be motivated by natural random experiments which are good approximations to real life processes from which many insurance data arise. Hence, not surprisingly, they together offer a reasonable fit to many insurance data sets of interest. The appropriateness of a particular distribution for the set of data can be determined using standard statistical methodologies, as we discuss later in this chapter.</li>
<li>They provide a rich enough basis for generating other distributions that even better approximate or well cater to more real situations of interest to us.
<ul>
<li>The three distributions are either one-parameter or two-parameter distributions. In fitting to data, a parameter is assigned a particular value. The set of these distributions can be enlarged to their <a href="#" class="tooltip" style="color:green"><em>convex hulls</em><span style="font-size:8pt">The convex hull of a set of points x is the smallest convex set that contains x</span></a> by treating the parameter(s) as a random variable (or vector) with its own probability distribution, with this larger set of distributions offering greater flexibility. A simple example that is better addressed by such an enlargement is a portfolio of claims generated by insureds belonging to many different <a href="#" class="tooltip" style="color:green"><em>risk classes</em><span style="font-size:8pt">The formation of different premiums for the same coverage based on each homogeneous group’s characteristics.</span></a>.</li>
<li>In insurance data, we may observe either a marginal or inordinate number of zeros, that is, zero claims by risk. When fitting to the data, a frequency distribution in its standard specification often fails to reasonably account for this occurrence. The natural modification of the above three distributions, however, accommodate this phenomenon well towards offering a better fit.</li>
<li>In insurance we are interested in total claims paid, whose distribution results from compounding the fitted frequency distribution with a severity distribution. These three distributions have properties that make it easy to work with the resulting aggregate severity distribution.</li>
</ul></li>
</ul>
<!-- \phantom{It is useful to get an example here of a discrete distribution with three point support and derive its mgf/pgf/moments etc..} -->
<div id="S:binomial-distribution" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Binomial Distribution</h4>
<p>We begin with the binomial distribution which arises from any finite sequence of identical and independent experiments with <a href="#" class="tooltip" style="color:green"><em>binary outcomes</em><span style="font-size:8pt">Outcomes whose unit can take on only two possible states, traditionally labeled as 0 and 1</span></a>. The most canonical of such experiments is the (biased or unbiased) coin tossing experiment with the outcome being heads or tails. So if <span class="math inline">\(N\)</span> denotes the number of heads in a sequence of <span class="math inline">\(m\)</span> independent coin tossing experiments with an identical coin which turns heads up with probability <span class="math inline">\(q\)</span>, then the distribution of <span class="math inline">\(N\)</span> is called the <a href="#" class="tooltip" style="color:green"><em>binomial distribution</em><span style="font-size:8pt">A random variable has a binomial distribution (with parameters m and q) if it is the number of “successes” in a fixed number m of independent random trials, all of which have the same probability q of resulting in “success.” </span></a> with parameters <span class="math inline">\((m,q)\)</span>, with <span class="math inline">\(m\)</span> a positive integer and <span class="math inline">\(q\in[0,1]\)</span>. Note that when <span class="math inline">\(q=0\)</span> (resp., <span class="math inline">\(q=1\)</span>) then the distribution is degenerate with <span class="math inline">\(N=0\)</span> (resp., <span class="math inline">\(N=m\)</span>) with probability <span class="math inline">\(1\)</span>. Clearly, its support when <span class="math inline">\(q\in(0,1)\)</span> equals <span class="math inline">\(\{0,1,\ldots,m\}\)</span> with <em>pmf</em> given by<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display">\[
p_k= {m \choose k} q^k (1-q)^{m-k}, \quad k=0,\ldots,m.
\]</span></p>
<p>where <span class="math display">\[{m \choose k} = \frac{m!}{k!(m-k)!}\]</span></p>
<p>The reason for its name is that the <em>pmf</em> takes values among the terms that arise from the binomial expansion of <span class="math inline">\((q +(1-q))^m\)</span>. This realization then leads to the the following expression for the <em>pgf</em> of the binomial distribution:</p>
<p><span class="math display">\[
\begin{array}{ll}
P_N(z) &amp;= \sum_{k=0}^m z^k {m \choose k} q^k (1-q)^{m-k} \\
&amp;= \sum_{k=0}^m  {m \choose k} (zq)^k (1-q)^{m-k} \\
&amp;= (qz+(1-q))^m = (1+q(z-1))^m.
\end{array}
\]</span></p>
<p>Note that the above expression for the <em>pgf</em> confirms the fact that the binomial distribution is the <a href="#" class="tooltip" style="color:green"><em>m-convolution</em><span style="font-size:8pt">The addition of m independent random variables</span></a> of the Bernoulli distribution, which is the binomial distribution with <span class="math inline">\(m=1\)</span> and <em>pgf</em> <span class="math inline">\((1+q(z-1))\)</span>. By “m-convolution,” we mean that we can write <span class="math inline">\(N\)</span> as the sum of <span class="math inline">\(N_1,\ldots,N_m\)</span>. Here, <span class="math inline">\(N_i\)</span> are <a href="#" class="tooltip" style="color:green"><em>iid</em><span style="font-size:8pt">Independent and identically distributed</span></a> Bernoulli variates. Also, note that the <em>mgf</em> of the binomial distribution is given by <span class="math inline">\((1+q(e^t-1))^m\)</span>.</p>
<p>The mean and variance of the binomial distribution can be found in a few different ways. To emphasize the key property that it is a <span class="math inline">\(m\)</span>-convolution of the Bernoulli distribution, we derive below the moments using this property. We begin by observing that the Bernoulli distribution with parameter <span class="math inline">\(q\)</span> assigns probability of <span class="math inline">\(q\)</span> and <span class="math inline">\(1-q\)</span> to <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>, respectively. So its mean equals <span class="math inline">\(q\)</span> (<span class="math inline">\(=0\times (1-q) + 1\times q\)</span>); note that its raw second moment equals its mean as <span class="math inline">\(N^2=N\)</span> with probability <span class="math inline">\(1\)</span>. Using these two facts we see that the variance equals <span class="math inline">\(q(1-q)\)</span>. Moving on to the binomial distribution with parameters <span class="math inline">\(m\)</span> and <span class="math inline">\(q\)</span>, using the fact that it is the <span class="math inline">\(m\)</span>-convolution of the Bernoulli distribution, we write <span class="math inline">\(N\)</span> as the sum of <span class="math inline">\(N_1,\ldots,N_m\)</span>, where <span class="math inline">\(N_i\)</span> are <em>iid</em> Bernoulli variates, as above. Now using the moments of Bernoulli and linearity of the expectation, we see that</p>
<p><span class="math display">\[
\mathrm{E}[{N}]=\mathrm{E}\left[{\sum_{i=1}^m N_i}\right] = \sum_{i=1}^m ~\mathrm{E}[N_i] = mq.
\]</span></p>
<p>Also, using the fact that the variance of the sum of independent random variables is the sum of their variances, we see that<br />
<span class="math display">\[
\mathrm{Var}[{N}]=\mathrm{Var}~\left[{\sum_{i=1}^m N_i}\right]=\sum_{i=1}^m \mathrm{Var}[{N_i}] = mq(1-q).
\]</span>
Alternate derivations of the above moments are suggested in the exercises. One important observation, especially from the point of view of applications, is that the mean is greater than the variance unless <span class="math inline">\(q=0\)</span>.</p>
</div>
<div id="S:poisson-distribution" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Poisson Distribution</h4>
<p>After the binomial distribution, the <a href="#" class="tooltip" style="color:green"><em>Poisson distribution</em><span style="font-size:8pt">A discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event</span></a> (named after the French polymath Simeon Denis Poisson) is probably the most well known of discrete distributions. This is partly due to the fact that it arises naturally as the distribution of the count of the random occurrences of a type of event in a certain time period, if the rate of occurrences of such events is a constant. It also arises as the asymptotic limit of the binomial distribution with <span class="math inline">\(m\rightarrow \infty\)</span> and <span class="math inline">\(mq\rightarrow \lambda\)</span>.</p>
<p>The Poisson distribution is parametrized by a single parameter usually denoted by <span class="math inline">\(\lambda\)</span> which takes values in <span class="math inline">\((0,\infty)\)</span>. Its <em>pmf</em> is given by
<span class="math display">\[
p_k = \frac{e^{-\lambda}\lambda^k}{k!}, k=0,1,\ldots
\]</span>
It is easy to check that the above specifies a <em>pmf</em> as the terms are clearly non-negative, and that they sum to one follows from the infinite Taylor series expansion of <span class="math inline">\(e^\lambda\)</span>. More generally, we can derive its <em>pgf</em>, <span class="math inline">\(P_N(\cdot)\)</span>, as follows:
<span class="math display">\[
P_N(z)= \sum_{k=0}^\infty p_k z^k = \sum_{k=0}^\infty  \frac{e^{-\lambda}\lambda^kz^k}{k!} = e^{-\lambda} e^{\lambda z}
= e^{\lambda(z-1)}, \forall z\in\mathbb{R}.
\]</span>
From the above, we derive its <em>mgf</em> as follows:
<span class="math display">\[
M_N(t)=P_N(e^t)=e^{\lambda(e^t-1)}, t\in \mathbb{R}.
\]</span>
Towards deriving its mean, we note that for the Poisson distribution
<span class="math display">\[
kp_k=\begin{cases}
0,  &amp;k=0 \cr
\lambda~p_{k-1}, &amp;k\geq1 .
\end{cases}
\]</span>
This can be checked easily. In particular, this implies that
<span class="math display">\[
\mathrm{E}[{N}]=\sum_{k\geq 0} k~p_k =\lambda\sum_{k\geq 1} p_{k-1} = \lambda\sum_{j\geq 0} p_{j} =\lambda.
\]</span>
In fact, more generally, using either a generalization of the above or using <a href="#thm:2.1">Theorem 2.1</a>, we see that
<span class="math display">\[
\mathrm{E}{\prod\limits_{i=0}^{m-1} (N-i)}=\left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=1}=\lambda^m, \quad m\geq 1.
\]</span>
This, in particular, implies that
<span class="math display">\[
\mathrm{Var}[{N}]=\mathrm{E}[{N^2}]-[\mathrm{E}({N})]^2 = \mathrm{E}~[N(N-1)]+\mathrm{E}[N]-(\mathrm{E}[{N]})^2=\lambda^2+\lambda-\lambda^2=\lambda.
\]</span>
Note that interestingly for the Poisson distribution <span class="math inline">\(\mathrm{Var}[N]=\mathrm{E}[N]\)</span>.</p>
</div>
<div id="S:negative-binomial-distribution" class="section level4">
<h4><span class="header-section-number">2.2.3.3</span> Negative Binomial Distribution</h4>
<p>The third important count distribution is the <a href="#" class="tooltip" style="color:green"><em>negative binomial distribution</em><span style="font-size:8pt">The number of successes until we observe the rth failure in independent repetitions of an experiment with binary outcomes</span></a>. Recall that the binomial distribution arose as the distribution of the number of <em>successes</em> in <span class="math inline">\(m\)</span> independent repetition of an experiment with binary outcomes. If we instead consider the number of <em>successes</em> until we observe the <span class="math inline">\(r\)</span>-th <em>failure</em> in independent repetitions of an experiment with binary outcomes, then its distribution is a negative binomial distribution. A particular case, when <span class="math inline">\(r=1\)</span>, is the geometric distribution. However when <span class="math inline">\(r\)</span> in not an integer, the above random experiment would not be applicable. In the following, we allow the parameter <span class="math inline">\(r\)</span> to be any positive real number to then motivate the distribution more generally. To explain its name, we recall the binomial series, <em>i.e.</em>
<span class="math display">\[
(1+x)^s= 1 + s x + \frac{s(s-1)}{2!}x^2 + \ldots..., \quad s\in\mathbb{R}; \vert x \vert&lt;1.
\]</span>
If we define <span class="math inline">\({s \choose k}\)</span>, the generalized binomial coefficient, by
<span class="math display">\[
{s \choose k}=\frac{s(s-1)\cdots(s-k+1)}{k!},
\]</span>
then we have
<span class="math display">\[
(1+x)^s= \sum_{k=0}^{\infty} {s \choose k} x^k, \quad s\in\mathbb{R}; \vert x \vert&lt;1.
\]</span>
If we let <span class="math inline">\(s=-r\)</span>, then we see that the above yields
<span class="math display">\[
(1-x)^{-r}= 1 + r x + \frac{(r+1)r}{2!}x^2 + \ldots...= \sum_{k=0}^\infty {r+k-1 \choose k} x^k, \quad r\in\mathbb{R}; \vert x \vert&lt;1.
\]</span>
This implies that if we define <span class="math inline">\(p_k\)</span> as
<span class="math display">\[
p_k = {r+k-1 \choose k} \left(\frac{1}{1+\beta}\right)^r \left(\frac{\beta}{1+\beta}\right)^k, \quad k=0,1,\ldots
\]</span>
for <span class="math inline">\(r&gt;0\)</span> and <span class="math inline">\(\beta\geq0\)</span>, then it defines a valid <em>pmf</em>. Such defined distribution is called the negative binomial distribution with parameters <span class="math inline">\((r,\beta)\)</span> with <span class="math inline">\(r&gt;0\)</span> and <span class="math inline">\(\beta\geq 0\)</span>. Moreover, the binomial series also implies that the <em>pgf</em> of this distribution is given by
<span class="math display">\[
\begin{aligned}
  P_N(z) &amp;= (1-\beta(z-1))^{-r}, \quad \vert z \vert &lt; 1+\frac{1}{\beta}, \beta\geq0. 
\end{aligned}
\]</span>
The above implies that the <em>mgf</em> is given by
<span class="math display">\[
\begin{aligned}
  M_N(t) &amp;= (1-\beta(e^t-1))^{-r}, \quad t &lt; \log\left(1+\frac{1}{\beta}\right), \beta\geq0. 
\end{aligned}
\]</span>
We derive its moments using <a href="#thm:2.1">Theorem 2.1</a> as follows:</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathrm{E}[N]&amp;=&amp;M&#39;(0)= \left. r\beta e^t (1-\beta(e^t-1))^{-r-1}\right\vert_{t=0}=r\beta;\\
\mathrm{E}[N^2]&amp;=&amp;M&#39;&#39;(0)= \left.\left[ r\beta e^t (1-\beta(e^t-1))^{-r-1} + r(r+1)\beta^2 e^{2t} (1-\beta(e^t-1))^{-r-2}\right]\right\vert_{t=0}\\
&amp;=&amp;r\beta(1+\beta)+r^2\beta^2;\\
\hbox{and }\mathrm{Var}[N]&amp;=&amp;\mathrm{E}{[N^2]}-(\mathrm{E}[{N}])^2=r\beta(1+\beta)+r^2\beta^2-r^2\beta^2=r\beta(1+\beta)
\end{eqnarray*}\]</span></p>
<p>We note that when <span class="math inline">\(\beta&gt;0\)</span>, we have <span class="math inline">\(\mathrm{Var}[N] &gt;\mathrm{E}[N]\)</span>. In other words, this distribution is <a href="#" class="tooltip" style="color:green"><em>overdispersed</em><span style="font-size:8pt">The presence of greater variability (statistical dispersion) in a data set than would be expected based on a given statistical model</span></a> (relative to the Poisson); similarly, when <span class="math inline">\(q&gt;0\)</span> the binomial distribution is said to be <a href="#" class="tooltip" style="color:green"><em>underdispersed</em><span style="font-size:8pt">There was less variation in the data than predicted</span></a> (relative to the Poisson).</p>
<p>Finally, we observe that the Poisson distribution also emerges as a limit of negative binomial distributions. Towards establishing this, let <span class="math inline">\(\beta_r\)</span> be such that as <span class="math inline">\(r\)</span> approaches infinity <span class="math inline">\(r\beta_r\)</span> approaches <span class="math inline">\(\lambda&gt;0\)</span>. Then we see that the mgfs of negative binomial distributions with parameters <span class="math inline">\((r,\beta_r)\)</span> satisfies
<span class="math display">\[
\lim_{r\rightarrow 0} (1-\beta_r(e^t-1))^{-r}=\exp\{\lambda(e^t-1)\},
\]</span>
with the right hand side of the above equation being the <em>mgf</em> of the Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<div id="surveyElement22">

</div>
<div id="surveyResult22">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz22.1" href="javascript:toggleQuiz
('display.Quiz22.2','display.Quiz22.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz22.2" style="display: none">
<p id="Quiz22Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz22.js">
</script>
</div>
</div>
</div>
<div id="S:the-a-b-0-class" class="section level2">
<h2><span class="header-section-number">2.3</span> The (<em>a</em>, <em>b</em>, 0) Class</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Define the (<em>a</em>,<em>b</em>,0) class of frequency distributions</li>
<li>Discuss the importance of the recursive relationship underpinning this class of distributions</li>
<li>Identify conditions under which this general class reduces to each of the binomial, Poisson, and negative binomial distributions</li>
</ul>
<hr />
<p>In the previous section we studied three distributions, namely the binomial, the Poisson and the negative binomial distributions. In the case of the Poisson, to derive its mean we used the the fact that
<span class="math display">\[
kp_k=\lambda p_{k-1}, \quad k\geq 1,
\]</span>
which can be expressed equivalently as
<span class="math display">\[
\frac{p_k}{p_{k-1}}=\frac{\lambda}{k}, \quad k\geq 1. 
\]</span>
Interestingly, we can similarly show that for the binomial distribution
<span class="math display">\[
\frac{p_k}{p_{k-1}}=\frac{-q}{1-q}+\left(\frac{(m+1)q}{1-q}\right)\frac{1}{k}, \quad k=1,\ldots,m, 
\]</span>
and that for the negative binomial distribution
<span class="math display">\[
\frac{p_k}{p_{k-1}}=\frac{\beta}{1+\beta}+\left(\frac{(r-1)\beta}{1+\beta}\right)\frac{1}{k}, \quad k\geq 1. 
\]</span>
The above relationships are all of the form</p>
<p><span class="math display" id="eq:ab0">\[\begin{equation}
\frac{p_k}{p_{k-1}}=a+\frac{b}{k}, \quad k\geq 1; 
\tag{2.1}
\end{equation}\]</span></p>
<p>this raises the question if there are any other distributions which satisfy this seemingly general recurrence relation. Note that the ratio on the left, the ratio of two probabilities, is non-negative.</p>
<h5 style="text-align: center;">
<a id="displayTheory.ab0.1" href="javascript:toggleTheory('toggleTheory.ab0.1','displayCode.ab0.1');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.ab0.1" style="display: none">
<hr />
<p><strong>Snippet of Theory.</strong> To begin with, let <span class="math inline">\(a&lt;0\)</span>. In this case as <span class="math inline">\(k\rightarrow \infty\)</span>, <span class="math inline">\((a+b/k)\rightarrow a&lt;0\)</span>. It follows that if <span class="math inline">\(a&lt;0\)</span> then <span class="math inline">\(b\)</span> should satisfy <span class="math inline">\(b=-ka\)</span>, for some <span class="math inline">\(k\geq 1\)</span>. Any such pair <span class="math inline">\((a,b)\)</span> can be written as
<span class="math display">\[
\left(\frac{-q}{1-q},\frac{(m+1)q}{1-q}\right), \quad q\in(0,1), m\geq 1;
\]</span>
note that the case <span class="math inline">\(a&lt;0\)</span> with <span class="math inline">\(a+b=0\)</span> yields the degenerate at <span class="math inline">\(0\)</span> distribution which is the binomial distribution with <span class="math inline">\(q=0\)</span> and arbitrary <span class="math inline">\(m\geq 1\)</span>.</p>
<p>In the case of <span class="math inline">\(a=0\)</span>, again by non-negativity of the ratio <span class="math inline">\(p_k/p_{k-1}\)</span>, we have <span class="math inline">\(b\geq 0\)</span>. If <span class="math inline">\(b=0\)</span> the distribution is degenerate at <span class="math inline">\(0\)</span>, which is a binomial with <span class="math inline">\(q=0\)</span> or a Poisson distribution with <span class="math inline">\(\lambda=0\)</span> or a negative binomial distribution with <span class="math inline">\(\beta=0\)</span>. If <span class="math inline">\(b&gt;0\)</span>, then clearly such a distribution is a Poisson distribution with mean (<em>i.e.</em> <span class="math inline">\(\lambda\)</span>) equal to <span class="math inline">\(b\)</span>, as presented at the beginning of this section.</p>
<p>In the case of <span class="math inline">\(a&gt;0\)</span>, again by non-negativity of the ratio <span class="math inline">\(p_k/p_{k-1}\)</span>, we have <span class="math inline">\(a+b/k\geq 0\)</span> for all <span class="math inline">\(k\geq 1\)</span>. The most stringent of these is the inequality <span class="math inline">\(a+b\geq 0\)</span>. Note that <span class="math inline">\(a+b=0\)</span> again results in degeneracy at <span class="math inline">\(0\)</span>; excluding this case we have <span class="math inline">\(a+b&gt;0\)</span> or equivalently <span class="math inline">\(b=(r-1)a\)</span> with <span class="math inline">\(r&gt;0\)</span>. Some algebra easily yields the following expression for <span class="math inline">\(p_k\)</span>:
<span class="math display">\[
p_k = {r+k-1 \choose k} p_0 a^k, \quad k=1,2,\ldots. 
\]</span>
The above series converges for <span class="math inline">\(a&lt;1\)</span> when <span class="math inline">\(r&gt;0\)</span>, with the sum given by <span class="math inline">\(p_0 \cdot ((1-a)^{(-r)}-1)\)</span>. Hence, equating the latter to <span class="math inline">\(1-p_0\)</span> we get <span class="math inline">\(p_0=(1-a)^{(r)}\)</span>. So in this case the pair <span class="math inline">\((a,b)\)</span> is of the form <span class="math inline">\((a,(r-1)a)\)</span>, for <span class="math inline">\(r&gt;0\)</span> and <span class="math inline">\(0&lt;a&lt;1\)</span>; since an equivalent parametrization is <span class="math inline">\((\beta/(1+\beta),(r-1)\beta/(1+\beta))\)</span>, for <span class="math inline">\(r&gt;0\)</span> and <span class="math inline">\(\beta&gt;0\)</span>, we see from above that such distributions are negative binomial distributions.</p>
<hr />
</div>
<p>From the above development we see that not only does the recurrence <a href="ChapFrequency-Modeling.html#eq:ab0">(2.1)</a> tie these three distributions together, but also it characterizes them. For this reason these three distributions are collectively referred to in the actuarial literature as <a href="#" class="tooltip" style="color:green"><em>(a,b,0) class</em><span style="font-size:8pt"></span></a> of distributions, with <span class="math inline">\(0\)</span> referring to the starting point of the recurrence. Note that the value of <span class="math inline">\(p_0\)</span> is implied by <span class="math inline">\((a,b)\)</span> since the probabilities have to sum to one. Of course, <a href="ChapFrequency-Modeling.html#eq:ab0">(2.1)</a> as a recurrence relation for <span class="math inline">\(p_k\)</span> makes the computation of the <em>pmf</em> efficient by removing redundancies. Later, we will see that it does so even in the case of compound distributions with the frequency distribution belonging to the <span class="math inline">\((a,b,0)\)</span> class - this fact is the more important motivating reason to study these three distributions from this viewpoint.</p>
<p><strong>Example 2.3.1.</strong>
A discrete probability distribution has the following properties
<span class="math display">\[
\begin{aligned}
p_k&amp;=c\left( 1+\frac{2}{k}\right) p_{k-1} \:\:\: k=1,2,3,\ldots\\
p_1&amp;= \frac{9}{256}
\end{aligned}
\]</span>
Determine the expected value of this discrete random variable.</p>
<h5 style="text-align: center;">
<a id="displayExample.2.3.1" href="javascript:toggleEX('toggleExample.2.3.1','displayExample.2.3.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.2.3.1" style="display: none">
<p><strong>Solution:</strong> Since the <em>pmf</em> satisfies the <span class="math inline">\((a,b,0)\)</span> recurrence relation we know that the underlying distribution is one among the binomial, Poisson, and negative binomial distributions. Since the ratio of the parameters (<em>i.e.</em> <span class="math inline">\(b/a\)</span>) equals <span class="math inline">\(2\)</span>, we know that it is negative binomial and that <span class="math inline">\(r=3\)</span>. Moreover, since for a negative binomial <span class="math inline">\(p_1=r(1+\beta)^{-(r+1)}\beta\)</span>, we have
<span class="math display">\[
\begin{aligned}
&amp;&amp;\frac{9}{256}=&amp;3\frac{\beta}{(1+\beta)^4}\\
\implies &amp;&amp;\frac{3}{(1+3)^4}=&amp;\frac{\beta}{(1+\beta)^4}\\
\implies &amp;&amp;\beta=&amp;3.
\end{aligned}
\]</span>
Finally, since the mean of a negative binomial is <span class="math inline">\(r\beta\)</span> we have the mean of the given distribution equals <span class="math inline">\(9\)</span>.</p>
</div>
<hr />
<div id="surveyElement23">

</div>
<div id="surveyResult23">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz23.1" href="javascript:toggleQuiz
('display.Quiz23.2','display.Quiz23.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz23.2" style="display: none">
<p id="Quiz23Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz23.js">
</script>
</div>
<div id="S:estimating-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.4</span> Estimating Frequency Distributions</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Define a likelihood for a sample of observations from a discrete distribution</li>
<li>Define the maximum likelihood estimator for a random sample of observations from a discrete distribution</li>
<li>Calculate the maximum likelihood estimator for the binomial, Poisson, and negative binomial distributions</li>
</ul>
<hr />
<div id="S:parameter-estimation" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Parameter Estimation</h3>
<p>In Section <a href="ChapFrequency-Modeling.html#S:basic-frequency-distributions">2.2</a> we introduced three distributions of importance in modeling various types of count data arising from insurance. Let us now suppose that we have a set of count data to which we wish to fit a distribution, and that we have determined that one of these <span class="math inline">\((a,b,0)\)</span> distributions is more appropriate than the others. Since each one of these forms a class of distributions if we allow its parameter(s) to take any permissible value, there remains the task of determining the <strong>best</strong> value of the parameter(s) for the data at hand. This is a statistical point estimation problem, and in parametric inference problems the statistical inference paradigm of <em>maximum likelihood</em> usually yields efficient estimators. In this section we describe this paradigm and derive the maximum likelihood estimators.</p>
<p>Let us suppose that we observe the independent and identically distributed, <em>iid</em>, random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> from a distribution with <em>pmf</em> <span class="math inline">\(p_\theta\)</span>, where <span class="math inline">\(\theta\)</span> is a vector of parameters and an unknown value in the parameter space <span class="math inline">\(\Theta\subseteq \mathbb{R}^d\)</span>. For example, in the case of the Poisson distribution, there is a single parameter so that <span class="math inline">\(d=1\)</span> and</p>
<p><span class="math display">\[
p_\theta(x)=e^{-\theta}\frac{\theta^x}{x!}, \quad x=0,1,\ldots,
\]</span></p>
<p>with <span class="math inline">\(\theta &gt; 0\)</span>. In the case of the binomial distribution we have</p>
<p><span class="math display">\[
p_\theta(x)= {m \choose x} q^x(1-q)^{m-x}, \quad x=0,1,\ldots,m.
\]</span></p>
<p>For some applications, we can view <span class="math inline">\(m\)</span> as a parameter and so take <span class="math inline">\(d=2\)</span> so that <span class="math inline">\(\theta=(m,q)\in \{0,1,2,\ldots\}\times[0,1]\)</span>.</p>
<p>Let us suppose that the observations are <span class="math inline">\(x_1,\ldots,x_n\)</span>, observed values of the random sample <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> presented earlier. In this case, the probability of observing this sample from <span class="math inline">\(p_\theta\)</span> equals
<span class="math display">\[
\prod_{i=1}^n p_\theta(x_i).
\]</span>
The above, denoted by <span class="math inline">\(L(\theta)\)</span>, viewed as a function of <span class="math inline">\(\theta\)</span>, is called the <em>likelihood</em>. Note that we suppressed its dependence on the data, to emphasize that we are viewing it as a function of the parameter vector. For example, in the case of the Poisson distribution we have
<span class="math display">\[
L(\lambda)=e^{-n\lambda} \lambda^{\sum_{i=1}^n x_i} \left(\prod_{i=1}^n x_i!\right)^{-1}.
\]</span>
In the case of the binomial distribution we have
<span class="math display">\[
L(m,q)=\left(\prod_{i=1}^n {m \choose x_i}\right) q^{\sum_{i=1}^n x_i} (1-q)^{nm-\sum_{i=1}^n x_i} .
\]</span>
The <a href="#" class="tooltip" style="color:green"><em>maximum likelihood estimator (mle)</em><span style="font-size:8pt">The possible value of the parameter for which the chance of observing the data largest</span></a> for <span class="math inline">\(\theta\)</span> is any maximizer of the likelihood; in a sense the <em>mle</em> chooses the set of parameter values that best explains the observed observations. Appendix Section <a href="CAppA.html#S:AppA:MLE">15.2.2</a> reviews the foundations of maximum likelihood estimation with more mathematical details in Appendix Chapter <a href="CAppC.html#CAppC">17</a>.</p>
<p><strong>Special Case: Three Bernoulli Outcomes.</strong> To illustrate, consider a sample of size <span class="math inline">\(n=3\)</span> from a Bernoulli distribution (binomial with <span class="math inline">\(m=1\)</span>) with values <span class="math inline">\(0,1,0\)</span>. The likelihood in this case is easily checked to equal
<span class="math display">\[
L(q)=q(1-q)^2,
\]</span>
and the plot of the likelihood is given in Figure <a href="ChapFrequency-Modeling.html#fig:berlik">2.1</a>. As shown in the plot, the maximum value of the likelihood equals <span class="math inline">\(4/27\)</span> and is attained at <span class="math inline">\(q=1/3\)</span>, and hence the maximum likelihood estimate for <span class="math inline">\(q\)</span> is <span class="math inline">\(1/3\)</span> for the given sample. In this case one can resort to algebra to show that
<span class="math display">\[
q(1-q)^2=\left(q-\frac{1}{3}\right)^2\left(q-\frac{4}{3}\right)+\frac{4}{27},
\]</span>
and conclude that the maximum equals <span class="math inline">\(4/27\)</span>, and is attained at <span class="math inline">\(q=1/3\)</span> (using the fact that the first term is non-positive in the interval <span class="math inline">\([0,1]\)</span>).</p>
<p>But as is apparent, this way of deriving the <em>mle</em> using algebra does not generalize. In general, one resorts to calculus to derive the <em>mle</em> - note that for some likelihoods one may have to resort to other optimization methods, especially when the likelihood has many <a href="#" class="tooltip" style="color:green"><em>local extrema</em><span style="font-size:8pt">The largest and smallest value of the function within a given range</span></a>. It is customary to equivalently maximize the logarithm of the likelihood<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> <span class="math inline">\(L(\cdot)\)</span>, denoted by <span class="math inline">\(l(\cdot)\)</span>, and look at the set of zeros of its first derivative<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> <span class="math inline">\(l&#39;(\cdot)\)</span>. In the case of the above likelihood, <span class="math inline">\(l(q)=\log(q)+2\log(1-q)\)</span>, and
<span class="math display">\[
l&#39;(q)=\frac{\rm d}{{\rm d}q}l(q)=\frac{1}{q}-\frac{2}{1-q}.
\]</span>
The unique zero of <span class="math inline">\(l&#39;(\cdot)\)</span> equals <span class="math inline">\(1/3\)</span>, and since <span class="math inline">\(l&#39;&#39;(\cdot)\)</span> is negative, we have <span class="math inline">\(1/3\)</span> is the unique maximizer of the likelihood and hence its maximum likelihood estimate.</p>

<div class="figure" style="text-align: center"><span id="fig:berlik"></span>
<img src="Figures/figure2.1.png" alt="Likelihood of a \((0,1,0)\) \(3\)-sample from Bernoulli" width="45%" />
<p class="caption">
Figure 2.1: <strong>Likelihood of a <span class="math inline">\((0,1,0)\)</span> <span class="math inline">\(3\)</span>-sample from Bernoulli</strong>
</p>
</div>
</div>
<div id="S:frequency-distributions-mle" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Frequency Distributions MLE</h3>
<p>In the following, we derive the maximum likelihood estimator, <em>mle</em>, for the three members of the <span class="math inline">\((a,b,0)\)</span> class. We begin by summarizing the discussion above. In the setting of observing <em>iid</em>, independent and identically distributed, random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> from a distribution with <em>pmf</em> <span class="math inline">\(p_\theta\)</span>, where <span class="math inline">\(\theta\)</span> takes an unknown value in <span class="math inline">\(\Theta\subseteq \mathbb{R}^d\)</span>, the likelihood <span class="math inline">\(L(\cdot)\)</span>, a function on <span class="math inline">\(\Theta\)</span> is defined as
<span class="math display">\[
L(\theta)=\prod_{i=1}^n p_\theta(x_i),
\]</span>
where <span class="math inline">\(x_1,\ldots,x_n\)</span> are the observed values. The <em>mle</em> of <span class="math inline">\(\theta\)</span>, denoted as <span class="math inline">\(\hat{\theta}_{\rm MLE}\)</span>, is a function which maps the observations to an element of the set of maximizers of <span class="math inline">\(L(\cdot)\)</span>, namely
<span class="math display">\[
\{\theta \vert L(\theta)=\max_{\eta\in\Theta}L(\eta)\}.
\]</span>
Note the above set is a function of the observations, even though this dependence is not made explicit. In the case of the three distributions that we study, and quite generally, the above set is a singleton with probability tending to one (with increasing sample size). In other words, for many commonly used distributions and when the sample size is large, the likelihood estimate is uniquely defined with high probability.</p>
<p>In the following, we assume that we have observed <span class="math inline">\(n\)</span> <em>iid</em> random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> from the distribution under consideration, even though the parametric value is unknown. Also, <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> will denote the observed values. We note that in the case of count data, and data from discrete distributions in general, the likelihood can alternately be represented as
<span class="math display">\[
L(\theta)=\prod_{k\geq 0} \left(p_\theta(k)\right)^{m_k},
\]</span>
where <span class="math inline">\(m_k\)</span> is the number of observations equal to <span class="math inline">\(k\)</span>. Mathematically, we have
<span class="math display">\[
m_k= \left\vert \{i\vert x_i=k, 1\leq i \leq n\} \right\vert=\sum_{i= 1}^n I(x_i=k), \quad k\geq 0.
\]</span>
Note that this transformation retains all of the data, compiling it in a streamlined manner. For large <span class="math inline">\(n\)</span> it leads to compression of the data in the sense of <em>sufficiency</em>. Below, we present expressions for the <em>mle</em> in terms of <span class="math inline">\(\{m_k\}_{k\geq 1}\)</span> as well.</p>
<p><strong>Special Case: Poisson Distribution.</strong> In this case, as noted above, the likelihood is given by
<span class="math display">\[
L(\lambda)=\left(\prod_{i=1}^n x_i!\right)^{-1}e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i} .
\]</span>
Taking logarithms, the log-likelihood is
<span class="math display">\[
l(\lambda)= -\sum_{i=1}^n \log(x_i!) -n\lambda +\log(\lambda) \cdot \sum_{i=1}^n x_i .
\]</span>
Taking a derivative, we have
<span class="math display">\[
l&#39;(\lambda)= -n +\frac{1}{\lambda}\sum_{i=1}^n x_i.
\]</span>
In evaluating <span class="math inline">\(l&#39;&#39;(\lambda)\)</span>, when <span class="math inline">\(\sum_{i=1}^n x_i&gt;0\)</span>, <span class="math inline">\(l&#39;&#39;&lt; 0\)</span>. Consequently, the maximum is attained at the sample mean, <span class="math inline">\(\overline{x}\)</span>, presented below. When <span class="math inline">\(\sum_{i=1}^n x_i=0\)</span>, the likelihood is a decreasing function and hence the maximum is attained at the least possible parameter value; this results in the maximum likelihood estimate being zero. Hence, we have<br />
<span class="math display">\[
\overline{x} = \hat{\lambda}_{\rm MLE} = \frac{1}{n}\sum_{i=1}^n x_i. 
\]</span>
Note that the sample mean can be computed also as
<span class="math display">\[
\overline{x} = \frac{1}{n} \sum_{k\geq 1} k \cdot m_k ~.
\]</span>
It is noteworthy that in the case of the Poisson, the exact distribution of <span class="math inline">\(\hat{\lambda}_{\rm MLE}\)</span> is available in closed form - it is a scaled Poisson - when the underlying distribution is a Poisson. This is so as the sum of independent Poisson random variables is a Poisson as well. Of course, for large sample size one can use the ordinary <a href="#" class="tooltip" style="color:green"><em>Central Limit Theorem (CLT)</em><span style="font-size:8pt">In some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed.</span></a> to derive a normal approximation. Note that the latter approximation holds even if the underlying distribution is any distribution with a finite second moment.</p>
<p><strong>Special Case: Binomial Distribution.</strong> Unlike the case of the Poisson distribution, the parameter space in the case of the binomial is <span class="math inline">\(2\)</span>-dimensional. Hence the optimization problem is a bit more challenging. We begin by observing that the likelihood is given by
<span class="math display">\[
L(m,q)= \left(\prod_{i=1}^n {m \choose x_i}\right) q^{\sum_{i=1}^n x_i} (1-q)^{nm-\sum_{i=1}^n x_i} .
\]</span>
Taking logarithms, the log-likelihood is</p>
<p><span class="math display">\[
\begin{array}{ll}
l(m,q) &amp;= \sum_{i=1}^n \log\left({m \choose x_i}\right) + \left({\sum_{i=1}^n x_i}\right)\log(q) \\
&amp; \ \ \ + \left({nm-\sum_{i=1}^n x_i}\right)\log(1-q) \\
&amp;= \sum_{i=1}^n \log\left({m \choose x_i}\right) + n \overline{x}\log(q) + n\left({m- \overline{x}}\right)\log(1-q) ,
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\overline{x} = n^{-1} \sum_{i=1}^n x_i\)</span>. Note that since <span class="math inline">\(m\)</span> takes only non-negative integer values, we cannot use multivariate calculus to find the optimal values. Nevertheless, we can use single variable calculus to show that</p>
<p><span class="math display" id="eq:binmle">\[\begin{equation}
\hat{q}_{MLE}\times \hat{m}_{MLE} = \overline{x}.  
\tag{2.2}
\end{equation}\]</span></p>
<h5 style="text-align: center;">
<a id="displayTheory.Binomial.1" href="javascript:toggleTheory('toggleTheory.Binomial.1','displayCode.Binomial.1');"><i><strong>Show A Verification of Equation (2.2)</strong></i></a>
</h5>
<div id="toggleTheory.Binomial.1" style="display: none">
<hr />
<p>Towards this we note that for a fixed value of <span class="math inline">\(m\)</span>,
<span class="math display">\[
\frac{\delta}{\delta q} l(m,q) = \frac{n \overline{x}}{q}- \frac{n\left({m- \overline{x}}\right)}{1-q},
\]</span>
and that
<span class="math display">\[
\frac{\delta^2}{\delta q^2} l(m,q) 
= -\frac{n \overline{x}}{q^2}+ \frac{n\left({m- \overline{x}}\right)}{(1-q)^2} \le 0.
\]</span>
The above implies that for any fixed value of <span class="math inline">\(m\)</span>, the maximizing value of <span class="math inline">\(q\)</span> satisfies
<span class="math display">\[
mq=\overline{x},
\]</span>
and hence we establish equation <a href="ChapFrequency-Modeling.html#eq:binmle">(2.2)</a>.</p>
<hr />
</div>
<p>With equation <a href="ChapFrequency-Modeling.html#eq:binmle">(2.2)</a>, the above reduces the task to the search for <span class="math inline">\(\hat{m}_{\rm MLE}\)</span>, which is a maximizer of</p>
<p><span class="math display" id="eq:binlikm">\[\begin{equation}
L\left(m,\frac{\overline{x}}{m} \right).
\tag{2.3}
\end{equation}\]</span></p>
<p>Note the likelihood would be zero for values of <span class="math inline">\(m\)</span> smaller than <span class="math inline">\(\max\limits_{1\leq i \leq n}x_i\)</span>, and hence <span class="math inline">\(\hat{m}_{\rm MLE}\geq \max_{1\leq i \leq n}x_i\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Binomial.2" href="javascript:toggleTheory('toggleTheory.Binomial.2','displayCode.Binomial.2');"><i><strong>Technical Note on the Poisson Approximation to the Binomial</strong></i></a>
</h5>
<div id="toggleTheory.Binomial.2" style="display: none">
<hr />
<p>Towards specifying an algorithm to compute <span class="math inline">\(\hat{m}_{\rm MLE}\)</span>, we first point out that for some data sets <span class="math inline">\(\hat{m}_{\rm MLE}\)</span> could equal <span class="math inline">\(\infty\)</span>, indicating that a Poisson distribution would render a better fit than any binomial distribution. This is so as the binomial distribution with parameters <span class="math inline">\((m,\overline{x}/m)\)</span> approaches the Poisson distribution with parameter <span class="math inline">\(\overline{x}\)</span> with <span class="math inline">\(m\)</span> approaching infinity. The fact that some data sets <strong>prefer</strong> a Poisson distribution should not be surprising since in the above sense the set of Poisson distribution is on the boundary of the set of binomial distributions. Interestingly, in <span class="citation">Olkin, Petkau, and Zidek (<a href="#ref-olkin1981" role="doc-biblioref">1981</a>)</span> they show that if the sample mean is less than or equal to the sample variance then <span class="math inline">\(\hat{m}_{\rm MLE}=\infty\)</span>; otherwise, there exists a finite <span class="math inline">\(m\)</span> that maximizes equation <a href="ChapFrequency-Modeling.html#eq:binlikm">(2.3)</a>.</p>
<hr />
</div>
<p>In Figure <a href="ChapFrequency-Modeling.html#fig:MLEm">2.2</a> below we display the plot of <span class="math inline">\(L\left(m,\overline{x}/m\right)\)</span> for three different samples of size <span class="math inline">\(5\)</span>; they differ only in the value of the sample maximum. The first sample of <span class="math inline">\((2,2,2,4,5)\)</span> has the ratio of sample mean to sample variance greater than <span class="math inline">\(1\)</span> (<span class="math inline">\(1.875\)</span>), the second sample of <span class="math inline">\((2,2,2,4,6)\)</span> has the ratio equal to <span class="math inline">\(1.25\)</span> which is closer to <span class="math inline">\(1\)</span>, and the third sample of <span class="math inline">\((2,2,2,4,7)\)</span> has the ratio less than <span class="math inline">\(1\)</span> (<span class="math inline">\(0.885\)</span>). For these three samples, as shown in Figure <a href="ChapFrequency-Modeling.html#fig:MLEm">2.2</a>, <span class="math inline">\(\hat{m}_{\rm MLE}\)</span> equals <span class="math inline">\(7\)</span>, <span class="math inline">\(18\)</span> and <span class="math inline">\(\infty\)</span>, respectively. Note that the limiting value of <span class="math inline">\(L\left(m,\overline{x}/m\right)\)</span> as <span class="math inline">\(m\)</span> approaches infinity equals</p>
<p><span class="math display" id="eq:Poilik">\[\begin{equation}
\left(\prod_{i=1}^n x_i! \right)^{-1} \exp\left(-n \overline{x}~\right) \left(~\overline{x}~\right)^{n\overline{x}}. 
\tag{2.4}
\end{equation}\]</span></p>
<p>Also, note that Figure <a href="ChapFrequency-Modeling.html#fig:MLEm">2.2</a> shows that the <em>mle</em> of <span class="math inline">\(m\)</span> is non-<a href="#" class="tooltip" style="color:green"><em>robust</em><span style="font-size:8pt">Resistant to errors in the results, produced by deviations from assumptions</span></a>, <em>i.e.</em> changes in a small proportion of the data set can cause large changes in the estimator.</p>
<p>The above discussion suggests the following simple algorithm:</p>
<ul>
<li><em>Step 1</em>. If the sample mean is less than or equal to the sample variance, then set <span class="math inline">\(\hat{m}_{MLE}=\infty\)</span>. The <em>mle</em> suggested distribution is a Poisson distribution with <span class="math inline">\(\hat{\lambda}=\overline{x}\)</span>.</li>
<li><em>Step 2</em>. If the sample mean is greater than the sample variance, then compute <span class="math inline">\(L(m,\overline{x}/m)\)</span> for <span class="math inline">\(m\)</span> values greater than or equal to the sample maximum until <span class="math inline">\(L(m,\overline{x}/m)\)</span> is close to the value of the Poisson likelihood given in <a href="ChapFrequency-Modeling.html#eq:Poilik">(2.4)</a>. The value of <span class="math inline">\(m\)</span> that corresponds to the maximum value of <span class="math inline">\(L(m,\overline{x}/m)\)</span> among those computed equals <span class="math inline">\(\hat{m}_{MLE}\)</span>.</li>
</ul>
<p>We note that if the underlying distribution is the binomial distribution with parameters <span class="math inline">\((m,q)\)</span> (with <span class="math inline">\(q&gt;0\)</span>) then <span class="math inline">\(\hat{m}_{MLE}\)</span> equals <span class="math inline">\(m\)</span> for large sample sizes. Also, <span class="math inline">\(\hat{q}_{MLE}\)</span> will have an asymptotically normal distribution and converge with probability one to <span class="math inline">\(q\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:MLEm"></span>
<img src="Figures/figure2.2.png" alt="Plot of \(L(m,\bar{x}/m)\) for a Binomial Distribution" width="80%" />
<p class="caption">
Figure 2.2: <strong>Plot of <span class="math inline">\(L(m,\bar{x}/m)\)</span> for a Binomial Distribution</strong>
</p>
</div>
<p><strong>Special Case: Negative Binomial Distribution.</strong> The case of the negative binomial distribution is similar to that of the binomial distribution in the sense that we have two parameters and the <em>mle</em>s are not available in closed form. A difference between them is that unlike the binomial parameter <span class="math inline">\(m\)</span> which takes positive integer values, the parameter <span class="math inline">\(r\)</span> of the negative binomial can assume any positive real value. This makes the optimization problem a tad more complex. We begin by observing that the likelihood can be expressed in the following form:
<span class="math display">\[
L(r,\beta)=\left(\prod_{i=1}^n {r+x_i-1 \choose x_i}{r+x_i-1 \choose x_i}\right) (1+\beta)^{-n(r+\overline{x})} \beta^{n\overline{x}}.  
\]</span>
The above implies that log-likelihood is given by
<span class="math display">\[
l(r,\beta)=\sum_{i=1}^n \log{r+x_i-1 \choose x_i} -n(r+\overline{x}) \log(1+\beta) +n\overline{x}\log\beta,
\]</span>
and hence
<span class="math display">\[
\frac{\delta}{\delta\beta} l(r,\beta) = -\frac{n(r+\overline{x})}{1+\beta} + \frac{n\overline{x}}{\beta}.
\]</span>
Equating the above to zero, we get
<span class="math display">\[
\hat{r}_{MLE}\times \hat{\beta}_{MLE} = \overline{x}.
\]</span>
The above reduces the two dimensional optimization problem to a one-dimensional problem - we need to maximize
<span class="math display">\[
l(r,\overline{x}/r)=\sum_{i=1}^n \log{r+x_i-1 \choose x_i} -n(r+\overline{x}) \log(1+\overline{x}/r) +n\overline{x}\log(\overline{x}/r),
\]</span>
with respect to <span class="math inline">\(r\)</span>, with the maximizing <span class="math inline">\(r\)</span> being its <em>mle</em> and <span class="math inline">\(\hat{\beta}_{MLE}=\overline{x}/\hat{r}_{MLE}\)</span>. In <span class="citation">Levin, Reeds, and others (<a href="#ref-levin1977" role="doc-biblioref">1977</a>)</span> it is shown that if the sample variance is greater than the sample mean then there exists a unique <span class="math inline">\(r&gt;0\)</span> that maximizes <span class="math inline">\(l(r,\overline{x}/r)\)</span> and hence a unique
<em>mle</em> for <span class="math inline">\(r\)</span> and <span class="math inline">\(\beta\)</span>. Also, they show that if <span class="math inline">\(\hat{\sigma}^2\leq \overline{x}\)</span>, then the negative binomial likelihood will be dominated by the Poisson likelihood with <span class="math inline">\(\hat{\lambda}=\overline{x}\)</span>. In other words, a Poisson distribution offers a better fit to the data. The guarantee in the case of <span class="math inline">\(\hat{\sigma}^2&gt;\hat{\mu}\)</span> permits us to use some algorithm to maximize <span class="math inline">\(l(r,\overline{x}/r)\)</span>. Towards an alternate method of computing the likelihood, we note that</p>
<p><span class="math display">\[
\begin{array}{ll}
l(r,\overline{x}/r)&amp;=\sum_{i=1}^n \sum_{j=1}^{x_i}\log(r-1+j) - \sum_{i=1}^n\log(x_i!) \\
&amp; \ \ \ - n(r+\overline{x}) \log(r+\overline{x}) + nr\log(r) + n\overline{x}\log(\overline{x}),
\end{array}
\]</span></p>
<p>which yields
<span class="math display">\[
\left(\frac{1}{n}\right)\frac{\delta}{\delta r}l(r,\overline{x}/r)=\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{r-1+j} - \log(r+\overline{x}) + \log(r).
\]</span>
We note that, in the above expressions for the terms involving a double summation, the inner sum equals zero if <span class="math inline">\(x_i=0\)</span>. The <em>maximum likelihood estimate</em> for <span class="math inline">\(r\)</span> is a root of the last expression and we can use a root finding algorithm to compute it. Also, we have
<span class="math display">\[
\left(\frac{1}{n}\right)\frac{\delta^2}{\delta r^2}l(r,\overline{x}/r)=\frac{\overline{x}}{r(r+\overline{x})}-\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{(r-1+j)^2}.
\]</span>
A simple but quickly converging iterative root finding algorithm is the <a href="#" class="tooltip" style="color:green"><em>Newton’s method</em><span style="font-size:8pt">A root-finding algorithm which produces successively better approximations to the roots of a real-valued function</span></a>, which incidentally the Babylonians are believed to have used for computing square roots. Under this method, an initial approximation is selected for the root and new approximations for the root are successively generated until convergence. Applying the Newton’s method to our problem results in the following algorithm:<br />
<!-- \begin{enumerate}[leftmargin=0.5in,label=Step \roman*] -->
<em>Step i</em>. Choose an approximate solution, say <span class="math inline">\(r_0\)</span>. Set <span class="math inline">\(k\)</span> to <span class="math inline">\(0\)</span>.<br />
<em>Step ii</em>. Define <span class="math inline">\(r_{k+1}\)</span> as
<span class="math display">\[
r_{k+1}= r_k - \frac{\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{r_k-1+j} - \log(r_k+\overline{x}) + \log(r_k)}{\frac{\overline{x}}{r_k(r_k+\overline{x})}-\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{(r_k-1+j)^2}}
\]</span><br />
<em>Step iii</em>. If <span class="math inline">\(r_{k+1}\sim r_k\)</span>, then report <span class="math inline">\(r_{k+1}\)</span> as <em>maximum likelihood estimate</em>; else increment <span class="math inline">\(k\)</span> by <span class="math inline">\(1\)</span> and repeat <em>Step ii</em>.</p>
<p>For example, we simulated a <span class="math inline">\(5\)</span> observation sample of <span class="math inline">\(41, 49, 40, 27, 23\)</span> from the negative binomial with parameters <span class="math inline">\(r=10\)</span> and <span class="math inline">\(\beta=5\)</span>. Choosing the starting value of <span class="math inline">\(r\)</span> such that
<span class="math display">\[
r\beta=\hat{\mu} \quad \hbox{and} \quad r\beta(1+\beta)=\hat{\sigma}^2
\]</span>
where <span class="math inline">\(\hat{\mu}\)</span> represents the estimated mean and <span class="math inline">\(\hat{\sigma}^2\)</span> is the estimated variance. This leads to the starting value for <span class="math inline">\(r\)</span> of <span class="math inline">\(23.14286\)</span>. The iterates of <span class="math inline">\(r\)</span> from the Newton’s method are
<span class="math display">\[
21.39627, 21.60287, 21.60647, 21.60647;
\]</span>
the rapid convergence seen above is typical of the Newton’s method. Hence in this example, <span class="math inline">\(\hat{r}_{MLE} \sim 21.60647\)</span> and <span class="math inline">\(\hat{\beta}_{MLE} = 1.66616\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayCode.Freq.1" href="javascript:togglecode('toggleCode.Freq.1','displayCode.Freq.1');"><i><strong>Show R Implementation of Newtons Method - Negative Binomial MLE for r</strong></i></a>
</h5>
<div id="toggleCode.Freq.1" style="display: none">
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="ChapFrequency-Modeling.html#cb8-1"></a>Newton&lt;-<span class="cf">function</span>(x,abserr){</span>
<span id="cb8-2"><a href="ChapFrequency-Modeling.html#cb8-2"></a>mu&lt;-<span class="kw">mean</span>(x);</span>
<span id="cb8-3"><a href="ChapFrequency-Modeling.html#cb8-3"></a>sigma2&lt;-<span class="kw">mean</span>(x<span class="op">^</span><span class="dv">2</span>)<span class="op">-</span>mu<span class="op">^</span><span class="dv">2</span>;</span>
<span id="cb8-4"><a href="ChapFrequency-Modeling.html#cb8-4"></a>r&lt;-mu<span class="op">^</span><span class="dv">2</span><span class="op">/</span>(sigma2<span class="op">-</span>mu);</span>
<span id="cb8-5"><a href="ChapFrequency-Modeling.html#cb8-5"></a>b&lt;-<span class="ot">TRUE</span>;</span>
<span id="cb8-6"><a href="ChapFrequency-Modeling.html#cb8-6"></a>iter&lt;-<span class="dv">0</span>;</span>
<span id="cb8-7"><a href="ChapFrequency-Modeling.html#cb8-7"></a><span class="cf">while</span> (b) {</span>
<span id="cb8-8"><a href="ChapFrequency-Modeling.html#cb8-8"></a>tr&lt;-r;</span>
<span id="cb8-9"><a href="ChapFrequency-Modeling.html#cb8-9"></a>m1&lt;-<span class="kw">mean</span>(<span class="kw">c</span>(x[x<span class="op">==</span><span class="dv">0</span>],<span class="kw">sapply</span>(x[x<span class="op">&gt;</span><span class="dv">0</span>],<span class="cf">function</span>(z){<span class="kw">sum</span>(<span class="dv">1</span><span class="op">/</span>(tr<span class="op">:</span>(tr<span class="dv">-1</span><span class="op">+</span>z)))})));</span>
<span id="cb8-10"><a href="ChapFrequency-Modeling.html#cb8-10"></a>m2&lt;-<span class="kw">mean</span>(<span class="kw">c</span>(x[x<span class="op">==</span><span class="dv">0</span>],<span class="kw">sapply</span>(x[x<span class="op">&gt;</span><span class="dv">0</span>],<span class="cf">function</span>(z){<span class="kw">sum</span>(<span class="dv">1</span><span class="op">/</span>(tr<span class="op">:</span>(tr<span class="dv">-1</span><span class="op">+</span>z))<span class="op">^</span><span class="dv">2</span>)})));</span>
<span id="cb8-11"><a href="ChapFrequency-Modeling.html#cb8-11"></a>r&lt;-tr<span class="op">-</span>(m1<span class="op">-</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">+</span>mu<span class="op">/</span>tr))<span class="op">/</span>(mu<span class="op">/</span>(tr<span class="op">*</span>(tr<span class="op">+</span>mu))<span class="op">-</span>m2);</span>
<span id="cb8-12"><a href="ChapFrequency-Modeling.html#cb8-12"></a>b&lt;-<span class="op">!</span>(<span class="kw">abs</span>(tr<span class="op">-</span>r)<span class="op">&lt;</span>abserr);</span>
<span id="cb8-13"><a href="ChapFrequency-Modeling.html#cb8-13"></a>iter&lt;-iter<span class="op">+</span><span class="dv">1</span>;</span>
<span id="cb8-14"><a href="ChapFrequency-Modeling.html#cb8-14"></a>}</span>
<span id="cb8-15"><a href="ChapFrequency-Modeling.html#cb8-15"></a><span class="kw">c</span>(r,iter)</span>
<span id="cb8-16"><a href="ChapFrequency-Modeling.html#cb8-16"></a>}</span></code></pre></div>
</div>
<hr />
<p>To summarize our discussion of <a href="#" class="tooltip" style="color:green"><em>MLE</em><span style="font-size:8pt">Maximum likelihood estimate</span></a> for the <span class="math inline">\((a,b,0)\)</span> class of distributions, in Figure <a href="ChapFrequency-Modeling.html#fig:MLEab0">2.3</a> below we plot the maximum value of the Poisson likelihood, <span class="math inline">\(L(m,\overline{x}/m)\)</span> for the binomial, and <span class="math inline">\(L(r,\overline{x}/r)\)</span> for the negative binomial, for the three samples of size <span class="math inline">\(5\)</span> given in <a href="#tab:2.1">Table 2.1</a>. The data was constructed to cover the three orderings of the sample mean and variance. As shown in the Figure <a href="ChapFrequency-Modeling.html#fig:MLEab0">2.3</a>, and supported by theory, if <span class="math inline">\(\hat{\mu}&lt;\hat{\sigma}^2\)</span> then the negative binomial results in a higher maximum likelihood value; if <span class="math inline">\(\hat{\mu}=\hat{\sigma}^2\)</span> the Poisson has the highest likelihood value; and finally in the case that <span class="math inline">\(\hat{\mu}&gt;\hat{\sigma}^2\)</span> the binomial gives a better fit than the others. So before fitting a frequency data with an <span class="math inline">\((a,b,0)\)</span> distribution, it is best to start with examining the ordering of <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span>. We again emphasize that the Poisson is on the <strong>boundary</strong> of the negative binomial and binomial distributions. So in the case that <span class="math inline">\(\hat{\mu}\geq\hat{\sigma}^2\)</span> (<span class="math inline">\(\hat{\mu}\leq\hat{\sigma}^2\)</span>, resp.) the Poisson yields a better fit than the negative binomial
(binomial, resp.), which is indicated by <span class="math inline">\(\hat{r}=\infty\)</span> (<span class="math inline">\(\hat{m}=\infty\)</span>, respectively).</p>
<div style="page-break-after: always;"></div>
<p><a id=tab:2.1></a></p>
<p>Table 2.1. <strong>Three Samples of Size 5</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{c|c|c}
\hline
\text{Data} &amp; \text{Mean }(\hat{\mu}) &amp; \text{Variance }(\hat{\sigma}^2) \\
\hline
(2,3,6,8,9) &amp; 5.60 &amp; 7.44 \\ 
(2,5,6,8,9) &amp; 6 &amp; 6\\
(4,7,8,10,11) &amp; 8 &amp; 6\\\hline
\end{array}
}
\]</span></p>

<div class="figure" style="text-align: center"><span id="fig:MLEab0"></span>
<img src="Figures/figure2.3.png" alt="Plot of \((a,b,0)\) Partially Maximized Likelihoods" width="80%" />
<p class="caption">
Figure 2.3: <strong>Plot of</strong> <span class="math inline">\((a,b,0)\)</span> <strong>Partially Maximized Likelihoods</strong>
</p>
</div>
<div id="surveyElement24">

</div>
<div id="surveyResult24">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz24.1" href="javascript:toggleQuiz
('display.Quiz24.2','display.Quiz24.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz24.2" style="display: none">
<p id="Quiz24Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz24.js">
</script>
</div>
</div>
<div id="S:other-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.5</span> Other Frequency Distributions</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Define the (a,b,1) class of frequency distributions and discuss the importance of the recursive relationship underpinning this class of distributions</li>
<li>Interpret zero truncated and modified versions of the binomial, Poisson, and negative binomial distributions</li>
<li>Compute probabilities using the recursive relationship</li>
</ul>
<hr />
<p>In the previous sections, we discussed three distributions with supports contained in the set of non-negative integers, which well cater to many insurance applications. Moreover, typically by allowing the parameters to be a function of known (to the insurer) <a href="#" class="tooltip" style="color:green"><em>explanatory variables</em><span style="font-size:8pt">In regression, the explanatory variable is the one that is supposed to “explain” the other.</span></a> such as age, sex, geographic location (territory), and so forth, these distributions allow us to explain claim probabilities in terms of these variables. The field of statistical study that studies such models is known as <a href="#" class="tooltip" style="color:green"><em>regression analysis</em><span style="font-size:8pt">A set of statistical processes for estimating the relationships among variables</span></a> - it is an important topic of actuarial interest that we will not pursue in this book; see <span class="citation">Frees (<a href="#ref-frees2009regression" role="doc-biblioref">2009</a>)</span>.</p>
<p>There are clearly infinitely many other count distributions, and more importantly the above distributions by themselves do not cater to all practical needs. In particular, one feature of some insurance data is that the proportion of zero counts can be out of place with the proportion of other counts to be explainable by the above distributions. In the following we modify the above distributions to allow for arbitrary probability for zero count irrespective of the assignment of relative probabilities for the other counts. Another feature of a data set which is naturally comprised of <a href="#" class="tooltip" style="color:green"><em>homogeneous</em><span style="font-size:8pt">Units of exposure that face approximately the same expected frequency and severity of loss.</span></a> subsets is that while the above distributions may provide good fits to each subset, they may fail to do so to the whole data set. Later we naturally extend the <span class="math inline">\((a,b,0)\)</span> distributions to be able to cater to, in particular, such data sets.</p>
<div id="S:zero-truncation-or-modification" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Zero Truncation or Modification</h3>
<p>Let us suppose that we are looking at auto insurance policies which appear in a database of auto claims made in a certain period. If one is to study the number of claims that these policies have made during this period, then clearly the distribution has to assign a probability of zero to the count variable assuming the value zero. In other words, by restricting attention to count data from policies in the database of claims, we have in a sense zero-truncated the count data of all policies. In personal lines (like auto), policyholders may not want to report that first claim because of fear that it may increase future insurance rates - this behavior inflates the proportion of zero counts. Examples such as the latter modify the proportion of zero counts. Interestingly, natural modifications of the three distributions considered above are able to provide good fits to zero-modified/truncated data sets arising in insurance.</p>
<p>As presented below, we modify the probability assigned to zero count by the <span class="math inline">\((a,b,0)\)</span> class while maintaining the relative probabilities assigned to non-zero counts - zero modification. Note that since the <span class="math inline">\((a,b,0)\)</span> class of distributions satisfies the recurrence <a href="ChapFrequency-Modeling.html#eq:ab0">(2.1)</a>, maintaining relative probabilities of non-zero counts implies that recurrence <a href="ChapFrequency-Modeling.html#eq:ab0">(2.1)</a> is satisfied for <span class="math inline">\(k\geq 2\)</span>. This leads to the definition of the following class of distributions.</p>
<p><strong>Definition</strong>. A count distribution is a member of the <span class="math inline">\((a, b, 1)\)</span> class if for some constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> the probabilities <span class="math inline">\(p_k\)</span> satisfy</p>
<p><span class="math display" id="eq:ab1">\[\begin{equation}
\frac{p_k}{p_{k-1}}=a+\frac{b}{k},\quad k\geq 2.
\tag{2.5}
\end{equation}\]</span></p>
<p>Note that since the recursion starts with <span class="math inline">\(p_1\)</span>, and not <span class="math inline">\(p_0\)</span>, we refer to this super-class of <span class="math inline">\((a,b,0)\)</span> distributions by <a href="#" class="tooltip" style="color:green"><em>(a,b,1)</em><span style="font-size:8pt">A count distribution with probabilities satisfying p_k/p_{k-1}=a+b/k, for some some constants a and b and k&gt;=2</span></a>. To understand this class, recall that each valid pair of values for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> of the <span class="math inline">\((a,b,0)\)</span> class corresponds to a unique vector of probabilities <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span>. If we now look at the probability vector <span class="math inline">\(\{\tilde{p}_k\}_{k\geq 0}\)</span> given by
<span class="math display">\[
\tilde{p}_k= \frac{1-\tilde{p}_0}{1-p_0}\cdot p_k, \quad k\geq 1,
\]</span>
where <span class="math inline">\(\tilde{p}_0\in[0,1)\)</span> is arbitrarily chosen, then since the relative probabilities for positive values according to <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span> and <span class="math inline">\(\{\tilde{p}_k\}_{k\geq 0}\)</span> are the same, we have <span class="math inline">\(\{\tilde{p}_k\}_{k\geq 0}\)</span> satisfies recurrence <a href="ChapFrequency-Modeling.html#eq:ab1">(2.5)</a>. This, in particular, shows that the class of <span class="math inline">\((a,b,1)\)</span> distributions is strictly wider than that of <span class="math inline">\((a,b,0)\)</span>.</p>
<p>In the above, we started with a pair of values for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> that led to a valid <span class="math inline">\((a,b,0)\)</span> distribution, and then looked at the <span class="math inline">\((a,b,1)\)</span> distributions that corresponded to this <span class="math inline">\((a,b,0)\)</span> distribution. We now argue that the <span class="math inline">\((a,b,1)\)</span> class allows for a larger set of permissible distributions for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> than the <span class="math inline">\((a,b,0)\)</span> class. Recall from Section <a href="ChapFrequency-Modeling.html#S:the-a-b-0-class">2.3</a> that in the case of <span class="math inline">\(a&lt;0\)</span> we did not use the fact that the recurrence <a href="ChapFrequency-Modeling.html#eq:ab0">(2.1)</a> started at <span class="math inline">\(k=1\)</span>, and hence the set of pairs <span class="math inline">\((a,b)\)</span> with <span class="math inline">\(a&lt;0\)</span> that are permissible for the <span class="math inline">\((a,b,0)\)</span> class is identical to those that are permissible for the <span class="math inline">\((a,b,1)\)</span> class. The same conclusion is easily drawn for pairs with <span class="math inline">\(a=0\)</span>. In the case that <span class="math inline">\(a&gt;0\)</span>, instead of the constraint <span class="math inline">\(a+b&gt;0\)</span> for the <span class="math inline">\((a,b,0)\)</span> class we now have the weaker constraint of <span class="math inline">\(a+b/2&gt;0\)</span> for the <span class="math inline">\((a,b,1)\)</span> class. With the parametrization <span class="math inline">\(b=(r-1)a\)</span> as used in Section <a href="ChapFrequency-Modeling.html#S:the-a-b-0-class">2.3</a>, instead of <span class="math inline">\(r&gt;0\)</span> we now have the weaker constraint of <span class="math inline">\(r&gt;-1\)</span>. In particular, we see that while zero modifying a <span class="math inline">\((a,b,0)\)</span> distribution leads to a distribution in the <span class="math inline">\((a,b,1)\)</span> class, the conclusion does not hold in the other direction.</p>
<!-- %\textcolor{blue}{Add an example with a>0 and b=-3/2*a?}    -->
<p>Zero modification of a count distribution <span class="math inline">\(F\)</span> such that it assigns zero probability to zero count is called a <a href="#" class="tooltip" style="color:green"><em>zero truncation</em><span style="font-size:8pt">Zero modification of a count distribution such that it assigns zero probability to zero count </span></a> of <span class="math inline">\(F\)</span>. Hence, the zero truncated version of probabilities <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span> is given by
<span class="math display">\[
\tilde{p}_k=\begin{cases}
0, &amp; k=0;\\
\frac{p_k}{1-p_0}, &amp; k\geq 1.
\end{cases}
\]</span></p>
<p>In particular, we have that a zero modification of a count distribution <span class="math inline">\(\{p_k^T\}_{k\geq 0}\)</span>, denoted by <span class="math inline">\(\{p^M_k\}_{k\geq 0}\)</span>, can be written as a <a href="#" class="tooltip" style="color:green"><em>convex combination</em><span style="font-size:8pt">A linear combination of points where all coefficients are non-negative and sum to 1</span></a> of the <a href="#" class="tooltip" style="color:green"><em>degenerate distribution</em><span style="font-size:8pt">A deterministic distribution and takes only a single value</span></a> at <span class="math inline">\(0\)</span> and the zero truncation of <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span>, denoted by <span class="math inline">\(\{p^T_k\}_{k\geq 0}\)</span>. That is we have
<span class="math display">\[
p^M_k= p^M_0 \cdot \delta_{0}(k) + (1-p^M_0) \cdot p^T_k, \quad k\geq 0.  
\]</span></p>
<p><strong>Example 2.5.1. Zero Truncated/Modified Poisson</strong>.
Consider a Poisson distribution with parameter <span class="math inline">\(\lambda=2\)</span>. Calculate <span class="math inline">\(p_k, k=0,1,2,3\)</span>, for the usual (unmodified), truncated and a modified
version with <span class="math inline">\((p_0^M=0.6)\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.2.5.1" href="javascript:toggleEX('toggleExample.2.5.1','displayExample.2.5.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.2.5.1" style="display: none">
<p><strong>Solution.</strong> For the Poisson distribution as a member of the (<span class="math inline">\(a,b\)</span>,0) class, we have <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=\lambda=2\)</span>. Thus, we may use the recursion <span class="math inline">\(p_k = \lambda p_{k-1}/k= 2 p_{k-1}/k\)</span> for each type, after determining starting probabilities. The calculation of probabilities for <span class="math inline">\(k\leq 3\)</span> is shown in <a href="#tab:2.2">Table 2.2</a>.</p>
<p><a id=tab:2.2></a></p>
<p>Table 2.2. <strong>Calculation of Probabilities for</strong> <span class="math inline">\(k\leq 3\)</span></p>
<p><span class="math display">\[
\begin{matrix}
\begin{array}{c|c|c|c}
\hline
k &amp; p_k &amp; p_k^T &amp; p_k^M\\\hline
0 &amp; p_0=e^{-\lambda}=0.135335 &amp; 0 &amp; 0.6\\\hline
1 &amp; p_1=p_0(0+\frac{\lambda}{1})=0.27067 &amp;
\frac{p_1}{1-p_0}=0.313035 &amp;
\frac{1-p_0^M}{1-p_0}~p_1=0.125214\\\hline
2 &amp; p_2=p_1\left( \frac{\lambda}{2}\right)=0.27067 &amp;
p_2^T=p_1^T\left(\frac{\lambda}{2}\right)=0.313035 &amp;
p_2^M=p_1^M\left(\frac{\lambda}{2}\right)=0.125214\\\hline
3 &amp; p_3=p_2\left(\frac{\lambda}{3}\right)=0.180447 &amp;
p_3^T=p_2^T\left(\frac{\lambda}{3}\right)=0.208690 &amp;
p_3^M=p_2^M\left(\frac{\lambda}{3}\right)=0.083476\\\hline
\end{array}
\end{matrix}
\]</span></p>
</div>
<hr />
<div id="surveyElement25">

</div>
<div id="surveyResult25">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz25.1" href="javascript:toggleQuiz
('display.Quiz25.2','display.Quiz25.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz25.2" style="display: none">
<p id="Quiz25Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz25.js">
</script>
</div>
</div>
<div id="S:mixture-distributions" class="section level2">
<h2><span class="header-section-number">2.6</span> Mixture Distributions</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Define a mixture distribution when the mixing component is based on a finite number of sub-groups</li>
<li>Compute mixture distribution probabilities from mixing proportions and knowledge of the distribution of each subgroup</li>
<li>Define a mixture distribution when the mixing component is continuous</li>
</ul>
<hr />
<p>In many applications, the underlying population consists of naturally defined sub-groups with some homogeneity within each sub-group. In such cases it is convenient to model the individual sub-groups, and in a ground-up manner model the whole population. As we shall see below, beyond the aesthetic appeal of the approach, it also extends the range of applications that can be catered to by standard parametric distributions.</p>
<p>Let <span class="math inline">\(k\)</span> denote the number of defined sub-groups in a population, and let <span class="math inline">\(F_i\)</span> denote the distribution of an observation drawn from the <span class="math inline">\(i\)</span>-th subgroup. If we let <span class="math inline">\(\alpha_i\)</span> denote the proportion of the population in the <span class="math inline">\(i\)</span>-th subgroup, with <span class="math inline">\(\sum_{i=1}^k \alpha_i=1\)</span>, then the distribution of a randomly chosen observation from the population, denoted by <span class="math inline">\(F\)</span>, is given by</p>
<p><span class="math display" id="eq:mixdefn">\[\begin{equation}
F(x)=\sum_{i=1}^k \alpha_i \cdot F_i(x).
\tag{2.6}
\end{equation}\]</span></p>
<p>The above expression can be seen as a direct application of the Law of Total Probability. As an example, consider a population of drivers split broadly into two sub-groups, those with at most five years of driving experience and those with more than five years experience. Let <span class="math inline">\(\alpha\)</span> denote the proportion of drivers with less than <span class="math inline">\(5\)</span> years experience, and <span class="math inline">\(F_{\leq 5}\)</span> and <span class="math inline">\(F_{&gt; 5}\)</span> denote the distribution of the count of claims in a year for a driver in each group, respectively. Then the distribution of claim count of a randomly selected driver is given by
<span class="math display">\[
\alpha\cdot F_{\leq 5}(x) + (1-\alpha)F_{&gt; 5}(x).
\]</span></p>
<p>An alternate definition of a <a href="#" class="tooltip" style="color:green"><em>mixture distribution</em><span style="font-size:8pt">The probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized</span></a> is as follows. Let <span class="math inline">\(N_i\)</span> be a random variable with distribution <span class="math inline">\(F_i\)</span>, <span class="math inline">\(i=1,\ldots, k\)</span>. Let <span class="math inline">\(I\)</span> be a random variable taking values <span class="math inline">\(1,2,\ldots,k\)</span> with probabilities <span class="math inline">\(\alpha_1,\ldots,\alpha_k\)</span>, respectively. Then the random variable <span class="math inline">\(N_I\)</span> has a distribution given by equation <a href="ChapFrequency-Modeling.html#eq:mixdefn">(2.6)</a><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<p>In <a href="ChapFrequency-Modeling.html#eq:mixdefn">(2.6)</a> we see that the distribution function is a convex combination of the component distribution functions. This result easily extends to the probability mass function, the survival function, the raw moments, and the expectation as these are all linear mappings of the distribution function. We note that this is not true for central moments like the variance, and conditional measures like the hazard rate function. In the case of variance it is easily seen as</p>
<p><span class="math display" id="eq:LawTotalVariation">\[\begin{equation}
\mathrm{Var}{[N_I]}=\mathrm{E}[{\mathrm{Var}[{N_I\vert I}]]} + \mathrm{Var}[{\mathrm{E}[{N_I|I}}]]=\sum_{i=1}^k \alpha_i \mathrm{Var}[{N_i}] + \mathrm{Var}[{\mathrm{E}[{N_I|I}}]] .
\tag{2.7}
\end{equation}\]</span></p>
<p>Appendix Chapter <a href="CAppB.html#CAppB">16</a> provides additional background about this important expression.</p>
<!-- \phantom{Exercise or example for hazard rate density/survival expectation etc..} -->
<p><strong>Example 2.6.1. Actuarial Exam Question</strong>.
In a certain town the number of common colds an individual will get in a year follows a Poisson distribution that depends on the individual’s age and smoking status. The distribution of the population and the mean number of colds are as follows:</p>
<p><a id=tab:2.3></a></p>
<p>Table 2.3. <strong>The Distribution of the Population and the Mean Number of Colds</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{l|c|c}
\hline
 &amp; \text{Proportion of population} &amp;
\text{Mean number of colds}\\\hline
\text{Children} &amp; 0.3 &amp; 3\\
\text{Adult Non-Smokers} &amp; 0.6 &amp; 1\\
\text{Adult Smokers} &amp; 0.1 &amp; 4\\\hline
\end{array}
}
\]</span></p>
<!-- \def\labelenumi{\arabic{enumi}.} -->
<ol style="list-style-type: decimal">
<li>Calculate the probability that a randomly drawn person has 3 common colds in a year.</li>
<li>Calculate the conditional probability that a person with exactly 3 common colds in a year is an adult smoker.</li>
</ol>
<h5 style="text-align: center;">
<a id="displayExample.2.6.1" href="javascript:toggleEX('toggleExample.2.6.1','displayExample.2.6.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.2.6.1" style="display: none">
<p><strong>Solution.</strong></p>
<!-- \def\labelenumi{\arabic{enumi}.} -->
<ol style="list-style-type: decimal">
<li>Using Law of Total Probability, we can write the required probability as <span class="math inline">\(\Pr(N_I=3)\)</span>, with <span class="math inline">\(I\)</span> denoting the group of the randomly selected individual with <span class="math inline">\(1,2\)</span> and <span class="math inline">\(3\)</span> signifying the groups <em>Children</em>, <em>Adult Non-Smoker</em>, and <em>Adult Smoker</em>, respectively. Now by conditioning we get
<span class="math display">\[
\Pr(N_I=3)=0.3\cdot\Pr(N_1=3)+0.6\cdot\Pr(N_2=3)+0.1\cdot\Pr(N_3=3),
\]</span>
with <span class="math inline">\(N_1,N_2\)</span> and <span class="math inline">\(N_3\)</span> following Poisson distributions with means <span class="math inline">\(3,1\)</span>, and <span class="math inline">\(4\)</span>, respectively. Using the above, we get <span class="math inline">\(\Pr(N_I=3)\sim0.1235\)</span></li>
<li>The conditional probability of event A given event B, <span class="math inline">\(\Pr(A\vert B) = \frac{\Pr(A,B)}{\Pr(B)}\)</span>. The required conditional probability in this problem can then be written as <span class="math inline">\(\Pr(I=3\vert N_I=3)\)</span>, which equals
<span class="math display">\[
\Pr(I=3\vert N_I=3)=\frac{\Pr(I=3,N_3=3)}{\Pr(N_I=3)}\sim\frac{0.1 \times 0.1954}{0.1235}\sim 0.1581.
\]</span></li>
</ol>
</div>
<hr />
<p>In the above example, the number of subgroups <span class="math inline">\(k\)</span> was equal to three. In general, <span class="math inline">\(k\)</span> can be any natural number, but when <span class="math inline">\(k\)</span> is large it is parsimonious from a modeling point of view to take the following <em>infinitely many subgroup</em> approach. To motivate this approach, let the <span class="math inline">\(i\)</span>-th subgroup be such that its component distribution <span class="math inline">\(F_i\)</span> is given by <span class="math inline">\(G_{\tilde{\theta_i}}\)</span>, where <span class="math inline">\(G_\cdot\)</span> is a parametric family of distributions with parameter space <span class="math inline">\(\Theta\subseteq \mathbb{R}^d\)</span>. With this assumption, the distribution function <span class="math inline">\(F\)</span> of a randomly drawn observation from the population is given by
<span class="math display">\[
F(x)=\sum_{i=1}^k \alpha_i G_{\tilde{\theta_i}}(x),\quad \forall x\in\mathbb{R} ,
\]</span></p>
<p>similar to equation <a href="ChapFrequency-Modeling.html#eq:mixdefn">(2.6)</a>. Alternately, it can be written as<br />
<span class="math display">\[
F(x)=\mathrm{E}[{G_{\tilde{\vartheta}}(x)}],\quad \forall x\in\mathbb{R},
\]</span>
where <span class="math inline">\(\tilde{\vartheta}\)</span> takes values <span class="math inline">\(\tilde{\theta_i}\)</span> with probability <span class="math inline">\(\alpha_i\)</span>, for <span class="math inline">\(i=1,\ldots,k\)</span>. The above makes it clear that when <span class="math inline">\(k\)</span> is large, one could model the above by treating <span class="math inline">\(\tilde{\vartheta}\)</span> as continuous random variable.</p>
<p>To illustrate this approach, suppose we have a population of drivers with the distribution of claims for an individual driver being distributed as a Poisson. Each person has their own (personal) expected number of claims <span class="math inline">\(\lambda\)</span> - smaller values for good drivers, and larger values for others. There is a distribution of <span class="math inline">\(\lambda\)</span> in the population; a popular and convenient choice for modeling this distribution is a gamma distribution with parameters <span class="math inline">\((\alpha, \theta)\)</span> (the gamma distribution will be introduced formally in Section <a href="ChapSeverity.html#S:Loss:Gamma">3.2.1</a>). With these specifications it turns out that the resulting distribution of <span class="math inline">\(N\)</span>, the claims of a randomly chosen driver, is a negative binomial with parameters <span class="math inline">\((r=\alpha,\beta=\theta)\)</span>. This can be shown in many ways, but a straightforward argument is as follows:</p>
<p><span class="math display">\[
\begin{array}{ll}
\Pr(N=k)&amp;= \int_0^\infty \frac{e^{-\lambda}\lambda^k}{k!} \frac{\lambda^{\alpha-1}e^{-\lambda/\theta}}{\Gamma{(\alpha)}\theta^{\alpha}} d\lambda = 
\frac{1}{k!\Gamma(\alpha)\theta^\alpha}\int_0^\infty \lambda^{\alpha+k-1}e^{-\lambda(1+1/\theta)}~d\lambda \\
&amp;=\frac{\Gamma{(\alpha+k)}}{k!\Gamma(\alpha)\theta^\alpha(1+1/\theta)^{\alpha+k}} \\
&amp;={\alpha+k-1 \choose k}\left(\frac{1}{1+\theta}\right)^\alpha\left(\frac{\theta}{1+\theta}\right)^k, \quad k=0,1,\ldots
\end{array}
\]</span></p>
<p>Note that the above derivation implicitly uses the following:
<span class="math display">\[
f_{N\vert\Lambda=\lambda}(N=k)=\frac{e^{-\lambda}\lambda^k}{k!}, \quad k\geq 0; \quad \hbox{and} \quad f_{\Lambda}(\lambda)= \frac{\lambda^{\alpha-1}e^{-\lambda/\theta}}{\Gamma{(\alpha)}\theta^{\alpha}}, \quad \lambda&gt;0.
\]</span></p>
<p>By considering mixtures of a parametric class of distributions, we increase the richness of the class. This expansion of distributions results in the mixture class being able to cater well to more applications that the parametric class we started with. Mixture modeling is an important modeling technique in insurance applications and later chapters will cover more aspects of this modeling technique.</p>
<p><strong>Example 2.6.2.</strong>
Suppose that <span class="math inline">\(N|\Lambda \sim\)</span> Poisson<span class="math inline">\((\Lambda)\)</span> and that <span class="math inline">\(\Lambda \sim\)</span> gamma with mean of 1 and variance of 2. Determine the probability that <span class="math inline">\(N=1\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.2.6.2" href="javascript:toggleEX('toggleExample.2.6.2','displayExample.2.6.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.2.6.2" style="display: none">
<p><strong>Solution.</strong> For a gamma distribution with parameters <span class="math inline">\((\alpha, \theta)\)</span>, we have that the mean is <span class="math inline">\(\alpha \theta\)</span> and the variance is <span class="math inline">\(\alpha \theta^2\)</span>. Using these expressions we have
<span class="math display">\[
\begin{aligned}
\alpha &amp;= \frac{1}{2} \text{   and   } \theta =2.
\end{aligned}
\]</span>
Now, one can directly use the above result to conclude that <span class="math inline">\(N\)</span> is distributed as a negative binomial with <span class="math inline">\(r = \alpha = \frac{1}{2}\)</span> and <span class="math inline">\(\beta= \theta =2\)</span>. Thus
<span class="math display">\[
\begin{aligned}
\Pr(N=1)  &amp;= {1+r-1 \choose 1}(\frac{1}{(1+\beta)^r})\left(\frac{\beta}{1+\beta}\right)^1 \\
&amp;=                 {1+\frac{1}{2}-1 \choose 1}{\frac{1}{(1+2)^{1/2}}}\left(\frac{2}{1+2}\right)^1\\
&amp;=  \frac{1}{3^{3/2}} = 0.19245 .
\end{aligned}
\]</span></p>
</div>
<hr />
<div id="surveyElement26">

</div>
<div id="surveyResult26">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz26.1" href="javascript:toggleQuiz
('display.Quiz26.2','display.Quiz26.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz26.2" style="display: none">
<p id="Quiz26Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz26.js">
</script>
</div>
<div id="S:goodness-of-fit" class="section level2">
<h2><span class="header-section-number">2.7</span> Goodness of Fit</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Calculate a goodness of fit statistic to compare a hypothesized discrete distribution to a sample of discrete observations</li>
<li>Compare the statistic to a reference distribution to assess the adequacy of the fit</li>
</ul>
<hr />
<p>In the above we have discussed three basic frequency distributions, along with their extensions through zero modification/truncation and by looking at mixtures of these distributions. Nevertheless, these classes still remain parametric and hence by their very nature a small subset of the class of all possible frequency distributions (that is, the set of distributions on non-negative integers). Hence, even though we have talked about methods for estimating the unknown parameters, the <em>fitted</em> distribution is not be a good representation of the underlying distribution if the latter is <strong>far</strong> from the class of distribution used for modeling. In fact, it can be shown that the <em>maximum likelihood estimator</em> converges to a value such that the corresponding distribution is a Kullback-Leibler <em>projection</em> of the underlying distribution on the class of distributions used for modeling. Below we present one testing method - Pearson’s chi-square statistic - to check for the <a href="#" class="tooltip" style="color:green"><em>goodness of fit</em><span style="font-size:8pt">The goodness of fit of a statistical model describes how well it fits a set of observations.</span></a> of the fitted distribution. For more details on the <a href="#" class="tooltip" style="color:green"><em>Pearson’s chi-square test</em><span style="font-size:8pt">A statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance</span></a>, at an introductory mathematical statistics level, we refer the reader to Section 9.1 of <span class="citation">Hogg, Tanis, and Zimmerman (<a href="#ref-zimmerman2015" role="doc-biblioref">2015</a>)</span>.</p>
<p>In <span class="math inline">\(1993\)</span>, a portfolio of <span class="math inline">\(n=7,483\)</span> automobile insurance policies from a major Singaporean insurance company had the distribution of auto accidents per policyholder as given in <a href="#tab:2.4">Table 2.4</a>.</p>
<p><a id=tab:2.4></a></p>
<p>Table 2.4. <strong>Singaporean Automobile Accident Data</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{l|c|c|c|c|c|c}
\hline
\text{Count }(k) &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; \text{Total}\\
\hline
\text{No. of Policies with }k\text{ accidents }(m_k) &amp; 6,996 &amp; 455 &amp; 28 &amp; 4 &amp; 0 &amp; 7,483\\
\hline
\end{array}
}
\]</span></p>
<p>If we a fit a Poisson distribution, then the <em>mle</em> for <span class="math inline">\(\lambda\)</span>, the Poisson mean, is the sample mean which is given by
<span class="math display">\[
\overline{N} = \frac{0\cdot 6996 + 1 \cdot 455 + 2 \cdot 28 + 3 \cdot 4 + 4 \cdot 0}{7483} = 0.06989.
\]</span>
Now if we use Poisson (<span class="math inline">\(\hat{\lambda}_{MLE}\)</span>) as the fitted distribution, then a tabular comparison of the fitted counts and observed counts is given by <a href="#tab:2.5">Table 2.5</a> below, where <span class="math inline">\(\hat{p}_k\)</span> represents the estimated probabilities under the fitted Poisson distribution.</p>
<p><a id=tab:2.5></a></p>
<p>Table 2.5. <strong>Comparison of Observed to Fitted Counts: Singaporean Auto Data</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{c|r|r}
\hline
\text{Count}  &amp; \text{Observed}  &amp; \text{Fitted Counts}\\
(k) &amp; (m_k) &amp; \text{Using Poisson }(n\hat{p}_k)\\
\hline
0 &amp; 6,996 &amp; 6,977.86 \\
1 &amp; 455 &amp; 487.70 \\
2 &amp; 28 &amp; 17.04 \\
3 &amp; 4 &amp; 0.40 \\
\geq 4 &amp; 0 &amp; 0.01\\
\hline
\text{Total} &amp; 7,483 &amp; 7,483.00\\
\hline
\end{array}
}
\]</span></p>
<p>While the fit seems <em>reasonable</em>, a tabular comparison falls short of a statistical test of the hypothesis that the underlying distribution is indeed Poisson. The Pearson’s chi-square statistic is a goodness of fit statistical measure that can be used for this purpose. To explain this statistic let us suppose that a dataset of size <span class="math inline">\(n\)</span> is grouped into <span class="math inline">\(k\)</span> cells with <span class="math inline">\(m_k/n\)</span> and <span class="math inline">\(\hat{p}_k\)</span>, for <span class="math inline">\(k=1\ldots,K\)</span> being the observed and estimated probabilities of an observation belonging to the <span class="math inline">\(k\)</span>-th cell, respectively. The Pearson’s chi-square test statistic is then given by
<span class="math display">\[
\sum_{k=1}^K\frac{\left( m_k-n\widehat{p}_k \right) ^{2}}{n\widehat{p}_k}.
\]</span>
The motivation for the above statistic derives from the fact that
<span class="math display">\[
\sum_{k=1}^K\frac{\left( m_k-n{p}_k \right) ^{2}}{n{p}_k}
\]</span>
has a limiting <a href="#" class="tooltip" style="color:green"><em>chi-square distribution</em><span style="font-size:8pt">The chi-squared distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables</span></a> with <span class="math inline">\(K-1\)</span> degrees of freedom if <span class="math inline">\(p_k\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span> are the true cell probabilities. Now suppose that only the summarized data represented by <span class="math inline">\(m_k\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span> is available. Further, if <span class="math inline">\(p_k\)</span>’s are functions of <span class="math inline">\(s\)</span> parameters, replacing <span class="math inline">\(p_k\)</span>’s by any <em>efficiently</em> estimated probabilities <span class="math inline">\(\widehat{p}_k\)</span>’s results in the statistic continuing to have a limiting chi-square distribution but with degrees of freedom given by <span class="math inline">\(K-1-s\)</span>. Such efficient estimates can be derived for example by using the <em>mle</em> method (with a <a href="#" class="tooltip" style="color:green"><em>multinomial likelihood</em><span style="font-size:8pt">The multinomial distribution models the probability of counts for rolling a k-sided die n times</span></a>) or by estimating the <span class="math inline">\(s\)</span> parameters which minimizes the Pearson’s chi-square statistic above. For example, the <code>R</code> code below does calculate an estimate for <span class="math inline">\(\lambda\)</span> doing the latter and results in the estimate <span class="math inline">\(0.06623153\)</span>, close but different from the <em>mle</em> of <span class="math inline">\(\lambda\)</span> using the full data:</p>
<pre><code>m&lt;-c(6996,455,28,4,0);
op&lt;-m/sum(m);
g&lt;-function(lam){sum((op-c(dpois(0:3,lam),1-ppois(3,lam)))^2)};
optim(sum(op*(0:4)),g,method=&quot;Brent&quot;,lower=0,upper=10)$par</code></pre>
<p>When one uses the full data to estimate the probabilities, the asymptotic distribution is <em>in between</em> chi-square distributions with parameters <span class="math inline">\(K-1\)</span> and <span class="math inline">\(K-1-s\)</span>. In practice it is common to ignore this subtlety and assume the limiting chi-square has <span class="math inline">\(K-1-s\)</span> degrees of freedom. Interestingly, this practical shortcut works quite well in the case of the Poisson distribution.</p>
<p>For the Singaporean auto data the Pearson’s chi-square statistic equals <span class="math inline">\(41.98\)</span> using the full data <em>mle</em> for <span class="math inline">\({\lambda}\)</span>. Using the limiting distribution of chi-square with <span class="math inline">\(5-1-1=3\)</span> degrees of freedom, we see that the value of <span class="math inline">\(41.98\)</span> is way out in the tail (<span class="math inline">\(99\)</span>-th percentile is below <span class="math inline">\(12\)</span>). Hence we can conclude that the Poisson distribution provides an inadequate fit for the data.</p>
<p>In the above, we started with the cells as given in the above tabular summary. In practice, a relevant question is how to define the cells so that the chi-square distribution is a good approximation to the finite sample distribution of the statistic. A rule of thumb is to define the cells in such a way to have at least <span class="math inline">\(80\%\)</span>, if not all, of the cells having expected counts greater than <span class="math inline">\(5\)</span>. Also, it is clear that a larger number of cells results in a higher power of the test, and hence a simple rule of thumb is to maximize the number of cells such that each cell has at least 5 observations.</p>
<div id="surveyElement27">

</div>
<div id="surveyResult27">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz27.1" href="javascript:toggleQuiz
('display.Quiz27.2','display.Quiz27.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz27.2" style="display: none">
<p id="Quiz27Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz27.js">
</script>
</div>
<div id="S:exercises" class="section level2">
<h2><span class="header-section-number">2.8</span> Exercises</h2>
<div id="theoretical-exercises" class="section level4 unnumbered">
<h4>Theoretical Exercises</h4>
<p><strong>Exercise 2.1.</strong> Derive an expression for <span class="math inline">\(p_N(\cdot)\)</span> in terms of <span class="math inline">\(F_N(\cdot)\)</span> and <span class="math inline">\(S_N(\cdot)\)</span>.</p>
<p><strong>Exercise 2.2.</strong> A measure of center of location must be <strong>equi-variant</strong> with respect to shifts, or location transformations. In other words, if <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> are two random variables such that <span class="math inline">\(N_1+c\)</span> has the same distribution as <span class="math inline">\(N_2\)</span>, for some constant <span class="math inline">\(c\)</span>, then the difference between the measures of the center of location of <span class="math inline">\(N_2\)</span> and <span class="math inline">\(N_1\)</span> must equal <span class="math inline">\(c\)</span>. Show that the mean satisfies this property.</p>
<p><strong>Exercise 2.3.</strong> Measures of dispersion should be invariant with respect to shifts and scale equi-variant. Show that standard deviation satisfies these properties by doing the following:</p>
<ul>
<li>Show that for a random variable <span class="math inline">\(N\)</span>, its standard deviation equals that of <span class="math inline">\(N+c\)</span>, for any constant <span class="math inline">\(c\)</span>.</li>
<li>Show that for a random variable <span class="math inline">\(N\)</span>, its standard deviation equals <span class="math inline">\(1/c\)</span> times that of <span class="math inline">\(cN\)</span>, for any positive constant <span class="math inline">\(c\)</span>.</li>
</ul>
<p><strong>Exercise 2.4.</strong> Let <span class="math inline">\(N\)</span> be a random variable with probability mass function given by
<span class="math display">\[
p_N(k)= \begin{cases}
\left(\frac{6}{\pi^2}\right)\left(\frac{1}{k^{2}}\right), &amp; k\geq 1;\\
0, &amp;\hbox{otherwise}.
\end{cases}
\]</span>
Show that the mean of <span class="math inline">\(N\)</span> is <span class="math inline">\(\infty\)</span>.</p>
<p><strong>Exercise 2.5.</strong> Let <span class="math inline">\(N\)</span> be a random variable with a finite second moment. Show that the function <span class="math inline">\(\psi(\cdot)\)</span> defined by
<span class="math display">\[
\psi(x)=\mathrm{E}{(N-x)^2}. \quad x\in\mathbb{R}
\]</span>
is minimized at <span class="math inline">\(\mu_N\)</span> without using calculus. Also, give a proof of this fact using derivatives. Conclude that the minimum value equals the variance of <span class="math inline">\(N\)</span>.</p>
<p><strong>Exercise 2.6.</strong> Derive the first two central moments of the <span class="math inline">\((a,b,0)\)</span> distributions using the methods mentioned below:</p>
<ul>
<li>For the binomial distribution, derive the moments using only its <em>pmf</em>, then its <em>mgf</em>, and then its <em>pgf</em>.</li>
<li>For the Poisson distribution, derive the moments using only its <em>mgf</em>.</li>
<li>For the negative binomial distribution, derive the moments using only its <em>pmf</em>, and then its <em>pgf</em>.</li>
</ul>
<p><strong>Exercise 2.7.</strong> Let <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> be two independent Poisson random variables with means <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, respectively. Identify the conditional distribution of <span class="math inline">\(N_1\)</span> given <span class="math inline">\(N_1+N_2\)</span>.</p>
<p><strong>Exercise 2.8.</strong> (<strong>Non-Uniqueness of the MLE</strong>) Consider the following parametric family of densities indexed by the parameter <span class="math inline">\(p\)</span> taking values in <span class="math inline">\([0,1]\)</span>:
<span class="math display">\[
f_p(x)=p\cdot\phi(x+2)+(1-p)\cdot\phi(x-2), \quad x\in\mathbb{R},
\]</span>
where <span class="math inline">\(\phi(\cdot)\)</span> represents the standard normal density.</p>
<ul>
<li>Show that for all <span class="math inline">\(p\in[0,1]\)</span>, <span class="math inline">\(f_p(\cdot)\)</span> above is a valid density function.</li>
<li>Find an expression in <span class="math inline">\(p\)</span> for the mean and the variance of <span class="math inline">\(f_p(\cdot)\)</span>.</li>
<li>Let us consider a sample of size one consisting of <span class="math inline">\(x\)</span>. Show that when <span class="math inline">\(x\)</span> equals <span class="math inline">\(0\)</span>, the set of <em>maximum likelihood estimates</em> for <span class="math inline">\(p\)</span> equals <span class="math inline">\([0,1]\)</span>; also show that the <em>mle</em> is unique otherwise.</li>
</ul>
<p><strong>Exercise 2.9.</strong> Graph the region of the plane corresponding to values of <span class="math inline">\((a,b)\)</span> that give rise to valid <span class="math inline">\((a,b,0)\)</span> distributions. Do the same for <span class="math inline">\((a,b,1)\)</span> distributions.</p>
<p><strong>Exercise 2.10.</strong> (<strong>Computational Complexity</strong>) For the <span class="math inline">\((a,b,0)\)</span> class of distributions, count the number of basic mathematical operations (addition, subtraction, multiplication, division) needed to compute the <span class="math inline">\(n\)</span> probabilities <span class="math inline">\(p_0\ldots p_{n-1}\)</span> using the recurrence relationship. For the negative binomial distribution with non-integer <span class="math inline">\(r\)</span>, count the number of such operations. What do you observe?</p>
<p><strong>Exercise 2.11.</strong> (** **) Using the development of Section 2.3 rigorously show that not only does the recurrence <a href="ChapFrequency-Modeling.html#eq:ab0">(2.1)</a> tie the binomial, the Poisson and the negative binomial distributions together, but that it also characterizes them.</p>
</div>
<div id="exercises-with-a-practical-focus" class="section level4 unnumbered">
<h4>Exercises with a Practical Focus</h4>
<p><strong>Exercise 2.12. Actuarial Exam Question.</strong> You are given:</p>
<!-- \def\labelenumi{\arabic{enumi}.} -->
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p_k\)</span> denotes the probability that the number of claims equals <span class="math inline">\(k\)</span>
for <span class="math inline">\(k=0,1,2,\ldots\)</span></li>
<li><span class="math inline">\(\frac{p_n}{p_m}=\frac{m!}{n!}, m\ge 0, n\ge 0\)</span></li>
</ol>
<p>Using the corresponding zero-modified claim count distribution with
<span class="math inline">\(p_0^M=0.1\)</span>, calculate <span class="math inline">\(p_1^M\)</span>.</p>
<p><strong>Exercise 2.13. Actuarial Exam Question.</strong> During a one-year period, the number of accidents per day was distributed as follows:</p>
<!-- \label{tabsing3} -->
<p><span class="math display">\[
\begin{matrix}
\begin{array}{c|c|c|c|c|c|c}
\hline
\text{No. of Accidents} &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\
\hline
\text{No. of Days} &amp; 209 &amp; 111 &amp; 33 &amp; 7 &amp; 5 &amp; 2\\
\hline
\end{array}
\end{matrix}
\]</span></p>
<p>You use a chi-square test to measure the fit of a Poisson distribution with mean 0.60. The minimum expected number of observations in any group should be 5. The maximum number of groups should be used. Determine the value of the chi-square statistic.</p>
</div>
<div id="additional-exercises" class="section level4 unnumbered">
<h4>Additional Exercises</h4>
<p>Here are a set of exercises that guide the viewer through some of the theoretical foundations of <strong>Loss Data Analytics</strong>. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C/STAM.</p>
<p style="text-align: center;">
<a href="https://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-problems/">Frequency Distribution Guided Tutorials</a>
</p>
</div>
</div>
<div id="Freq-further-reading-and-resources" class="section level2">
<h2><span class="header-section-number">2.9</span> Further Resources and Contributors</h2>
<p>Appendix Chapter <a href="CAppA.html#CAppA">15</a> gives a general introduction to maximum likelihood theory regarding estimation of parameters from a parametric family. Appendix Chapter <a href="CAppC.html#CAppC">17</a> gives more specific examples and expands some of the concepts.</p>
<div id="contributors-1" class="section level4 unnumbered">
<h4>Contributors</h4>
<ul>
<li><strong>N.D. Shyamalkumar</strong>, The University of Iowa, and <strong>Krupa Viswanathan</strong>, Temple University, are the principal authors of the initial version of this chapter. Email: <a href="mailto:shyamal-kumar@uiowa.edu" class="email">shyamal-kumar@uiowa.edu</a> for chapter comments and suggested improvements.</li>
<li>Chapter reviewers include: Chunsheng Ban, Paul Johnson, Hirokazu (Iwahiro) Iwasawa, Dalia Khalil, Tatjana Miljkovic, Rajesh Sahasrabuddhe, and Michelle Xia.</li>
</ul>
</div>
<div id="S:rcode" class="section level3">
<h3><span class="header-section-number">2.9.1</span> TS 2.A. R Code for Plots</h3>
<p><strong>Code for Figure <a href="ChapFrequency-Modeling.html#fig:MLEm">2.2</a>:</strong></p>
<h5 style="text-align: center;">
<a id="displayCode.Freq.2" href="javascript:togglecode('toggleCode.Freq.2','displayCode.Freq.2');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleCode.Freq.2" style="display: none">
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="ChapFrequency-Modeling.html#cb10-1"></a>likm&lt;-<span class="cf">function</span>(m){</span>
<span id="cb10-2"><a href="ChapFrequency-Modeling.html#cb10-2"></a>  <span class="kw">prod</span>((<span class="kw">dbinom</span>(x,m,<span class="kw">mean</span>(x)<span class="op">/</span>m)))</span>
<span id="cb10-3"><a href="ChapFrequency-Modeling.html#cb10-3"></a>}</span>
<span id="cb10-4"><a href="ChapFrequency-Modeling.html#cb10-4"></a>x&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>);</span>
<span id="cb10-5"><a href="ChapFrequency-Modeling.html#cb10-5"></a>n&lt;-(<span class="dv">5</span><span class="op">:</span><span class="dv">100</span>);</span>
<span id="cb10-6"><a href="ChapFrequency-Modeling.html#cb10-6"></a><span class="co"># Computing the Likelihood </span></span>
<span id="cb10-7"><a href="ChapFrequency-Modeling.html#cb10-7"></a>ll&lt;-<span class="kw">sapply</span>(n,likm); </span>
<span id="cb10-8"><a href="ChapFrequency-Modeling.html#cb10-8"></a><span class="co"># Computing the MLE</span></span>
<span id="cb10-9"><a href="ChapFrequency-Modeling.html#cb10-9"></a>n[ll<span class="op">==</span><span class="kw">max</span>(ll)]</span>
<span id="cb10-10"><a href="ChapFrequency-Modeling.html#cb10-10"></a><span class="co"># Storing the Likelihood Curve</span></span>
<span id="cb10-11"><a href="ChapFrequency-Modeling.html#cb10-11"></a>y&lt;-<span class="kw">cbind</span>(n,ll);</span>
<span id="cb10-12"><a href="ChapFrequency-Modeling.html#cb10-12"></a></span>
<span id="cb10-13"><a href="ChapFrequency-Modeling.html#cb10-13"></a><span class="co"># Second Dataset</span></span>
<span id="cb10-14"><a href="ChapFrequency-Modeling.html#cb10-14"></a>x&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>);</span>
<span id="cb10-15"><a href="ChapFrequency-Modeling.html#cb10-15"></a>ll&lt;-<span class="kw">sapply</span>(n,likm);</span>
<span id="cb10-16"><a href="ChapFrequency-Modeling.html#cb10-16"></a>n[ll<span class="op">==</span><span class="kw">max</span>(ll)]</span>
<span id="cb10-17"><a href="ChapFrequency-Modeling.html#cb10-17"></a>y&lt;-<span class="kw">cbind</span>(y,ll);</span>
<span id="cb10-18"><a href="ChapFrequency-Modeling.html#cb10-18"></a></span>
<span id="cb10-19"><a href="ChapFrequency-Modeling.html#cb10-19"></a><span class="co"># Third Dataset</span></span>
<span id="cb10-20"><a href="ChapFrequency-Modeling.html#cb10-20"></a>x&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">7</span>);</span>
<span id="cb10-21"><a href="ChapFrequency-Modeling.html#cb10-21"></a>ll&lt;-<span class="kw">sapply</span>(n,likm);</span>
<span id="cb10-22"><a href="ChapFrequency-Modeling.html#cb10-22"></a>n[ll<span class="op">==</span><span class="kw">max</span>(ll)]</span>
<span id="cb10-23"><a href="ChapFrequency-Modeling.html#cb10-23"></a>y&lt;-<span class="kw">cbind</span>(y,ll);</span>
<span id="cb10-24"><a href="ChapFrequency-Modeling.html#cb10-24"></a></span>
<span id="cb10-25"><a href="ChapFrequency-Modeling.html#cb10-25"></a><span class="kw">colnames</span>(y)&lt;-<span class="kw">c</span>(<span class="st">&quot;m&quot;</span>,<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,5)$&quot;</span>,<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,6)$&quot;</span>,<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,7)$&quot;</span>);</span>
<span id="cb10-26"><a href="ChapFrequency-Modeling.html#cb10-26"></a>dy&lt;-<span class="kw">data.frame</span>(y);</span>
<span id="cb10-27"><a href="ChapFrequency-Modeling.html#cb10-27"></a><span class="kw">library</span>(tikzDevice);</span>
<span id="cb10-28"><a href="ChapFrequency-Modeling.html#cb10-28"></a><span class="kw">library</span>(ggplot2);</span>
<span id="cb10-29"><a href="ChapFrequency-Modeling.html#cb10-29"></a><span class="kw">options</span>(<span class="dt">tikzMetricPackages =</span> <span class="kw">c</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage[utf8]{inputenc}&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage[T1]{fontenc}&quot;</span>, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">usetikzlibrary{calc}&quot;</span>, </span>
<span id="cb10-30"><a href="ChapFrequency-Modeling.html#cb10-30"></a>                               <span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage{amssymb}&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage{amsmath}&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage[active]{preview}&quot;</span>))</span>
<span id="cb10-31"><a href="ChapFrequency-Modeling.html#cb10-31"></a><span class="kw">tikz</span>(<span class="dt">file =</span> <span class="st">&quot;plot_test.tex&quot;</span>, <span class="dt">width =</span> <span class="fl">6.25</span>, <span class="dt">height =</span> <span class="fl">3.125</span>);</span>
<span id="cb10-32"><a href="ChapFrequency-Modeling.html#cb10-32"></a><span class="kw">ggplot</span>(dy) <span class="op">+</span><span class="st"> </span></span>
<span id="cb10-33"><a href="ChapFrequency-Modeling.html#cb10-33"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>m, <span class="dt">y=</span>(X..tilde.x...<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">4</span>.<span class="dv">5</span>..),<span class="dt">shape=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,5):</span><span class="ch">\\</span><span class="st">hat{m}=7$&quot;</span>),<span class="dt">size=</span><span class="fl">0.75</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb10-34"><a href="ChapFrequency-Modeling.html#cb10-34"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>m, <span class="dt">y=</span>(X..tilde.x...<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">4</span>.<span class="dv">6</span>..),<span class="dt">shape=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,6):</span><span class="ch">\\</span><span class="st">hat{m}=18$&quot;</span>),<span class="dt">size=</span><span class="fl">0.75</span>) <span class="op">+</span></span>
<span id="cb10-35"><a href="ChapFrequency-Modeling.html#cb10-35"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>m, <span class="dt">y=</span>(X..tilde.x...<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">4</span>.<span class="dv">7</span>..),<span class="dt">shape=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,7):</span><span class="ch">\\</span><span class="st">hat{m}=</span><span class="ch">\\</span><span class="st">infty$&quot;</span>),<span class="dt">size=</span><span class="fl">0.75</span>) <span class="op">+</span></span>
<span id="cb10-36"><a href="ChapFrequency-Modeling.html#cb10-36"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">7</span>),<span class="dt">y=</span>dy<span class="op">$</span>X..tilde.x...<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">4</span>.<span class="dv">5</span>..[<span class="dv">3</span>],<span class="dt">colour=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">hat{m}$&quot;</span>,<span class="dt">shape=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,5):</span><span class="ch">\\</span><span class="st">hat{m}=7$&quot;</span>),<span class="dt">size=</span><span class="fl">0.75</span>)<span class="op">+</span></span>
<span id="cb10-37"><a href="ChapFrequency-Modeling.html#cb10-37"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">18</span>),<span class="dt">y=</span>dy<span class="op">$</span>X..tilde.x...<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">2</span>.<span class="dv">4</span>.<span class="dv">6</span>..[<span class="dv">14</span>],<span class="dt">colour=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">hat{m}$&quot;</span>,<span class="dt">shape=</span><span class="st">&quot;$</span><span class="ch">\\</span><span class="st">tilde{x}=(2,2,2,4,6):</span><span class="ch">\\</span><span class="st">hat{m}=18$&quot;</span>),<span class="dt">size=</span><span class="fl">0.75</span>)<span class="op">+</span></span>
<span id="cb10-38"><a href="ChapFrequency-Modeling.html#cb10-38"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;m&quot;</span>,<span class="dt">y=</span><span class="st">&quot;$L(m,</span><span class="ch">\\</span><span class="st">overline{x}/m)$&quot;</span>,<span class="dt">title=</span><span class="st">&quot;MLE for $m$: Non-Robustness of MLE &quot;</span>); </span>
<span id="cb10-39"><a href="ChapFrequency-Modeling.html#cb10-39"></a><span class="kw">dev.off</span>();</span></code></pre></div>
</div>
<hr />
<p><strong>Code for Figure <a href="ChapFrequency-Modeling.html#fig:MLEab0">2.3</a>:</strong></p>
<h5 style="text-align: center;">
<a id="displayCode.Freq.3" href="javascript:togglecode('toggleCode.Freq.3','displayCode.Freq.3');"><i><strong>Show R Code</strong></i></a>
</h5>
<div id="toggleCode.Freq.3" style="display: none">
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="ChapFrequency-Modeling.html#cb11-1"></a>likbinm&lt;-<span class="cf">function</span>(m){ </span>
<span id="cb11-2"><a href="ChapFrequency-Modeling.html#cb11-2"></a>  <span class="co"># binomial likelihood maximized w.r.t. p</span></span>
<span id="cb11-3"><a href="ChapFrequency-Modeling.html#cb11-3"></a>  <span class="kw">prod</span>((<span class="kw">dbinom</span>(x,m,<span class="kw">mean</span>(x)<span class="op">/</span>m)))</span>
<span id="cb11-4"><a href="ChapFrequency-Modeling.html#cb11-4"></a>}</span>
<span id="cb11-5"><a href="ChapFrequency-Modeling.html#cb11-5"></a></span>
<span id="cb11-6"><a href="ChapFrequency-Modeling.html#cb11-6"></a>liknbinm&lt;-<span class="cf">function</span>(r){</span>
<span id="cb11-7"><a href="ChapFrequency-Modeling.html#cb11-7"></a>  <span class="co"># negative binomial likelihood maximized w.r.t. beta</span></span>
<span id="cb11-8"><a href="ChapFrequency-Modeling.html#cb11-8"></a>  <span class="kw">prod</span>(<span class="kw">dnbinom</span>(x,r,<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(x)<span class="op">/</span>(<span class="kw">mean</span>(x)<span class="op">+</span>r)))</span>
<span id="cb11-9"><a href="ChapFrequency-Modeling.html#cb11-9"></a>}</span>
<span id="cb11-10"><a href="ChapFrequency-Modeling.html#cb11-10"></a></span>
<span id="cb11-11"><a href="ChapFrequency-Modeling.html#cb11-11"></a><span class="co"># Data Matrix; Three Samples, one in each Column; </span></span>
<span id="cb11-12"><a href="ChapFrequency-Modeling.html#cb11-12"></a><span class="co"># First Sample has Var&lt;Mean</span></span>
<span id="cb11-13"><a href="ChapFrequency-Modeling.html#cb11-13"></a><span class="co"># Second Sample has Var=Mean</span></span>
<span id="cb11-14"><a href="ChapFrequency-Modeling.html#cb11-14"></a><span class="co"># Third Sample has Var&gt;Mean</span></span>
<span id="cb11-15"><a href="ChapFrequency-Modeling.html#cb11-15"></a></span>
<span id="cb11-16"><a href="ChapFrequency-Modeling.html#cb11-16"></a>  X&lt;-<span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">9</span>)<span class="op">+</span><span class="dv">2</span>,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">9</span>),<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">9</span>)); </span>
<span id="cb11-17"><a href="ChapFrequency-Modeling.html#cb11-17"></a></span>
<span id="cb11-18"><a href="ChapFrequency-Modeling.html#cb11-18"></a><span class="co"># Used for creating the labels in the z matrix</span></span>
<span id="cb11-19"><a href="ChapFrequency-Modeling.html#cb11-19"></a>  ord_char&lt;-<span class="kw">c</span>(<span class="st">&quot;&lt;&quot;</span>,<span class="st">&quot;=&quot;</span>,<span class="st">&quot;&gt;&quot;</span>); </span>
<span id="cb11-20"><a href="ChapFrequency-Modeling.html#cb11-20"></a>  </span>
<span id="cb11-21"><a href="ChapFrequency-Modeling.html#cb11-21"></a><span class="co"># Empty matrices; </span></span>
<span id="cb11-22"><a href="ChapFrequency-Modeling.html#cb11-22"></a>  Y&lt;-<span class="kw">matrix</span>(<span class="dv">1</span>,<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">nrow=</span><span class="dv">0</span>); </span>
<span id="cb11-23"><a href="ChapFrequency-Modeling.html#cb11-23"></a>  Z&lt;-<span class="kw">matrix</span>(<span class="dv">1</span>,<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">nrow=</span><span class="dv">0</span>); </span>
<span id="cb11-24"><a href="ChapFrequency-Modeling.html#cb11-24"></a></span>
<span id="cb11-25"><a href="ChapFrequency-Modeling.html#cb11-25"></a><span class="cf">for</span> (i <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)) {</span>
<span id="cb11-26"><a href="ChapFrequency-Modeling.html#cb11-26"></a>  <span class="co"># Work with data in the i-th sample</span></span>
<span id="cb11-27"><a href="ChapFrequency-Modeling.html#cb11-27"></a>    x&lt;-X[,i]; </span>
<span id="cb11-28"><a href="ChapFrequency-Modeling.html#cb11-28"></a>  </span>
<span id="cb11-29"><a href="ChapFrequency-Modeling.html#cb11-29"></a>  <span class="co"># Binomial Likelihood</span></span>
<span id="cb11-30"><a href="ChapFrequency-Modeling.html#cb11-30"></a>      <span class="co"># Interval of n values covering the MLE</span></span>
<span id="cb11-31"><a href="ChapFrequency-Modeling.html#cb11-31"></a>        n&lt;-(<span class="dv">9</span><span class="op">:</span><span class="dv">100</span>);</span>
<span id="cb11-32"><a href="ChapFrequency-Modeling.html#cb11-32"></a>      <span class="co"># Evaluating the Likelihood at various values of n</span></span>
<span id="cb11-33"><a href="ChapFrequency-Modeling.html#cb11-33"></a>        ll&lt;-<span class="kw">sapply</span>(n,likbinm); </span>
<span id="cb11-34"><a href="ChapFrequency-Modeling.html#cb11-34"></a>      <span class="co"># Finding the MLE of n</span></span>
<span id="cb11-35"><a href="ChapFrequency-Modeling.html#cb11-35"></a>        n[ll<span class="op">==</span><span class="kw">max</span>(ll[<span class="op">!</span><span class="kw">is.na</span>(ll)])] </span>
<span id="cb11-36"><a href="ChapFrequency-Modeling.html#cb11-36"></a>      <span class="co"># Storing the data and the labels</span></span>
<span id="cb11-37"><a href="ChapFrequency-Modeling.html#cb11-37"></a>        Y&lt;-<span class="kw">rbind</span>(Y,<span class="kw">cbind</span>(n,ll));</span>
<span id="cb11-38"><a href="ChapFrequency-Modeling.html#cb11-38"></a>        Z&lt;-<span class="kw">rbind</span>(Z,<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">sigma}^2&quot;</span>,ord_char[i],<span class="st">&quot;</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">mu}$&quot;</span>),<span class="kw">length</span>(n)),<span class="kw">rep</span>(<span class="st">&quot;Binomial - $L(m,</span><span class="ch">\\</span><span class="st">overline{x}/m)$&quot;</span>,<span class="kw">length</span>(n))));</span>
<span id="cb11-39"><a href="ChapFrequency-Modeling.html#cb11-39"></a></span>
<span id="cb11-40"><a href="ChapFrequency-Modeling.html#cb11-40"></a>  <span class="co"># Negative Binomial Likelihood</span></span>
<span id="cb11-41"><a href="ChapFrequency-Modeling.html#cb11-41"></a>    <span class="co"># Interval of r values </span></span>
<span id="cb11-42"><a href="ChapFrequency-Modeling.html#cb11-42"></a>      r&lt;-(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>);</span>
<span id="cb11-43"><a href="ChapFrequency-Modeling.html#cb11-43"></a>    <span class="co"># Evaluating the Likelihood at various values of r</span></span>
<span id="cb11-44"><a href="ChapFrequency-Modeling.html#cb11-44"></a>      ll&lt;-<span class="kw">sapply</span>(r,liknbinm); </span>
<span id="cb11-45"><a href="ChapFrequency-Modeling.html#cb11-45"></a>    <span class="co"># Finding the MLE of r</span></span>
<span id="cb11-46"><a href="ChapFrequency-Modeling.html#cb11-46"></a>      ll[<span class="kw">is.na</span>(ll)]=<span class="dv">0</span>;</span>
<span id="cb11-47"><a href="ChapFrequency-Modeling.html#cb11-47"></a>      r[ll<span class="op">==</span><span class="kw">max</span>(ll[<span class="op">!</span><span class="kw">is.na</span>(ll)])]; </span>
<span id="cb11-48"><a href="ChapFrequency-Modeling.html#cb11-48"></a>    <span class="co"># Storing the data and the labels</span></span>
<span id="cb11-49"><a href="ChapFrequency-Modeling.html#cb11-49"></a>      Y&lt;-<span class="kw">rbind</span>(Y,<span class="kw">cbind</span>(r,ll));</span>
<span id="cb11-50"><a href="ChapFrequency-Modeling.html#cb11-50"></a>      Z&lt;-<span class="kw">rbind</span>(Z,<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">sigma}^2&quot;</span>,ord_char[i],<span class="st">&quot;</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">mu}$&quot;</span>),<span class="kw">length</span>(r)),<span class="kw">rep</span>(<span class="st">&quot;Neg.Binomial - $L(r,</span><span class="ch">\\</span><span class="st">overline{x}/r)$&quot;</span>,<span class="kw">length</span>(r))));</span>
<span id="cb11-51"><a href="ChapFrequency-Modeling.html#cb11-51"></a>      </span>
<span id="cb11-52"><a href="ChapFrequency-Modeling.html#cb11-52"></a>  <span class="co"># Poisson Likelihood</span></span>
<span id="cb11-53"><a href="ChapFrequency-Modeling.html#cb11-53"></a>    <span class="co"># Storing the data and the labels</span></span>
<span id="cb11-54"><a href="ChapFrequency-Modeling.html#cb11-54"></a>    <span class="co"># In the Poisson case MLE is the sample mean</span></span>
<span id="cb11-55"><a href="ChapFrequency-Modeling.html#cb11-55"></a>      Y&lt;-<span class="kw">rbind</span>(Y,<span class="kw">cbind</span>(r,<span class="kw">rep</span>(<span class="kw">prod</span>(<span class="kw">dpois</span>(x,<span class="kw">mean</span>(x))),<span class="kw">length</span>(r))));</span>
<span id="cb11-56"><a href="ChapFrequency-Modeling.html#cb11-56"></a>      Z&lt;-<span class="kw">rbind</span>(Z,<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">sigma}^2&quot;</span>,ord_char[i],<span class="st">&quot;</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">mu}$&quot;</span>),<span class="kw">length</span>(r)),<span class="kw">rep</span>(<span class="st">&quot;Poisson - $L(</span><span class="ch">\\</span><span class="st">overline{x})$&quot;</span>,<span class="kw">length</span>(r))));</span>
<span id="cb11-57"><a href="ChapFrequency-Modeling.html#cb11-57"></a>}</span>
<span id="cb11-58"><a href="ChapFrequency-Modeling.html#cb11-58"></a></span>
<span id="cb11-59"><a href="ChapFrequency-Modeling.html#cb11-59"></a>  <span class="co"># Assigning Column Names</span></span>
<span id="cb11-60"><a href="ChapFrequency-Modeling.html#cb11-60"></a>    <span class="kw">colnames</span>(Y)&lt;-<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;lik&quot;</span>);</span>
<span id="cb11-61"><a href="ChapFrequency-Modeling.html#cb11-61"></a>    <span class="kw">colnames</span>(Z)&lt;-<span class="kw">c</span>(<span class="st">&quot;dataset&quot;</span>,<span class="st">&quot;Distribution&quot;</span>);</span>
<span id="cb11-62"><a href="ChapFrequency-Modeling.html#cb11-62"></a>  <span class="co"># Creating a Dataframe for using ggplot</span></span>
<span id="cb11-63"><a href="ChapFrequency-Modeling.html#cb11-63"></a>    dy&lt;-<span class="kw">cbind</span>(<span class="kw">data.frame</span>(Y),<span class="kw">data.frame</span>(Z));</span>
<span id="cb11-64"><a href="ChapFrequency-Modeling.html#cb11-64"></a></span>
<span id="cb11-65"><a href="ChapFrequency-Modeling.html#cb11-65"></a></span>
<span id="cb11-66"><a href="ChapFrequency-Modeling.html#cb11-66"></a><span class="kw">library</span>(tikzDevice);</span>
<span id="cb11-67"><a href="ChapFrequency-Modeling.html#cb11-67"></a><span class="kw">library</span>(ggplot2);</span>
<span id="cb11-68"><a href="ChapFrequency-Modeling.html#cb11-68"></a><span class="kw">options</span>(<span class="dt">tikzMetricPackages =</span> <span class="kw">c</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage[utf8]{inputenc}&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage[T1]{fontenc}&quot;</span>, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">usetikzlibrary{calc}&quot;</span>, </span>
<span id="cb11-69"><a href="ChapFrequency-Modeling.html#cb11-69"></a>                               <span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage{amssymb}&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage{amsmath}&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">usepackage[active]{preview}&quot;</span>))</span>
<span id="cb11-70"><a href="ChapFrequency-Modeling.html#cb11-70"></a><span class="kw">tikz</span>(<span class="dt">file =</span> <span class="st">&quot;plot_test_2.tex&quot;</span>, <span class="dt">width =</span> <span class="fl">6.25</span>, <span class="dt">height =</span> <span class="fl">6.25</span>);</span>
<span id="cb11-71"><a href="ChapFrequency-Modeling.html#cb11-71"></a><span class="kw">ggplot</span>(<span class="dt">data=</span>dy,<span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>lik,<span class="dt">col=</span>Distribution)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.25</span>) <span class="op">+</span><span class="st"> </span><span class="kw">facet_grid</span>(dataset<span class="op">~</span>.)<span class="op">+</span></span>
<span id="cb11-72"><a href="ChapFrequency-Modeling.html#cb11-72"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;m/r&quot;</span>,<span class="dt">y=</span><span class="st">&quot;Likelihood&quot;</span>,<span class="dt">title=</span><span class="st">&quot;&quot;</span>); </span>
<span id="cb11-73"><a href="ChapFrequency-Modeling.html#cb11-73"></a><span class="kw">dev.off</span>();</span></code></pre></div>
</div>
<hr />

</div>
</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references">
<div id="ref-billingsley">
<p>Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-frees2009regression">
<p>Frees, Edward W. 2009. <em>Regression Modeling with Actuarial and Financial Applications</em>. Cambridge University Press.</p>
</div>
<div id="ref-zimmerman2015">
<p>Hogg, Robert V, Elliot A Tanis, and Dale L Zimmerman. 2015. <em>Probability and Statistical Inference, 9th Edition</em>. Pearson, New York.</p>
</div>
<div id="ref-levin1977">
<p>Levin, Bruce, James Reeds, and others. 1977. “Compound Multinomial Likelihood Functions Are Unimodal: Proof of a Conjecture of Ij Good.” <em>The Annals of Statistics</em> 5 (1): 79–87.</p>
</div>
<div id="ref-olkin1981">
<p>Olkin, Ingram, A John Petkau, and James V Zidek. 1981. “A Comparison of N Estimators for the Binomial Distribution.” <em>Journal of the American Statistical Association</em> 76 (375): 637–42.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>For convenience, we have indexed <span class="math inline">\(\mu_N\)</span> with the random variable <span class="math inline">\(N\)</span> instead of <span class="math inline">\(F_N\)</span> or <span class="math inline">\(p_N\)</span>, even though it is solely a function of the distribution of the random variable.<a href="ChapFrequency-Modeling.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><span class="math inline">\(0^0 = 1\)</span><a href="ChapFrequency-Modeling.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In the following we suppress the reference to <span class="math inline">\(N\)</span> and denote the <em>pmf</em> by the sequence <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span>, instead of the function <span class="math inline">\(p_N(\cdot)\)</span>.<a href="ChapFrequency-Modeling.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>For the theoretical basis underlying the above argument, see <span class="citation">Billingsley (<a href="#ref-billingsley" role="doc-biblioref">2008</a>)</span>.<a href="ChapFrequency-Modeling.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>The set of maximizers of <span class="math inline">\(L(\cdot)\)</span> are the same as the set of maximizers of any strictly increasing function of <span class="math inline">\(L(\cdot)\)</span>, and hence the same as those for <span class="math inline">\(l(\cdot)\)</span>.<a href="ChapFrequency-Modeling.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>A slight benefit of working with <span class="math inline">\(l(\cdot)\)</span> is that constant terms in <span class="math inline">\(L(\cdot)\)</span> do not appear in <span class="math inline">\(l&#39;(\cdot)\)</span> whereas they do in <span class="math inline">\(L&#39;(\cdot)\)</span>.<a href="ChapFrequency-Modeling.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>This in particular lays out a way to simulate from a mixture distribution that makes use of efficient simulation schemes that may exist for the component distributions.<a href="ChapFrequency-Modeling.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ChapIntro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ChapSeverity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
