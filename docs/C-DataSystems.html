<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Data and Systems | Loss Data Analytics</title>
  <meta name="description" content="Chapter 13 Data and Systems | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Data and Systems | Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 13 Data and Systems | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/openacttexts/Loss-Data-Analytics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Data and Systems | Loss Data Analytics" />
  
  <meta name="twitter:description" content="Chapter 13 Data and Systems | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C-BonusMalus.html">
<link rel="next" href="C-DependenceModel.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<!-- Mathjax -->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<!-- The following code is for the quizzes -->
<script src="https://surveyjs.azureedge.net/1.0.50/survey.jquery.js"></script>
<link href="https://surveyjs.azureedge.net/1.0.50/survey.css" type="text/css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.6.4/showdown.min.js"></script>  

<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the mardown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
         MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};
// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}
// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};



</script>  
<!-- This completes the code for the quizzes -->


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="C-Intro.html"><a href="C-Intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="C-Intro.html"><a href="C-Intro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics to Insurance Activities</a><ul>
<li class="chapter" data-level="1.1.1" data-path="C-Intro.html"><a href="C-Intro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="C-Intro.html"><a href="C-Intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.2</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="C-Intro.html"><a href="C-Intro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="C-Intro.html"><a href="C-Intro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="C-Intro.html"><a href="C-Intro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="C-Intro.html"><a href="C-Intro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="C-Intro.html"><a href="C-Intro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.3.1" data-path="C-Intro.html"><a href="C-Intro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="C-Intro.html"><a href="C-Intro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="C-Intro.html"><a href="C-Intro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="C-Intro.html"><a href="C-Intro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html"><i class="fa fa-check"></i><b>2</b> Frequency Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>2.1</b> Frequency Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Basic Frequency Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>2.2.1</b> Foundations</a></li>
<li class="chapter" data-level="2.2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>2.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> The (a, b, 0) Class</a></li>
<li class="chapter" data-level="2.4" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimating Frequency Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>2.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Frequency Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="2.7" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="2.8" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>2.9</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="2.9.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>2.9.1</b> TS 2.A. R Code for Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C-Severity.html"><a href="C-Severity.html"><i class="fa fa-check"></i><b>3</b> Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.1" data-path="C-Severity.html"><a href="C-Severity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Basic Distributional Quantities</a><ul>
<li class="chapter" data-level="3.1.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>3.1.1</b> Moments</a></li>
<li class="chapter" data-level="3.1.2" data-path="C-Severity.html"><a href="C-Severity.html#quantiles"><i class="fa fa-check"></i><b>3.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="C-Severity.html"><a href="C-Severity.html#moment-generating-function"><i class="fa fa-check"></i><b>3.1.3</b> Moment Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C-Severity.html"><a href="C-Severity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions for Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Loss:Gamma"><i class="fa fa-check"></i><b>3.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="C-Severity.html"><a href="C-Severity.html#pareto-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="C-Severity.html"><a href="C-Severity.html#S:LS:Weibull"><i class="fa fa-check"></i><b>3.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="C-Severity.html"><a href="C-Severity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>3.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C-Severity.html"><a href="C-Severity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Methods of Creating New Distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="C-Severity.html"><a href="C-Severity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="3.3.2" data-path="C-Severity.html"><a href="C-Severity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>3.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="3.3.3" data-path="C-Severity.html"><a href="C-Severity.html#raising-to-a-power"><i class="fa fa-check"></i><b>3.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="3.3.4" data-path="C-Severity.html"><a href="C-Severity.html#exponentiation"><i class="fa fa-check"></i><b>3.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="3.3.5" data-path="C-Severity.html"><a href="C-Severity.html#finite-mixtures"><i class="fa fa-check"></i><b>3.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="3.3.6" data-path="C-Severity.html"><a href="C-Severity.html#continuous-mixtures"><i class="fa fa-check"></i><b>3.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="C-Severity.html"><a href="C-Severity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>3.4</b> Coverage Modifications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="3.4.2" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="3.4.3" data-path="C-Severity.html"><a href="C-Severity.html#coinsurance-and-inflation"><i class="fa fa-check"></i><b>3.4.3</b> Coinsurance and Inflation</a></li>
<li class="chapter" data-level="3.4.4" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C-Severity.html"><a href="C-Severity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>3.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="3.5.2" data-path="C-Severity.html"><a href="C-Severity.html#S:Loss:MLEModified"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Likelihood Estimators using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C-Severity.html"><a href="C-Severity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>3.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html"><i class="fa fa-check"></i><b>4</b> Model Selection and Estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation"><i class="fa fa-check"></i><b>4.1.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.1.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>4.1.2</b> Tools for Model Selection and Diagnostics</a></li>
<li class="chapter" data-level="4.1.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#starting-values"><i class="fa fa-check"></i><b>4.1.3</b> Starting Values</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>4.2</b> Model Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#iterative-model-selection"><i class="fa fa-check"></i><b>4.2.1</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="4.2.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-training-dataset"><i class="fa fa-check"></i><b>4.2.2</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="4.2.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>4.2.3</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="4.2.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-cross-validation"><i class="fa fa-check"></i><b>4.2.4</b> Model Selection Based on Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>4.3</b> Estimation using Modified Data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#parametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.1</b> Parametric Estimation using Modified Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.2</b> Nonparametric Estimation using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:BayesInference"><i class="fa fa-check"></i><b>4.4</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:IntroBayes"><i class="fa fa-check"></i><b>4.4.1</b> Introduction to Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-model"><i class="fa fa-check"></i><b>4.4.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-inference"><i class="fa fa-check"></i><b>4.4.3</b> Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>4.4.4</b> Conjugate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html"><i class="fa fa-check"></i><b>5</b> Aggregate Loss Models</a><ul>
<li class="chapter" data-level="5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>5.2</b> Individual Risk Model</a></li>
<li class="chapter" data-level="5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#collective-risk-model"><i class="fa fa-check"></i><b>5.3</b> Collective Risk Model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>5.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>5.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="5.3.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#analytic-results"><i class="fa fa-check"></i><b>5.3.3</b> Analytic Results</a></li>
<li class="chapter" data-level="5.3.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#tweedie-distribution"><i class="fa fa-check"></i><b>5.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>5.4</b> Computing the Aggregate Claims Distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>5.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="5.4.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#simulation"><i class="fa fa-check"></i><b>5.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>5.5</b> Effects of Coverage Modifications</a><ul>
<li class="chapter" data-level="5.5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>5.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="5.5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>5.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="5.5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>5.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>5.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#ts-5.a.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS 5.A.1. Individual Risk Model Properties</a></li>
<li><a href="C-AggLossModels.html#ts-5.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it">TS 5.A.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li><a href="C-AggLossModels.html#ts-5.a.3.-example-5.3.8-moment-generating-function-of-aggregate-loss-s_n">TS 5.A.3. Example 5.3.8 Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="C-Simulation.html"><a href="C-Simulation.html"><i class="fa fa-check"></i><b>6</b> Simulation and Resampling</a><ul>
<li class="chapter" data-level="6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#S:SimulationFundamentals"><i class="fa fa-check"></i><b>6.1</b> Simulation Fundamentals</a><ul>
<li class="chapter" data-level="6.1.1" data-path="C-Simulation.html"><a href="C-Simulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="6.1.2" data-path="C-Simulation.html"><a href="C-Simulation.html#S:InverseTransform"><i class="fa fa-check"></i><b>6.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="6.1.3" data-path="C-Simulation.html"><a href="C-Simulation.html#simulation-precision"><i class="fa fa-check"></i><b>6.1.3</b> Simulation Precision</a></li>
<li class="chapter" data-level="6.1.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:SimulationStatInference"><i class="fa fa-check"></i><b>6.1.4</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="C-Simulation.html"><a href="C-Simulation.html#S:Bootstrap"><i class="fa fa-check"></i><b>6.2</b> Bootstrapping and Resampling</a><ul>
<li class="chapter" data-level="6.2.1" data-path="C-Simulation.html"><a href="C-Simulation.html#bootstrap-foundations"><i class="fa fa-check"></i><b>6.2.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="6.2.2" data-path="C-Simulation.html"><a href="C-Simulation.html#bootstrap-precision-bias-standard-deviation-and-mse"><i class="fa fa-check"></i><b>6.2.2</b> Bootstrap Precision: Bias, Standard Deviation, and MSE</a></li>
<li class="chapter" data-level="6.2.3" data-path="C-Simulation.html"><a href="C-Simulation.html#confidence-intervals"><i class="fa fa-check"></i><b>6.2.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.2.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:ParametricBootStrap"><i class="fa fa-check"></i><b>6.2.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="C-Simulation.html"><a href="C-Simulation.html#S:CrossValidation"><i class="fa fa-check"></i><b>6.3</b> Cross-Validation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="C-Simulation.html"><a href="C-Simulation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.3.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="6.3.2" data-path="C-Simulation.html"><a href="C-Simulation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="6.3.3" data-path="C-Simulation.html"><a href="C-Simulation.html#cross-validation-and-bootstrap"><i class="fa fa-check"></i><b>6.3.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:ImportanceSampling"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.5" data-path="C-Simulation.html"><a href="C-Simulation.html#S:MCMC"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo Markov Chain (MCMC)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="C-Simulation.html"><a href="C-Simulation.html#hastings-metropolis"><i class="fa fa-check"></i><b>6.5.1</b> Hastings Metropolis</a></li>
<li class="chapter" data-level="6.5.2" data-path="C-Simulation.html"><a href="C-Simulation.html#gibbs-sampler"><i class="fa fa-check"></i><b>6.5.2</b> Gibbs sampler</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="C-Simulation.html"><a href="C-Simulation.html#Simulation:further-reading-and-resources"><i class="fa fa-check"></i><b>6.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="6.6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#ts-6.a.-bootstrap-applications-in-predictive-modeling"><i class="fa fa-check"></i><b>6.6.1</b> TS 6.A. Bootstrap Applications in Predictive Modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html"><i class="fa fa-check"></i><b>7</b> Premium Foundations</a><ul>
<li class="chapter" data-level="7.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:IntroductionRatemaking"><i class="fa fa-check"></i><b>7.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="7.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:AggRateMaking"><i class="fa fa-check"></i><b>7.2</b> Aggregate Ratemaking Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:PurePremium"><i class="fa fa-check"></i><b>7.2.1</b> Pure Premium Method</a></li>
<li class="chapter" data-level="7.2.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:LossRatio"><i class="fa fa-check"></i><b>7.2.2</b> Loss Ratio Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:PricingPrinciples"><i class="fa fa-check"></i><b>7.3</b> Pricing Principles</a><ul>
<li class="chapter" data-level="7.3.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#premium-principles"><i class="fa fa-check"></i><b>7.3.1</b> Premium Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#properties-of-premium-principles"><i class="fa fa-check"></i><b>7.3.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:HeterogeneousRisks"><i class="fa fa-check"></i><b>7.4</b> Heterogeneous Risks</a><ul>
<li class="chapter" data-level="7.4.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:ExposureToRisk"><i class="fa fa-check"></i><b>7.4.1</b> Exposure to Risk</a></li>
<li class="chapter" data-level="7.4.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:RatingFactors"><i class="fa fa-check"></i><b>7.4.2</b> Rating Factors</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:TrendDevelopment"><i class="fa fa-check"></i><b>7.5</b> Development and Trending</a><ul>
<li class="chapter" data-level="7.5.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#exposures-and-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Exposures and Premiums</a></li>
<li class="chapter" data-level="7.5.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#losses-claims-and-payments"><i class="fa fa-check"></i><b>7.5.2</b> Losses, Claims, and Payments</a></li>
<li class="chapter" data-level="7.5.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:CompareMethods"><i class="fa fa-check"></i><b>7.5.3</b> Comparing Pure Premium and Loss Ratio Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:GiniStatistic"><i class="fa fa-check"></i><b>7.6</b> Selecting a Premium</a><ul>
<li class="chapter" data-level="7.6.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#classic-lorenz-curve"><i class="fa fa-check"></i><b>7.6.1</b> Classic Lorenz Curve</a></li>
<li class="chapter" data-level="7.6.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#performance-curve-and-a-gini-statistic"><i class="fa fa-check"></i><b>7.6.2</b> Performance Curve and a Gini Statistic</a></li>
<li class="chapter" data-level="7.6.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#out-of-sample-validation"><i class="fa fa-check"></i><b>7.6.3</b> Out-of-Sample Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>7.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#ts-7.a.-rate-regulation"><i class="fa fa-check"></i>TS 7.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C-RiskClass.html"><a href="C-RiskClass.html"><i class="fa fa-check"></i><b>8</b> Risk Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>8.2</b> Poisson Regression Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>8.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>8.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>8.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="8.2.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#exercises-3"><i class="fa fa-check"></i><b>8.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>8.3</b> Categorical Variables and Multiplicative Tariff</a><ul>
<li class="chapter" data-level="8.3.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>8.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="8.3.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>8.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="8.3.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>8.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="8.3.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>8.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>8.4</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ts-8.a.-estimating-poisson-regression-models"><i class="fa fa-check"></i>TS 8.A. Estimating Poisson Regression Models</a></li>
<li class="chapter" data-level="" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ts-8.b.-selecting-rating-factors"><i class="fa fa-check"></i>TS 8.B. Selecting Rating Factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C-Credibility.html"><a href="C-Credibility.html"><i class="fa fa-check"></i><b>9</b> Experience Rating Using Credibility Theory</a><ul>
<li class="chapter" data-level="9.1" data-path="C-Credibility.html"><a href="C-Credibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>9.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="9.2" data-path="C-Credibility.html"><a href="C-Credibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.2</b> Limited Fluctuation Credibility</a><ul>
<li class="chapter" data-level="9.2.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:frequency"><i class="fa fa-check"></i><b>9.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="9.2.2" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>9.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="9.2.3" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>9.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="9.2.4" data-path="C-Credibility.html"><a href="C-Credibility.html#partial-credibility"><i class="fa fa-check"></i><b>9.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-credibility"><i class="fa fa-check"></i><b>9.3</b> Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.3.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>9.3.1</b> Credibility Z, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-straub-credibility"><i class="fa fa-check"></i><b>9.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="9.5" data-path="C-Credibility.html"><a href="C-Credibility.html#bayesian-inference-and-buhlmann-credibility"><i class="fa fa-check"></i><b>9.5</b> Bayesian Inference and Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.5.1" data-path="C-Credibility.html"><a href="C-Credibility.html#gamma-poisson-model"><i class="fa fa-check"></i><b>9.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="9.5.2" data-path="C-Credibility.html"><a href="C-Credibility.html#beta-binomial-model"><i class="fa fa-check"></i><b>9.5.2</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="9.5.3" data-path="C-Credibility.html"><a href="C-Credibility.html#exact-credibility"><i class="fa fa-check"></i><b>9.5.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="C-Credibility.html"><a href="C-Credibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>9.6</b> Estimating Credibility Parameters</a><ul>
<li class="chapter" data-level="9.6.1" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="9.6.2" data-path="C-Credibility.html"><a href="C-Credibility.html#nonparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.2</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.3" data-path="C-Credibility.html"><a href="C-Credibility.html#semiparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.3</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.4" data-path="C-Credibility.html"><a href="C-Credibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>9.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="C-Credibility.html"><a href="C-Credibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C-PortMgt.html"><a href="C-PortMgt.html"><i class="fa fa-check"></i><b>10</b> Insurance Portfolio Management including Reinsurance</a><ul>
<li class="chapter" data-level="10.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#introduction-to-insurance-portfolios"><i class="fa fa-check"></i><b>10.1</b> Introduction to Insurance Portfolios</a></li>
<li class="chapter" data-level="10.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Tails"><i class="fa fa-check"></i><b>10.2</b> Tails of Distributions</a><ul>
<li class="chapter" data-level="10.2.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>10.2.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="10.2.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>10.2.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>10.3</b> Risk Measures</a><ul>
<li class="chapter" data-level="10.3.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>10.3.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="10.3.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>10.3.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="10.3.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>10.3.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>10.4</b> Reinsurance</a><ul>
<li class="chapter" data-level="10.4.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>10.4.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.4.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>10.4.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.4.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>10.4.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="C-PortMgt.html"><a href="C-PortMgt.html#further-resources-and-contributors-1"><i class="fa fa-check"></i><b>10.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C-LossReserves.html"><a href="C-LossReserves.html"><i class="fa fa-check"></i><b>11</b> Loss Reserving</a><ul>
<li class="chapter" data-level="11.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:motivation"><i class="fa fa-check"></i><b>11.1</b> Motivation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:claim-types"><i class="fa fa-check"></i><b>11.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="11.1.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#why-reserving"><i class="fa fa-check"></i><b>11.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Data"><i class="fa fa-check"></i><b>11.2</b> Loss Reserve Data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#from-micro-to-macro"><i class="fa fa-check"></i><b>11.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="11.2.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#run-off-triangles"><i class="fa fa-check"></i><b>11.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="11.2.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#loss-reserve-notation"><i class="fa fa-check"></i><b>11.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Rcode"><i class="fa fa-check"></i><b>11.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Chain-ladder"><i class="fa fa-check"></i><b>11.3</b> The Chain-Ladder</a><ul>
<li class="chapter" data-level="11.3.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:DeterministicCL"><i class="fa fa-check"></i><b>11.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="11.3.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#macks-distribution-free-chain-ladder-model"><i class="fa fa-check"></i><b>11.3.2</b> Mack’s Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="11.3.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#r-code-for-chain-ladder-predictions"><i class="fa fa-check"></i><b>11.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:GLMs"><i class="fa fa-check"></i><b>11.4</b> GLMs and Bootstrap for Loss Reserves</a><ul>
<li class="chapter" data-level="11.4.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#model-specification"><i class="fa fa-check"></i><b>11.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="11.4.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>11.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="11.4.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#bootstrap"><i class="fa fa-check"></i><b>11.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="C-LossReserves.html"><a href="C-LossReserves.html#LossRe:further-reading-and-resources"><i class="fa fa-check"></i><b>11.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html"><i class="fa fa-check"></i><b>12</b> Experience Rating using Bonus-Malus</a><ul>
<li class="chapter" data-level="12.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:Intro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:NCD"><i class="fa fa-check"></i><b>12.2</b> NCD System in Several Countries</a><ul>
<li class="chapter" data-level="12.2.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#ncd-system-in-malaysia"><i class="fa fa-check"></i><b>12.2.1</b> NCD System in Malaysia</a></li>
<li class="chapter" data-level="12.2.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#ncd-system-in-other-countries"><i class="fa fa-check"></i><b>12.2.2</b> NCD System in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:BMS"><i class="fa fa-check"></i><b>12.3</b> BMS and Markov Chain Model</a><ul>
<li class="chapter" data-level="12.3.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>12.3.1</b> Transition Probability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:StatDist"><i class="fa fa-check"></i><b>12.4</b> BMS and Stationary Distribution</a><ul>
<li class="chapter" data-level="12.4.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>12.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-stationary-distribution"><i class="fa fa-check"></i><b>12.4.2</b> R Program for Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>12.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="12.4.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-premium-evolution"><i class="fa fa-check"></i><b>12.4.4</b> R Program for Premium Evolution</a></li>
<li class="chapter" data-level="12.4.5" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>12.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="12.4.6" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-convergence-rate"><i class="fa fa-check"></i><b>12.4.6</b> R Program for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:PremRtg"><i class="fa fa-check"></i><b>12.5</b> BMS and Premium Rating</a><ul>
<li class="chapter" data-level="12.5.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>12.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="12.5.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#frequency-model-poisson-and-negative-binomial-regressions"><i class="fa fa-check"></i><b>12.5.2</b> Frequency Model – Poisson and Negative Binomial Regressions</a></li>
<li class="chapter" data-level="12.5.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating-with-bonus-malus-data"><i class="fa fa-check"></i><b>12.5.3</b> Premium Rating with Bonus-Malus Data</a></li>
<li class="chapter" data-level="12.5.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating-without-bonus-malus-data"><i class="fa fa-check"></i><b>12.5.4</b> Premium Rating without Bonus-Malus Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C-DataSystems.html"><a href="C-DataSystems.html"><i class="fa fa-check"></i><b>13</b> Data and Systems</a><ul>
<li class="chapter" data-level="13.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data"><i class="fa fa-check"></i><b>13.1</b> Data</a><ul>
<li class="chapter" data-level="13.1.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-types-and-sources"><i class="fa fa-check"></i><b>13.1.1</b> Data Types and Sources</a></li>
<li class="chapter" data-level="13.1.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-structures-and-storage"><i class="fa fa-check"></i><b>13.1.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="13.1.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-quality"><i class="fa fa-check"></i><b>13.1.3</b> Data Quality</a></li>
<li class="chapter" data-level="13.1.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-cleaning"><i class="fa fa-check"></i><b>13.1.4</b> Data Cleaning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-preliminaries"><i class="fa fa-check"></i><b>13.2</b> Data Analysis Preliminaries</a><ul>
<li class="chapter" data-level="13.2.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:process"><i class="fa fa-check"></i><b>13.2.1</b> Data Analysis Process</a></li>
<li class="chapter" data-level="13.2.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-versus-confirmatory"><i class="fa fa-check"></i><b>13.2.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="13.2.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>13.2.3</b> Supervised versus Unsupervised</a></li>
<li class="chapter" data-level="13.2.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#parametric-versus-nonparametric"><i class="fa fa-check"></i><b>13.2.4</b> Parametric versus Nonparametric</a></li>
<li class="chapter" data-level="13.2.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:expred"><i class="fa fa-check"></i><b>13.2.5</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="13.2.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-modeling-versus-algorithmic-modeling"><i class="fa fa-check"></i><b>13.2.6</b> Data Modeling versus Algorithmic Modeling</a></li>
<li class="chapter" data-level="13.2.7" data-path="C-DataSystems.html"><a href="C-DataSystems.html#big-data-analysis"><i class="fa fa-check"></i><b>13.2.7</b> Big Data Analysis</a></li>
<li class="chapter" data-level="13.2.8" data-path="C-DataSystems.html"><a href="C-DataSystems.html#reproducible-analysis"><i class="fa fa-check"></i><b>13.2.8</b> Reproducible Analysis</a></li>
<li class="chapter" data-level="13.2.9" data-path="C-DataSystems.html"><a href="C-DataSystems.html#ethical-issues"><i class="fa fa-check"></i><b>13.2.9</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-techniques"><i class="fa fa-check"></i><b>13.3</b> Data Analysis Techniques</a><ul>
<li class="chapter" data-level="13.3.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-techniques"><i class="fa fa-check"></i><b>13.3.1</b> Exploratory Techniques</a></li>
<li class="chapter" data-level="13.3.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#confirmatory-techniques"><i class="fa fa-check"></i><b>13.3.2</b> Confirmatory Techniques</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#some-r-functions"><i class="fa fa-check"></i><b>13.4</b> Some R Functions</a></li>
<li class="chapter" data-level="13.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html"><i class="fa fa-check"></i><b>14</b> Dependence Modeling</a><ul>
<li class="chapter" data-level="14.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:VarTypes"><i class="fa fa-check"></i><b>14.1</b> Variable Types</a><ul>
<li class="chapter" data-level="14.1.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuaVar"><i class="fa fa-check"></i><b>14.1.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuanVar"><i class="fa fa-check"></i><b>14.1.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>14.1.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>14.2</b> Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>14.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>14.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>14.2.3</b> Nominal Variables</a></li>
<li class="chapter" data-level="14.2.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#ordinal-variables"><i class="fa fa-check"></i><b>14.2.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="14.2.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#interval-variables"><i class="fa fa-check"></i><b>14.2.5</b> Interval Variables</a></li>
<li class="chapter" data-level="14.2.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#discrete-and-continuous-variables"><i class="fa fa-check"></i><b>14.2.6</b> Discrete and Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>14.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="14.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>14.4</b> Application Using Copulas</a><ul>
<li class="chapter" data-level="14.4.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#data-description"><i class="fa fa-check"></i><b>14.4.1</b> Data Description</a></li>
<li class="chapter" data-level="14.4.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>14.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="14.4.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>14.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="14.4.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>14.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>14.5</b> Types of Copulas</a><ul>
<li class="chapter" data-level="14.5.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#elliptical-copulas"><i class="fa fa-check"></i><b>14.5.1</b> Elliptical Copulas</a></li>
<li class="chapter" data-level="14.5.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#archimedian-copulas"><i class="fa fa-check"></i><b>14.5.2</b> Archimedian Copulas</a></li>
<li class="chapter" data-level="14.5.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>14.5.3</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>14.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="14.7" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>14.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#ts-14.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 14.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C-AppA.html"><a href="C-AppA.html"><i class="fa fa-check"></i><b>15</b> Appendix A: Review of Statistical Inference</a><ul>
<li class="chapter" data-level="15.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>15.1</b> Basic Concepts</a><ul>
<li class="chapter" data-level="15.1.1" data-path="C-AppA.html"><a href="C-AppA.html#random-sampling"><i class="fa fa-check"></i><b>15.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="15.1.2" data-path="C-AppA.html"><a href="C-AppA.html#sampling-distribution"><i class="fa fa-check"></i><b>15.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="15.1.3" data-path="C-AppA.html"><a href="C-AppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>15.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>15.2</b> Point Estimation and Properties</a><ul>
<li class="chapter" data-level="15.2.1" data-path="C-AppA.html"><a href="C-AppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>15.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="15.2.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>15.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>15.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="15.3.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>15.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="15.3.2" data-path="C-AppA.html"><a href="C-AppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>15.3.2</b> Large-sample Properties of MLE</a></li>
<li class="chapter" data-level="15.3.3" data-path="C-AppA.html"><a href="C-AppA.html#confidence-interval"><i class="fa fa-check"></i><b>15.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>15.4</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="15.4.1" data-path="C-AppA.html"><a href="C-AppA.html#basic-concepts"><i class="fa fa-check"></i><b>15.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="15.4.2" data-path="C-AppA.html"><a href="C-AppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>15.4.2</b> Student-<span class="math inline">\(t\)</span> test based on MLE</a></li>
<li class="chapter" data-level="15.4.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>15.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="15.4.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>15.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C-AppB.html"><a href="C-AppB.html"><i class="fa fa-check"></i><b>16</b> Appendix B: Iterated Expectations</a><ul>
<li class="chapter" data-level="16.1" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>16.1</b> Conditional Distribution and Conditional Expectation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="C-AppB.html"><a href="C-AppB.html#conditional-distribution"><i class="fa fa-check"></i><b>16.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="16.1.2" data-path="C-AppB.html"><a href="C-AppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>16.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>16.2</b> Iterated Expectations and Total Variance</a><ul>
<li class="chapter" data-level="16.2.1" data-path="C-AppB.html"><a href="C-AppB.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>16.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="16.2.2" data-path="C-AppB.html"><a href="C-AppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>16.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="C-AppB.html"><a href="C-AppB.html#application"><i class="fa fa-check"></i><b>16.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="C-AppB.html"><a href="C-AppB.html#S:AppConjugateDistributions"><i class="fa fa-check"></i><b>16.3</b> Conjugate Distributions</a><ul>
<li class="chapter" data-level="16.3.1" data-path="C-AppB.html"><a href="C-AppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>16.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="16.3.2" data-path="C-AppB.html"><a href="C-AppB.html#conjugate-distributions"><i class="fa fa-check"></i><b>16.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C-AppC.html"><a href="C-AppC.html"><i class="fa fa-check"></i><b>17</b> Appendix C: Maximum Likelihood Theory</a><ul>
<li class="chapter" data-level="17.1" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>17.1</b> Likelihood Function</a><ul>
<li class="chapter" data-level="17.1.1" data-path="C-AppC.html"><a href="C-AppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="17.1.2" data-path="C-AppC.html"><a href="C-AppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>17.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>17.2</b> Maximum Likelihood Estimators</a><ul>
<li class="chapter" data-level="17.2.1" data-path="C-AppC.html"><a href="C-AppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>17.2.1</b> Definition and Derivation of MLE</a></li>
<li class="chapter" data-level="17.2.2" data-path="C-AppC.html"><a href="C-AppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotic Properties of MLE</a></li>
<li class="chapter" data-level="17.2.3" data-path="C-AppC.html"><a href="C-AppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>17.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>17.3</b> Statistical Inference Based on Maximum Likelhood Estimation</a><ul>
<li class="chapter" data-level="17.3.1" data-path="C-AppC.html"><a href="C-AppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>17.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="17.3.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>17.3.2</b> MLE and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html"><i class="fa fa-check"></i><b>18</b> Appendix D: Summary of Distributions</a><ul>
<li class="chapter" data-level="18.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#discrete-distributions"><i class="fa fa-check"></i><b>18.1</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="18.1.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>18.1.1</b> The (a,b,0) Class</a></li>
<li class="chapter" data-level="18.1.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>18.1.2</b> The (a,b,1) Class</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#continuous-distributions"><i class="fa fa-check"></i><b>18.2</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>18.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>18.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.3" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>18.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.4" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>18.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="18.2.5" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>18.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="18.2.6" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>18.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>18.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C:DataSystems" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Data and Systems</h1>
<p><em>Chapter Preview</em>. This chapter covers the learning areas on data and systems outlined in the IAA (International Actuarial Association) Education Syllabus published in September 2015. This chapter is organized into three major parts: data, data analysis, and data analysis techniques. The first part introduces data basics such as data types, data structures, data storages, and data sources. The second part discusses the process and various aspects of data analysis. The third part presents some commonly used techniques for data analysis.</p>
<div id="data" class="section level2">
<h2><span class="header-section-number">13.1</span> Data</h2>
<!-- % 7.1.3 - 7.1.6 data source, structure, storage, quality, preprocessing tools  inmon2014 -->
<!-- % 7.4.1 - 7.4.3 ethical (miles2014), governance, risks -->
<!-- % -->
<div id="data-types-and-sources" class="section level3">
<h3><span class="header-section-number">13.1.1</span> Data Types and Sources</h3>
<p>In terms of how data are collected, data can be divided into two types <span class="citation">(Hox and Boeije <a href="#ref-hox2005data">2005</a>)</span>: primary data and secondary data. Primary data are original data that are collected for a specific research problem. Secondary data are data originally collected for a different purpose and reused for another research problem. A major advantage of using primary data is that the theoretical constructs, the research design, and the data collection strategy can be tailored to the underlying research question to ensure that the data collected indeed help to solve the problem. A disadvantage of using primary data is that data collection can be costly and time-consuming. Using secondary data has the advantage of lower cost and faster access to relevant information. However, using secondary data may not be optimal for the research question under consideration.</p>
<p>In terms of the degree of organization of the data, data can be also divided into two types <span class="citation">(Inmon and Linstedt <a href="#ref-inmon2014">2014</a>; O’Leary <a href="#ref-leary2013bigdata">2013</a>; Hashem et al. <a href="#ref-hashem2015bigdata">2015</a>; Abdullah and Ahmad <a href="#ref-abdullah2013data">2013</a>; Pries and Dunnigan <a href="#ref-pries2015">2015</a>)</span>: structured data and unstructured data. <a href="#" class="tooltip" style="color:green"><em>Structured data</em><span style="font-size:8pt">Data that can be organized into a repository format, typically a database</span></a> have a predictable and regularly occurring format. In contrast, <a href="#" class="tooltip" style="color:green"><em>unstructured data</em><span style="font-size:8pt">Data that is not in a predefined format, most notably text, audio visual</span></a> are unpredictable and have no structure that is recognizable to a computer. Structured data consist of records, attributes, keys, and indices and are typically managed by a database management system (DBMS) such as IBM DB2, Oracle, MySQL, and Microsoft SQL Server. As a result, most units of structured data can be located quickly and easily. Unstructured data have many different forms and variations. One common form of unstructured data is text. Accessing unstructured data is clumsy. To find a given unit of data in a long text, for example, sequentially search is usually performed.</p>
<p>In terms of how the data are measured, data can be classified as qualitative or quantitative. <a href="#" class="tooltip" style="color:green"><em>Qualitative data</em><span style="font-size:8pt">Data which is non numerical in nature</span></a> are data about qualities, which cannot be actually measured. As a result, qualitative data are extremely varied in nature and include interviews, documents, and artifacts <span class="citation">(Miles, Hberman, and Sdana <a href="#ref-miles2014">2014</a>)</span>. <a href="#" class="tooltip" style="color:green"><em>Quantitative data</em><span style="font-size:8pt">Data which is numerical in nature</span></a> are data about quantities, which can be measured numerically with numbers. In terms of the level of measurement, quantitative data can be further classified as nominal, ordinal, interval, or ratio <span class="citation">(Gan <a href="#ref-gan2011">2011</a>)</span>. Nominal data, also called categorical data, are discrete data without a natural ordering. <a href="#" class="tooltip" style="color:green"><em>Ordinal data</em><span style="font-size:8pt">Data field with a natural ordering</span></a> are discrete data with a natural order. <a href="#" class="tooltip" style="color:green"><em>Interval data</em><span style="font-size:8pt">Continuous data which is broken into interval bands with a natural ordering</span></a> are continuous data with a specific order and equal intervals. Ratio data are interval data with a natural zero.</p>
<p>There exist a number of data sources. First, data can be obtained from university-based researchers who collect primary data. Second, data can be obtained from organizations that are set up for the purpose of releasing secondary data for general research community. Third, data can be obtained from national and regional statistical institutes that collect data. Finally, companies have corporate data that can be obtained for research purpose.</p>
<p>While it might be difficult to obtain data to address a specific research problem or answer a business question, it is relatively easy to obtain data to test a model or an algorithm for data analysis. In the modern era, readers can obtain datasets from the Internet easily. The following is a list of some websites to obtain real-world data:</p>
<ul>
<li><p><strong>UCI Machine Learning Repository</strong> This website (url: <a href="http://archive.ics.uci.edu/ml/index.php" class="uri">http://archive.ics.uci.edu/ml/index.php</a>) maintains more than 400 datasets that can be used to test machine learning algorithms.</p></li>
<li><p><strong>Kaggle</strong> The Kaggle website (url: <a href="https://www.kaggle.com/" class="uri">https://www.kaggle.com/</a>) include real-world datasets used for data science competition. Readers can download data from Kaggle by registering an account.</p></li>
<li><p><strong>DrivenData</strong> DrivenData aims at bringing cutting-edge practices in data science to solve some of the world’s biggest social challenges. In its website (url: <a href="https://www.drivendata.org/" class="uri">https://www.drivendata.org/</a>), readers can participate data science competitions and download datasets.</p></li>
<li><p><strong>Analytics Vidhya</strong> This website (url: <a href="https://datahack.analyticsvidhya.com/contest/all/" class="uri">https://datahack.analyticsvidhya.com/contest/all/</a>) allows you to participate and download datasets from practice problems and hackathon problems.</p></li>
<li><p><strong>KDD Cup</strong> KDD Cup is the annual Data Mining and Knowledge Discovery competition organized by ACM Special Interest Group on Knowledge Discovery and Data Mining. This website (url: <a href="http://www.kdd.org/kdd-cup" class="uri">http://www.kdd.org/kdd-cup</a>) contains the datasets used in past KDD Cup competitions since 1997.</p></li>
<li><p><strong>U.S. Government’s open data</strong> This website (url: <a href="https://www.data.gov/" class="uri">https://www.data.gov/</a>) contains about 200,000 datasets covering a wide range of areas including climate, education, energy, and finance.</p></li>
<li><p><strong>AWS Public Datasets</strong> In this website (url: <a href="https://aws.amazon.com/datasets/" class="uri">https://aws.amazon.com/datasets/</a>), Amazon provides a centralized repository of public datasets, including some huge datasets.</p></li>
</ul>
</div>
<div id="data-structures-and-storage" class="section level3">
<h3><span class="header-section-number">13.1.2</span> Data Structures and Storage</h3>
<p>As mentioned in the previous subsection, there are structured data as well as unstructured data. Structured data are highly organized data and usually have the following tabular format:</p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{lllll} \hline
 &amp; V_1 &amp; V_2 &amp; \cdots &amp; V_d \  
\\\hline
\textbf{x}_1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} \\
\textbf{x}_2 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} \\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
\textbf{x}_n &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nd} \\
\hline
\end{array}
\end{matrix}
\]</span></p>
<p>In other words, structured data can be organized into a table consists of rows and columns. Typically, each row represents a record and each column represents an attribute. A table can be decomposed into several tables that can be stored in a relational database such as the Microsoft SQL Server. The SQL (Structured Query Language) can be used to access and modify the data easily and efficiently.</p>
<p>Unstructured data do not follow a regular format <span class="citation">(Abdullah and Ahmad <a href="#ref-abdullah2013data">2013</a>)</span>. Examples of unstructured data include documents, videos, and audio files. Most of the data we encounter are unstructured data. In fact, the term ``big data’’ was coined to reflect this fact. Traditional relational databases cannot meet the challenges on the varieties and scales brought by massive unstructured data nowadays. NoSQL databases have been used to store massive unstructured data.</p>
<p>There are three main NoSQL databases <span class="citation">(Chen et al. <a href="#ref-chen2014b">2014</a>)</span>: key-value databases, column-oriented databases, and document-oriented databases. <a href="#" class="tooltip" style="color:green"><em>Key-value databases</em><span style="font-size:8pt">Data storage method that stores amd finds records using a unique key hash</span></a> use a simple data model and store data according to key-values. Modern key-value databases have higher expandability and smaller query response time than relational databases. Examples of key-value databases include Dynamo used by Amazon and Voldemort used by LinkedIn. <a href="#" class="tooltip" style="color:green"><em>Column-oriented databases</em><span style="font-size:8pt">Data storage method that stores records by column instead of by row</span></a> store and process data according to columns rather than rows. The columns and rows are segmented in multiple nodes to achieve expandability. Examples of column-oriented databases include BigTable developed by Google and Cassandra developed by FaceBook. <a href="#" class="tooltip" style="color:green"><em>Document databases</em><span style="font-size:8pt">Data storage method that uses the document metadata for search and retrieval, also known as semi-structured data</span></a> are designed to support more complex data forms than those stored in key-value databases. Examples of document databases include MongoDB, SimpleDB, and CouchDB. MongoDB is an open-source document-oriented database that stores documents as binary objects. SimpleDB is a distributed NoSQL database used by Amazon. CouchDB is another open-source document-oriented database.</p>
</div>
<div id="data-quality" class="section level3">
<h3><span class="header-section-number">13.1.3</span> Data Quality</h3>
<p>Accurate data are essential to useful data analysis. The lack of accurate data may lead to significant costs to organizations in areas such as correction activities, lost customers, missed opportunities, and incorrect decisions <span class="citation">(Olson <a href="#ref-olson2003">2003</a>)</span>.</p>
<p>Data has quality if it satisfies its intended use, that is, the data is accurate, timely, relevant, complete, understood, and trusted <span class="citation">(Olson <a href="#ref-olson2003">2003</a>)</span>. As a result, we first need to know the specification of the intended uses and then judge the suitability for those uses in order to assess the quality of the data. Unintended uses of data can arise from a variety of reasons and lead to serious problems.</p>
<p>Accuracy is the single most important component of high-quality data. Accurate data have the following properties <span class="citation">(Olson <a href="#ref-olson2003">2003</a>)</span>:</p>
<ul>
<li>The data elements are not missing and have valid values.</li>
<li>The values of the data elements are in the right ranges and have the right representations.</li>
</ul>
<p>Inaccurate data arise from different sources. In particular, the following areas are common areas where inaccurate data occur:</p>
<ul>
<li>Initial data entry. Mistakes (including deliberate errors) and system errors can occur during the initial data entry. Flawed data entry processes can result in inaccurate data.</li>
<li>Data decay. <a href="#" class="tooltip" style="color:green"><em>Data decay</em><span style="font-size:8pt">Corruption of data due to hardware failure in the storage device</span></a>, also known as data degradation, refers to the gradual corruption of computer data due to an accumulation of non-critical failures in a storage device.</li>
<li>Data moving and restructuring. Inaccurate data can also arise from data extracting, cleaning, transforming, loading, or integrating.</li>
<li>Data using. Faulty reporting and lack of understanding can lead to inaccurate data.</li>
</ul>
<p><a href="#" class="tooltip" style="color:green"><em>Reverification</em><span style="font-size:8pt">Manual process of checking the integrity of data</span></a> and analysis are two approaches to find inaccurate data elements. The first approach is done by people, who manually check every data element by going back to the original source of the data. The second approach is done by software with the skills of an analyst to search through data to find possible inaccurate data elements. To ensure that the data elements are 100% accurate, we must use reverification. However, reverification can be time-consuming and may not be possible for some data. Analytical techniques can also be used to identify inaccurate data elements. There are five types of analysis that can be used to identify inaccurate data <span class="citation">(Olson <a href="#ref-olson2003">2003</a>)</span>: <a href="#" class="tooltip" style="color:green"><em>data element analysis</em><span style="font-size:8pt">Analysis of the format and definition of each field</span></a>, <a href="#" class="tooltip" style="color:green"><em>structural analysis</em><span style="font-size:8pt">Statistical analysis of the structured data present to detect irregularities</span></a>, value correlation, aggregation correlation, and value inspection</p>
<p>Companies can create a data quality assurance program to create high-quality databases. For more information about management of data quality issues and data profiling techniques, readers are referred to <span class="citation">Olson (<a href="#ref-olson2003">2003</a>)</span>.</p>
</div>
<div id="data-cleaning" class="section level3">
<h3><span class="header-section-number">13.1.4</span> Data Cleaning</h3>
<p>Raw data usually need to be cleaned before useful analysis can be conducted. In particular, the following areas need attention when preparing data for analysis <span class="citation">(Janert <a href="#ref-janert2010">2010</a>)</span>:</p>
<ul>
<li><p><strong>Missing values</strong> It is common to have missing values in raw data. Depending on the situations, we can discard the record, discard the variable, or impute the missing values.</p></li>
<li><p><strong>Outliers</strong> Raw data may contain unusual data points such as outliers. We need to handle outliers carefully. We cannot just remove outliers without knowing the reason for their existence. Sometimes the outliers are caused by clerical errors. Sometimes outliers are the effect we are looking for.</p></li>
<li><p><strong>Junk</strong> Raw data may contain junks such as nonprintable characters. Junks are typically rare and not easy to get noticed. However, junks can cause serious problems in downstream applications.</p></li>
<li><p><strong>Format</strong> Raw data may be formatted in a way that is inconvenient for subsequent analysis. For example, components of a record may be split into multiple lines in a text file. In such cases, lines corresponding to a single record should be merged before loading to a data analysis software such as R.</p></li>
<li><p><strong>Duplicate records</strong> Raw data may contain duplicate records. Duplicate records should be recognized and removed. This task may not be trivial depending on what you consider ``duplicate.’’</p></li>
<li><p><strong>Merging datasets</strong> Raw data may come from different sources. In such cases, we need to merge the data from different sources to ensure compatibility.</p></li>
</ul>
<p>For more information about how to handle data in R, readers are referred to <span class="citation">Forte (<a href="#ref-forte2015">2015</a>)</span> and <span class="citation">Buttrey and Whitaker (<a href="#ref-buttrey2017">2017</a>)</span>.</p>
<div id="surveyElement131">

</div>
<div id="surveyResult131">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz131.1" href="javascript:toggleQuiz
('display.Quiz131.2','display.Quiz131.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz131.2" style="display: none">
<p id="Quiz131Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz131.js">
</script>
</div>
</div>
<div id="data-analysis-preliminaries" class="section level2">
<h2><span class="header-section-number">13.2</span> Data Analysis Preliminaries</h2>
<!-- aims 7.1.1  stages 7.1.2 -->
<!-- 7.5.1 visualization reporting -->
<!-- 7.5.2 reproducible mailund2017 -->
<p>Data analysis involves inspecting, cleansing, transforming, and modeling data to discover useful information to suggest conclusions and make decisions. Data analysis has a long history. In 1962, statistician John Tukey defined data analysis as:</p>
<blockquote>
<p>procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.</p>
<p>— <span class="citation">(Tukey <a href="#ref-tukey1962data">1962</a>)</span></p>
</blockquote>
<p>Recently, Judd and coauthors defined data analysis as the following equation<span class="citation">(Judd, McClelland, and Ryan <a href="#ref-judd2017">2017</a>)</span>:</p>
<p><span class="math display">\[\hbox{Data} = \hbox{Model} + \hbox{Error},\]</span> where Data represents a set of basic scores or observations to be analyzed, Model is a compact representation of the data, and Error is simply the amount the model fails to represent accurately. Using the above equation for data analysis, an analyst must resolve the following two conflicting goals:</p>
<ul>
<li>to add more parameters to the model so that the model represents the data better.</li>
<li>to remove parameters from the model so that the model is simple and parsimonious.</li>
</ul>
<p>In this section, we give a high-level introduction to data analysis, including different types of methods.</p>
<div id="S:process" class="section level3">
<h3><span class="header-section-number">13.2.1</span> Data Analysis Process</h3>
<p>Data analysis is part of an overall study. For example, Figure <a href="C-DataSystems.html#fig:study">13.1</a> shows the process of a typical study in behavioral and social sciences as described in <span class="citation">Albers (<a href="#ref-albers2017">2017</a>)</span>. The data analysis part consists of the following steps:</p>
<ul>
<li><p><strong>Exploratory analysis</strong> The purpose of this step is to get a feel of the relationships with the data and figure out what type of analysis for the data makes sense.</p></li>
<li><p><strong>Statistical analysis</strong> This step performs statistical analysis such as determining statistical significance and effect size.</p></li>
<li><p><strong>Make sense of the results</strong> This step interprets the statistical results in the context of the overall study.</p></li>
<li><p><strong>Determine implications</strong> This step interprets the data by connecting it to the study goals and the larger field of this study.</p></li>
</ul>
<p>The goal of the data analysis as described above focuses on explaining some phenomenon (See Section <a href="C-DataSystems.html#S:expred">13.2.5</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:study"></span>
<img src="Figures/Figure1.png" alt="The process of a typical study in behavioral and social sciences." width="80%" />
<p class="caption">
Figure 13.1: The process of a typical study in behavioral and social sciences.
</p>
</div>
<p><span class="citation">Shmueli (<a href="#ref-shmueli2010model">2010</a>)</span> described a general process for statistical modeling, which is shown in Figure <a href="C-DataSystems.html#fig:modeling">13.2</a>. Depending on the goal of the analysis, the steps differ in terms of the choice of methods, criteria, data, and information.</p>
<div class="figure" style="text-align: center"><span id="fig:modeling"></span>
<img src="Figures/Figure2.png" alt="The process of statistical modeling." width="80%" />
<p class="caption">
Figure 13.2: The process of statistical modeling.
</p>
</div>
</div>
<div id="exploratory-versus-confirmatory" class="section level3">
<h3><span class="header-section-number">13.2.2</span> Exploratory versus Confirmatory</h3>
<p>There are two phases of data analysis <span class="citation">(Good <a href="#ref-good1983data">1983</a>)</span>: <a href="#" class="tooltip" style="color:green"><em>exploratory data analysis</em><span style="font-size:8pt">Approach to analyzing data sets to summarize their main characteristics, using visual methods, descriptive statistics, clustering, dimension reduction</span></a> (EDA) and <a href="#" class="tooltip" style="color:green"><em>confirmatory data analysis</em><span style="font-size:8pt">Process used to challenge assumptions about the data through hypothesis tests, significance testing, model estimation, prediction, confidence intervals, and inference</span></a> (CDA). <a href="#tab:13.1">Table 13.1</a> summarizes some differences between EDA and CDA. EDA is usually applied to observational data with the goal of looking for patterns and formulating hypotheses. In contrast, CDA is often applied to experimental data (i.e., data obtained by means of a formal design of experiments) with the goal of quantifying the extent to which discrepancies between the model and the data could be expected to occur by chance <span class="citation">(Gelman <a href="#ref-gelman2004eda">2004</a>)</span>.</p>
<p><a id=tab:13.1></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{lll} \hline
 &amp; \textbf{EDA} &amp; \textbf{CDA} \\\hline
\text{Data} &amp; \text{Observational data} &amp; \text{Experimental data}\\[3mm]
\text{Goal} &amp; \text{Pattern recognition,}  &amp; \text{Hypothesis testing,}  \\
&amp; \text{formulate hypotheses} &amp; \text{estimation, prediction} \\[3mm]
\text{Techniques} &amp; \text{Descriptive statistics,} &amp; \text{Traditional statistical tools of} \\
&amp; \text{visualization, clustering} &amp; \text{inference, significance, and}\\
&amp; &amp; \text{confidence} \\
\hline
\end{array}
\end{matrix}
\]</span> <a href="#tab:13.1">Table 13.1</a>: Comparison of exploratory data analysis and confirmatory data analysis.</p>
<p>Techniques for EDA include descriptive statistics (e.g., mean, median, standard deviation, quantiles), distributions, histograms, correlation analysis, dimension reduction, and cluster analysis. Techniques for CDA include the traditional statistical tools of inference, significance, and confidence.</p>
</div>
<div id="supervised-versus-unsupervised" class="section level3">
<h3><span class="header-section-number">13.2.3</span> Supervised versus Unsupervised</h3>
<p>Methods for data analysis can be divided into two types <span class="citation">(Abbott <a href="#ref-abbott2014">2014</a>; Igual and Segu <a href="#ref-igual2017">2017</a>)</span>: supervised learning methods and unsupervised learning methods. <a href="#" class="tooltip" style="color:green"><em>Supervised learning methods</em><span style="font-size:8pt">Model that predicts a response target variable using explanatory predictors as input</span></a> work with labeled data, which include a target variable. Mathematically, supervised learning methods try to approximate the following function: <span class="math display">\[
Y = f(X_1, X_2, \ldots, X_p),
\]</span> where <span class="math inline">\(Y\)</span> is a target variable and <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span> are explanatory variables. Other terms are also used to mean a target variable. <a href="#tab:13.2">Table 13.2</a> gives a list of common names for different types of variables <span class="citation">(Edward W. Frees <a href="#ref-frees2009">2009</a><a href="#ref-frees2009">b</a>)</span>. When the target variable is a categorical variable, supervised learning methods are called <a href="#" class="tooltip" style="color:green"><em>classification methods</em><span style="font-size:8pt">Supervised learning method where the response is a categorical variable</span></a>. When the target variable is continuous, supervised learning methods are called <a href="#" class="tooltip" style="color:green"><em>regression methods</em><span style="font-size:8pt">Supervised learning method where the response is a continuous variable</span></a>.</p>
<p><a id=tab:13.2></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{ll}
\hline
\textbf{Target Variable}  &amp;  \textbf{Explanatory Variable}\\\hline
\text{Dependent variable} &amp; \text{Independent variable}\\
\text{Response} &amp; \text{Treatment} \\
\text{Output} &amp; \text{Input} \\
\text{Endogenous variable} &amp; \text{Exogenous variable} \\
\text{Predicted variable} &amp; \text{Predictor variable} \\
\text{Regressand} &amp; \text{Regressor} \\
\hline
\end{array}
\end{matrix}
\]</span> <a href="#tab:13.2">Table 13.2</a>: Common names of different variables.</p>
<p><a href="#" class="tooltip" style="color:green"><em>Unsupervised learning methods</em><span style="font-size:8pt">Models that work with explanatory variables only to describe patterns or groupings</span></a> work with unlabeled data, which include explanatory variables only. In other words, unsupervised learning methods do not use target variables. As a result, unsupervised learning methods are also called descriptive modeling methods.</p>
</div>
<div id="parametric-versus-nonparametric" class="section level3">
<h3><span class="header-section-number">13.2.4</span> Parametric versus Nonparametric</h3>
<p>Methods for data analysis can be parametric or nonparametric <span class="citation">(Abbott <a href="#ref-abbott2014">2014</a>)</span>. Parametric methods assume that the data follow a certain distribution. Nonparametric methods do not assume distributions for the data and therefore are called distribution-free methods.</p>
<p>Parametric methods have the advantage that if the distribution of the data is known, properties of the data and properties of the method (e.g., errors, convergence, coefficients) can be derived. A disadvantage of parametric methods is that analysts need to spend considerable time on figuring out the distribution. For example, analysts may try different transformation methods to transform the data so that it follows a certain distribution.</p>
<p>Since nonparametric methods make fewer assumptions, nonparametric methods have the advantage that they are more flexible, more <a href="#" class="tooltip" style="color:green"><em>robust</em><span style="font-size:8pt">Statistics which are more unaffected by outliers or small departures from model assumptions</span></a>, and applicable to non-quantitative data. However, a drawback of nonparametric methods is that the conclusions drawn from nonparametric methods are not as powerful as those drawn from parametric methods.</p>
</div>
<div id="S:expred" class="section level3">
<h3><span class="header-section-number">13.2.5</span> Explanation versus Prediction</h3>
<p>There are two goals in data analysis <span class="citation">(Breiman <a href="#ref-breiman2001modeling">2001</a>; Shmueli <a href="#ref-shmueli2010model">2010</a>)</span>: explanation and prediction. In some scientific areas such as economics, psychology, and environmental science, the focus of data analysis is to explain the causal relationships between the input variables and the response variable. In other scientific areas such as natural language processing and bioinformatics, the focus of data analysis is to predict what the responses are going to be given the input variables.</p>
<p><span class="citation">Shmueli (<a href="#ref-shmueli2010model">2010</a>)</span> discussed in detail the distinction between explanatory modeling and predictive modeling, which reflect the process of using data and methods for explaining or predicting, respectively. Explanatory modeling is commonly used for theory building and testing. However, predictive modeling is rarely used in many scientific fields as a tool for developing theory.</p>
<p><a href="#" class="tooltip" style="color:green"><em>Explanatory modeling</em><span style="font-size:8pt">Process where the modeling goal is to identify variables with meaningful and statistically significant relationships and test hypotheses</span></a> is typically done as follows:</p>
<ul>
<li><p>State the prevailing theory.</p></li>
<li><p>State causal hypotheses, which are given in terms of theoretical constructs rather than measurable variables. A causal diagram is usually included to illustrate the hypothesized causal relationship between the theoretical constructs.</p></li>
<li><p>Operationalize constructs. In this step, previous literature and theoretical justification are used to build a bridge between theoretical constructs and observable measurements.</p></li>
<li><p>Collect data and build models alongside the statistical hypotheses, which are operationalized from the research hypotheses.</p></li>
<li><p>Reach research conclusions and recommend policy. The statistical conclusions are converted into research conclusions. Policy recommendations are often accompanied.</p></li>
</ul>
<p><span class="citation">Shmueli (<a href="#ref-shmueli2010model">2010</a>)</span> defined <a href="#" class="tooltip" style="color:green"><em>predictive modeling</em><span style="font-size:8pt">Process where the modeling goal is to predict new observations</span></a> as the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations. Predictions include point predictions, interval predictions, regions, distributions, and rankings of new observations. Predictive model can be any method that produces predictions.</p>
</div>
<div id="data-modeling-versus-algorithmic-modeling" class="section level3">
<h3><span class="header-section-number">13.2.6</span> Data Modeling versus Algorithmic Modeling</h3>
<p><span class="citation">Breiman (<a href="#ref-breiman2001modeling">2001</a>)</span> discussed two cultures in the use of statistical modeling to reach conclusions from data: the data modeling culture and the algorithmic modeling culture. In the data modeling culture, the data are assumed to be generated by a given stochastic data model. In the algorithmic modeling culture, the data mechanism is treated as unknown and algorithmic models are used.</p>
<p><a href="#" class="tooltip" style="color:green"><em>Data modeling</em><span style="font-size:8pt">Assumes data generated comes from a stochastic data model</span></a> gives the statistics field many successes in analyzing data and getting information about the data mechanisms. However, <span class="citation">Breiman (<a href="#ref-breiman2001modeling">2001</a>)</span> argued that the focus on data models in the statistical community has led to some side effects such as</p>
<ul>
<li><p>Produced irrelevant theory and questionable scientific conclusions.</p></li>
<li><p>Kept statisticians from using algorithmic models that might be more suitable.</p></li>
<li><p>Restricted the ability of statisticians to deal with a wide range of problems.</p></li>
</ul>
<p><a href="#" class="tooltip" style="color:green"><em>Algorithmic modeling</em><span style="font-size:8pt">Assumes data generated comes from unknown algorithmic models</span></a> was used by industrial statisticians long time ago. However, the development of algorithmic methods was taken up by a community outside statistics <span class="citation">(Breiman <a href="#ref-breiman2001modeling">2001</a>)</span>. The goal of algorithmic modeling is <a href="#" class="tooltip" style="color:green"><em>predictive accuracy</em><span style="font-size:8pt">Quantitative measure of how well the explanatory variables predict the response outcome</span></a>. For some complex prediction problems, data models are not suitable. These prediction problems include speech recognition, image recognition, handwriting recognition, nonlinear time series prediction, and financial market prediction. The theory in algorithmic modeling focuses on the properties of algorithms, such as convergence and predictive accuracy.</p>
</div>
<div id="big-data-analysis" class="section level3">
<h3><span class="header-section-number">13.2.7</span> Big Data Analysis</h3>
<p>Unlike traditional data analysis, big data analysis employs additional methods and tools that can extract information rapidly from massive data. In particular, big data analysis uses the following processing methods <span class="citation">(Chen et al. <a href="#ref-chen2014b">2014</a>)</span>:</p>
<ul>
<li><p><strong>Bloom filter</strong> A bloom filter is a space-efficient probabilistic data structure that is used to determine whether an element belongs to a set. It has the advantages of high space efficiency and high query speed. A drawback of using bloom filter is that there is a certain misrecognition rate.</p></li>
<li><p><strong>Hashing</strong> Hashing is a method that transforms data into fixed-length numerical values through a hash function. It has the advantages of rapid reading and writing. However, sound hash functions are difficult to find.</p></li>
<li><p><strong>Indexing</strong> Indexing refers to a process of partitioning data in order to speed up reading. Hashing is a special case of indexing.</p></li>
<li><p><strong>Tries</strong> A trie, also called digital tree, is a method to improve query efficiency by using common prefixes of character strings to reduce comparison on character strings to the greatest extent.</p></li>
<li><p><strong>Parallel computing</strong> Parallel computing uses multiple computing resources to complete a computation task. Parallel computing tools include MPI (Message Passing Interface), MapReduce, and Dryad.</p></li>
</ul>
<p>Big data analysis can be conducted in the following levels <span class="citation">(Chen et al. <a href="#ref-chen2014b">2014</a>)</span>: memory-level, business intelligence (BI) level, and massive level. Memory-level analysis is conducted when the data can be loaded to the memory of a cluster of computers. Current hardware can handle hundreds of gigabytes (GB) of data in memory. BI level analysis can be conducted when the data surpass the memory level. It is common for BI level analysis products to support data over terabytes (TB). Massive level analysis is conducted when the data surpass the capabilities of products for BI level analysis. Usually Hadoop and MapReduce are used in massive level analysis.</p>
</div>
<div id="reproducible-analysis" class="section level3">
<h3><span class="header-section-number">13.2.8</span> Reproducible Analysis</h3>
<p>As mentioned in Section <a href="C-DataSystems.html#S:process">13.2.1</a>, a typical data analysis workflow includes collecting data, analyzing data, and reporting results. The data collected are saved in a database or files. The data are then analyzed by one or more <a href="#" class="tooltip" style="color:green"><em>scripts</em><span style="font-size:8pt">A program or sequence of instructions that is executed by another program</span></a>, which may save some intermediate results or always work on the raw data. Finally a report is produced to describe the results, which include relevant plots, tables, and summaries of the data. The workflow may subject to the following potential issues <span class="citation">(Mailund <a href="#ref-mailund2017">2017</a>, Chapter 2)</span>:</p>
<ul>
<li><p>The data are separated from the analysis scripts.</p></li>
<li><p>The documentation of the analysis is separated from the analysis itself.</p></li>
</ul>
<p>If the analysis is done on the raw data with a single script, then the first issue is not a major problem. If the analysis consists of multiple scripts and a script saves intermediate results that are read by the next script, then the scripts describe a workflow of data analysis. To reproduce an analysis, the scripts have to be executed in the right order. The workflow may cause major problems if the order of the scripts is not documented or the documentation is not updated or lost. One way to address the first issue is to write the scripts so that any part of the workflow can be run completely automatically at any time.</p>
<p>If the documentation of the analysis is synchronized with the analysis, then the second issue is not a major problem. However, the documentation may become completely useless if the scripts are changed but the documentation is not updated.</p>
<p><a href="#" class="tooltip" style="color:green"><em>Literate programming</em><span style="font-size:8pt">Coding practice where documentation and code are written together</span></a> is an approach to address the two issues mentioned above. In literate programming, the documentation of a program and the code of the program are written together. To do literate programming in R, one way is to use the R Markdown and the <span class="math inline">\(\texttt{knitr}\)</span> package.</p>
</div>
<div id="ethical-issues" class="section level3">
<h3><span class="header-section-number">13.2.9</span> Ethical Issues</h3>
<p>Analysts may face ethical issues and dilemmas during the data analysis process. In some fields, for example, ethical issues and dilemmas include participant consent, benefits, risk, confidentiality, and <a href="#" class="tooltip" style="color:green"><em>data ownership</em><span style="font-size:8pt">Governance process that details legal ownership of enterprise-wide data and outlines who has ability to create, edit, modify, share and restrict access to the data</span></a> <span class="citation">(Miles, Hberman, and Sdana <a href="#ref-miles2014">2014</a>)</span>. For data analysis in actuarial science and insurance in particular, we face the following ethical matters and issues <span class="citation">(Miles, Hberman, and Sdana <a href="#ref-miles2014">2014</a>)</span>:</p>
<ul>
<li><p><strong>Worthiness of the project</strong> Is the project worth doing? Will the project contribute in some significant way to a domain broader than my career? If a project is only opportunistic and does not have a larger significance, then it might be pursued with less care. The result may be looked good but not right.</p></li>
<li><p><strong>Competence</strong> Do I or the whole team have the expertise to carry out the project? Incompetence may lead to weakness in the analytics such as collecting large amounts of data poorly and drawing superficial conclusions.</p></li>
<li><p><strong>Benefits, costs, and reciprocity</strong> Will each stakeholder gain from the project? Are the benefit and the cost equitable? A project will likely to fail if the benefit and the cost for a stakeholder do not match.</p></li>
<li><p><strong>Privacy and confidentiality</strong> How do we make sure that the information is kept confidentially? Where raw data and analysis results are stored and how will have access to them should be documented in explicit confidentiality agreements.</p></li>
</ul>
<div id="surveyElement132">

</div>
<div id="surveyResult132">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz132.1" href="javascript:toggleQuiz
('display.Quiz132.2','display.Quiz132.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz132.2" style="display: none">
<p id="Quiz132Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz132.js">
</script>
</div>
</div>
<div id="data-analysis-techniques" class="section level2">
<h2><span class="header-section-number">13.3</span> Data Analysis Techniques</h2>
<!-- 7.2.1 - 7.2.3 exploratory, summarize, pca -->
<!-- 7.3.1 - 7.3.3 machine learning, problems solved by ml, techniques -->
<!-- statistics, machine learning, pattern recognition, data mining, predictive analytics, business intelligence, artificial intelligence -->
<p>Techniques for data analysis are drawn from different but overlapping fields such as statistics, machine learning, pattern recognition, and data mining. Statistics is a field that addresses reliable ways of gathering data and making inferences based on them <span class="citation">(Bandyopadhyay and Forster <a href="#ref-bandyo2011">2011</a>; Bluman <a href="#ref-bluman2012">2012</a>)</span>. The term machine learning was coined by Samuel in 1959 <span class="citation">(Samuel <a href="#ref-samuel1959ml">1959</a>)</span>. Originally, <a href="#" class="tooltip" style="color:green"><em>machine learning</em><span style="font-size:8pt">Study of algorithms and statistical models that perform a specific task without using explicit instructions, relying on patterns and inference</span></a> refers to the field of study where computers have the ability to learn without being explicitly programmed. Nowadays, machine learning has evolved to the broad field of study where computational methods use experience (i.e., the past information available for analysis) to improve performance or to make accurate predictions <span class="citation">(Bishop <a href="#ref-bishop2007">2007</a>; Clarke, Fokoue, and Zhang <a href="#ref-clarke2009">2009</a>; Mohri, Rostamizadeh, and Talwalkar <a href="#ref-mohri2012">2012</a>; Kubat <a href="#ref-kubat2017">2017</a>)</span>. There are four types of machine learning algorithms (See <a href="#tab:13.3">Table 13.3</a> depending on the type of the data and the type of the learning tasks.</p>
<p><a id=tab:13.3></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{rll} \hline
&amp; \textbf{Supervised} &amp; \textbf{Unsupervised} \\\hline
\textbf{Discrete Label} &amp; \text{Classification} &amp; \text{Clustering} \\
\textbf{Continuous Label} &amp; \text{Regression} &amp; \text{Dimension reduction} \\
\hline
\end{array}
\end{matrix}
\]</span> <a href="#tab:13.3">Table 13.3</a>: Types of machine learning algorithms.</p>
<p>Originating in engineering, <a href="#" class="tooltip" style="color:green"><em>pattern recognition</em><span style="font-size:8pt">Automated recognition of patterns and regularities in data</span></a> is a field that is closely related to machine learning, which grew out of computer science. In fact, pattern recognition and machine learning can be considered to be two facets of the same field <span class="citation">(Bishop <a href="#ref-bishop2007">2007</a>)</span>. <a href="#" class="tooltip" style="color:green"><em>Data mining</em><span style="font-size:8pt">Process of collecting, cleaning, processing, analyzing, and discovering patterns and useful insights from large data sets</span></a> is a field that concerns collecting, cleaning, processing, analyzing, and gaining useful insights from data <span class="citation">(Aggarwal <a href="#ref-aggarwal2015">2015</a>)</span>.</p>
<div id="exploratory-techniques" class="section level3">
<h3><span class="header-section-number">13.3.1</span> Exploratory Techniques</h3>
<p>Exploratory data analysis techniques include descriptive statistics as well as many unsupervised learning techniques such as data clustering and principal component analysis.</p>
<div id="descriptive-statistics" class="section level4">
<h4><span class="header-section-number">13.3.1.1</span> Descriptive Statistics</h4>
<p>In the mass noun sense, descriptive statistics is an area of statistics that concerns the collection, organization, summarization, and presentation of data <span class="citation">(Bluman <a href="#ref-bluman2012">2012</a>)</span>. In the count noun sense, descriptive statistics are summary statistics that quantitatively describe or summarize data.</p>
<p><a id=tab:13.4></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{ll} \hline
&amp; \textbf{Descriptive Statistics} \\\hline
\text{Measures of central tendency} &amp; \text{Mean, median, mode, midrange}\\
\text{Measures of variation} &amp; \text{Range, variance, standard deviation} \\
\text{Measures of position} &amp; \text{Quantile} \\
\hline
\end{array}
\end{matrix}
\]</span> <a href="#tab:13.4">Table 13.4</a>: Some commonly used descriptive statistics.</p>
<p><a href="#tab:13.4">Table 13.4</a> lists some commonly used descriptive statistics. In R, we can use the function <span class="math inline">\(\texttt{summary}\)</span> to calculate some of the descriptive statistics. For numeric data, we can visualize the descriptive statistics using a boxplot.</p>
<p>In addition to these quantitative descriptive statistics, we can also qualitatively describe shapes of the distributions <span class="citation">(Bluman <a href="#ref-bluman2012">2012</a>)</span>. For example, we can say that a distribution is positively skewed, symmetric, or negatively skewed. To visualize the distribution of a variable, we can draw a histogram.</p>
</div>
<div id="principal-component-analysis" class="section level4">
<h4><span class="header-section-number">13.3.1.2</span> Principal Component Analysis</h4>
<p><a href="#" class="tooltip" style="color:green"><em>Principal component analysis</em><span style="font-size:8pt">Dimension reduction technique that uses orthogonal transformations to convert a set of possibly correlated variables into a set of linearly uncorrelated variables</span></a> (PCA) is a statistical procedure that transforms a dataset described by possibly correlated variables into a dataset described by linearly uncorrelated variables, which are called principal components and are ordered according to their variances. PCA is a technique for dimension reduction. If the original variables are highly correlated, then the first few principal components can account for most of the variation of the original data.</p>
<p>The principal components of the variables are related to the eigenvectors and eigenvectors of the covariance matrix of the variables. For <span class="math inline">\(i=1,2,\ldots,d\)</span>, let <span class="math inline">\((\lambda_i, \textbf{e}_i)\)</span> be the <span class="math inline">\(i\)</span>th eigenvalue-eigenvector pair of the covariance matrix <span class="math inline">\({\Sigma}\)</span> of <span class="math inline">\(d\)</span> variables <span class="math inline">\(X_1,X_2,\ldots,X_d\)</span> such that <span class="math inline">\(\lambda_1\ge \lambda_2\ge \ldots\ge \lambda_d\ge 0\)</span> and the eigenvectors are normalized. Then the <span class="math inline">\(i\)</span>th principal component is given by</p>
<p><span class="math display">\[Z_{i} = \textbf{e}_i&#39; \textbf{X} =\sum_{j=1}^d e_{ij} X_j,\]</span> where <span class="math inline">\(\textbf{X}=(X_1,X_2,\ldots,X_d)&#39;\)</span>. It can be shown that <span class="math inline">\(\mathrm{Var~}{(Z_i)} = \lambda_i\)</span>. As a result, the proportion of variance explained by the <span class="math inline">\(i\)</span>th principal component is calculated as</p>
<p><span class="math display">\[\dfrac{\mathrm{Var~}{(Z_i)}}{ \sum_{j=1}^{d} \mathrm{Var~}{(Z_j)}} = \dfrac{\lambda_i}{\lambda_1+\lambda_2+\cdots+\lambda_d}.\]</span></p>
<p>For more information about PCA, readers are referred to <span class="citation">Mirkin (<a href="#ref-mirkin2011">2011</a>)</span>.</p>
</div>
<div id="cluster-analysis" class="section level4">
<h4><span class="header-section-number">13.3.1.3</span> Cluster Analysis</h4>
<p><a href="#" class="tooltip" style="color:green"><em>Cluster analysis</em><span style="font-size:8pt">Unsupervised learning method that aims to splot data into homogenous groups using a similarity measure</span></a> (aka data clustering) refers to the process of dividing a dataset into homogeneous groups or clusters such that points in the same cluster are similar and points from different clusters are quite distinct <span class="citation">(Gan, Ma, and Wu <a href="#ref-gan2007">2007</a>; Gan <a href="#ref-gan2011">2011</a>)</span>. Data clustering is one of the most popular tools for exploratory data analysis and has found applications in many scientific areas.</p>
<p>During the past several decades, many clustering algorithms have been proposed. Among these clustering algorithms, the <span class="math inline">\(k\)</span>-means algorithm is perhaps the most well-known algorithm due to its simplicity. To describe the <a href="#" class="tooltip" style="color:green"><em>k-means algorithm</em><span style="font-size:8pt">Type of clustering that aims to partition data into k mutually exclusive clusters by assigning observations to the cluster with the nearest centroid</span></a>, let <span class="math inline">\(X=\{\textbf{x}_1,\textbf{x}_2,\ldots,\textbf{x}_n\}\)</span> be a dataset containing <span class="math inline">\(n\)</span> points, each of which is described by <span class="math inline">\(d\)</span> numerical features. Given a desired number of clusters <span class="math inline">\(k\)</span>, the <span class="math inline">\(k\)</span>-means algorithm aims at minimizing the following objective function:</p>
<p><span class="math display">\[P(U,Z) = \sum_{l=1}^k\sum_{i=1}^n u_{il} \Vert \textbf{x}_i-\textbf{z}_l\Vert^2,\]</span> where <span class="math inline">\(U=(u_{il})_{n\times k}\)</span> is an <span class="math inline">\(n\times k\)</span> partition matrix, <span class="math inline">\(Z=\{\textbf{z}_1,\textbf{z}_2,\ldots,\textbf{z}_k\}\)</span> is a set of cluster centers, and <span class="math inline">\(\Vert\cdot\Vert\)</span> is the <span class="math inline">\(L^2\)</span> norm or Euclidean distance. The partition matrix <span class="math inline">\(U\)</span> satisfies the following conditions:</p>
<p><span class="math display">\[u_{il}\in \{0,1\},\quad i=1,2,\ldots,n,\:l=1,2,\ldots,k,\]</span> <span class="math display">\[\sum_{l=1}^k u_{il}=1,\quad i=1,2,\ldots,n.\]</span></p>
<p>The <span class="math inline">\(k\)</span>-means algorithm employs an iterative procedure to minimize the objective function. It repeatedly updates the partition matrix <span class="math inline">\(U\)</span> and the cluster centers <span class="math inline">\(Z\)</span> alternately until some stop criterion is met. For more information about <span class="math inline">\(k\)</span>-means, readers are referred to <span class="citation">Gan, Ma, and Wu (<a href="#ref-gan2007">2007</a>)</span> and <span class="citation">Mirkin (<a href="#ref-mirkin2011">2011</a>)</span>.</p>
</div>
</div>
<div id="confirmatory-techniques" class="section level3">
<h3><span class="header-section-number">13.3.2</span> Confirmatory Techniques</h3>
<p>Confirmatory data analysis techniques include the traditional statistical tools of inference, significance, and confidence.</p>
<div id="linear-models" class="section level4">
<h4><span class="header-section-number">13.3.2.1</span> Linear Models</h4>
<p>Linear models, also called <a href="#" class="tooltip" style="color:green"><em>linear regression models</em><span style="font-size:8pt">Supervised model that uses a linear function to approximate the relationship between the target and explanatory variables</span></a>, aim at using a linear function to approximate the relationship between the dependent variable and independent variables. A linear regression model is called a simple linear regression model if there is only one independent variable. When more than one independent variables are involved, a linear regression model is called a multiple linear regression model.</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> denote the independent and the dependent variables, respectively. For <span class="math inline">\(i=1,2,\ldots,n\)</span>, let <span class="math inline">\((x_i, y_i)\)</span> be the observed values of <span class="math inline">\((X,Y)\)</span> in the <span class="math inline">\(i\)</span>th case. Then the simple linear regression model is specified as follows <span class="citation">(Edward W. Frees <a href="#ref-frees2009">2009</a><a href="#ref-frees2009">b</a>)</span>:</p>
<p><span class="math display">\[y_i  = \beta_0 + \beta_1 x_i + \epsilon_i,\quad i=1,2,\ldots,n,\]</span> where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are parameters and <span class="math inline">\(\epsilon_i\)</span> is a random variable representing the error for the <span class="math inline">\(i\)</span>th case.</p>
<p>When there are multiple independent variables, the following multiple linear regression model is used:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \epsilon_i,\]</span> where <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_k\)</span> are unknown parameters to be estimated.</p>
<p>Linear regression models usually make the following assumptions:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(x_{i1},x_{i2},\ldots,x_{ik}\)</span> are nonstochastic variables.</p></li>
<li><p><span class="math inline">\(\mathrm{Var~}(y_i)=\sigma^2\)</span>, where <span class="math inline">\(\mathrm{Var~}(y_i)\)</span> denotes the variance of <span class="math inline">\(y_i\)</span>.</p></li>
<li><p><span class="math inline">\(y_1,y_2,\ldots,y_n\)</span> are independent random variables.</p></li>
</ol>
<p>For the purpose of obtaining tests and confidence statements with small samples, the following strong normality assumption is also made:</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><span class="math inline">\(\epsilon_1,\epsilon_2,\ldots,\epsilon_n\)</span> are normally distributed.</li>
</ol>
</div>
<div id="generalized-linear-models" class="section level4">
<h4><span class="header-section-number">13.3.2.2</span> Generalized Linear Models</h4>
<p>The <a href="#" class="tooltip" style="color:green"><em>generalized linear model</em><span style="font-size:8pt">Supervised model that generalizes linear regression by allowing the linear component to be related to the response variable via a link function and by allowing the variance of each measurement to be a function of its predicted value</span></a> (GLM) is a wide family of regression models that include linear regression models as special cases. In a GLM, the mean of the response (i.e., the dependent variable) is assumed to be a function of linear combinations of the explanatory variables, i.e.,</p>
<p><span class="math display">\[\mu_i = E[y_i],\]</span> <span class="math display">\[\eta_i = \textbf{x}_i&#39;\boldsymbol{\beta} = g(\mu_i),\]</span> where <span class="math inline">\(\textbf{x}_i=(1,x_{i1}, x_{i2}, \ldots, x_{ik})&#39;\)</span> is a vector of regressor values, <span class="math inline">\(\mu_i\)</span> is the mean response for the <span class="math inline">\(i\)</span>th case, and <span class="math inline">\(\eta_i\)</span> is a <a href="#" class="tooltip" style="color:green"><em>systematic component</em><span style="font-size:8pt">The linear combination of explanatory variables component in a glm</span></a> of the <a href="#" class="tooltip" style="color:green"><em>GLM</em><span style="font-size:8pt">Generalized linear model</span></a>. The function <span class="math inline">\(g(\cdot)\)</span> is known and is called the <a href="#" class="tooltip" style="color:green"><em>link function</em><span style="font-size:8pt">Function that relates between the linear predictor component to the mean of the target variable</span></a>. The mean response can vary by observations by allowing some parameters to change. However, the regression parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> are assumed to be the same among different observations.</p>
<p>GLMs make the following assumptions:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(x_{i1},x_{i2},\ldots,x_{in}\)</span> are nonstochastic variables.</p></li>
<li><p><span class="math inline">\(y_1,y_2,\ldots,y_n\)</span> are independent.</p></li>
<li><p>The dependent variable is assumed to follow a distribution from the linear exponential family.</p></li>
<li><p>The variance of the dependent variable is not assumed to be constant but is a function of the mean, i.e.,</p></li>
</ol>
<p><span class="math display">\[\mathrm{Var~}{(y_i)} = \phi \nu(\mu_i),\]</span> where <span class="math inline">\(\phi\)</span> denotes the dispersion parameter and <span class="math inline">\(\nu(\cdot)\)</span> is a function.</p>
<p>As we can see from the above specification, the GLM provides a unifying framework to handle different types of dependent variables, including discrete and continuous variables. For more information about GLMs, readers are referred to <span class="citation">Jong and Heller (<a href="#ref-dejong2008">2008</a>)</span> and <span class="citation">Edward W. Frees (<a href="#ref-frees2009">2009</a><a href="#ref-frees2009">b</a>)</span>.</p>
</div>
<div id="tree-based-models" class="section level4">
<h4><span class="header-section-number">13.3.2.3</span> Tree-based Models</h4>
<p><a href="#" class="tooltip" style="color:green"><em>Decision trees</em><span style="font-size:8pt">Modeling technique that uses a tree-like model of decisions to divide the sample space into non-overlapping regions to make predictions</span></a>, also known as tree-based models, involve dividing the predictor space (i.e., the space formed by independent variables) into a number of simple regions and using the mean or the mode of the region for prediction <span class="citation">(Breiman et al. <a href="#ref-breiman1984">1984</a>)</span>. There are two types of tree-based models: classification trees and regression trees. When the dependent variable is categorical, the resulting tree models are called classification trees. When the dependent variable is continuous, the resulting tree models are called regression trees.</p>
<p>The process of building classification trees is similar to that of building regression trees. Here we only briefly describe how to build a regression tree. To do that, the predictor space is divided into non-overlapping regions such that the following objective function</p>
<p><span class="math display">\[f(R_1,R_2,\ldots,R_J) = \sum_{j=1}^J \sum_{i=1}^n I_{R_j}(\textbf{x}_i)(y_i - \mu_j)^2\]</span> is minimized, where <span class="math inline">\(I\)</span> is an indicator function, <span class="math inline">\(R_j\)</span> denotes the set of indices of the observations that belong to the <span class="math inline">\(j\)</span>th box, <span class="math inline">\(\mu_j\)</span> is the mean response of the observations in the <span class="math inline">\(j\)</span>th box, <span class="math inline">\(\textbf{x}_i\)</span> is the vector of predictor values for the <span class="math inline">\(i\)</span>th observation, and <span class="math inline">\(y_i\)</span> is the response value for the <span class="math inline">\(i\)</span>th observation.</p>
<p>In terms of predictive accuracy, decision trees generally do not perform to the level of other regression and classification models. However, tree-based models may outperform linear models when the relationship between the response and the predictors is nonlinear. For more information about decision trees, readers are referred to <span class="citation">Breiman et al. (<a href="#ref-breiman1984">1984</a>)</span> and <span class="citation">Mitchell (<a href="#ref-mitchell1997">1997</a>)</span>.</p>
<!-- ###Statistical Inference -->
<div id="surveyElement133">

</div>
<div id="surveyResult133">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz133.1" href="javascript:toggleQuiz
('display.Quiz133.2','display.Quiz133.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz133.2" style="display: none">
<p id="Quiz133Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz133.js">
</script>
</div>
</div>
</div>
<div id="some-r-functions" class="section level2">
<h2><span class="header-section-number">13.4</span> Some R Functions</h2>
<!-- % 7.2.4 - 7.2.7 fit distribution, linear model, survival analysis, glm -->
<!-- % 7.3.4 nn, trees -->
<p>R is an open-source software for statistical computing and graphics. The R software can be downloaded from the R project website at . In this section, we give some R function for data analysis, especially the data analysis tasks mentioned in previous sections.</p>
<p><a id=tab:13.5></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{lll} \hline
\text{Data Analysis Task} &amp; \text{R package} &amp; \text{R Function} \\\hline
\text{Descriptive Statistics} &amp; \texttt{base} &amp; \texttt{summary}\\
\text{Principal Component Analysis} &amp; \texttt{stats} &amp; \texttt{prcomp} \\
\text{Data Clustering} &amp; \texttt{stats} &amp; \texttt{kmeans}, \texttt{hclust} \\
\text{Fitting Distributions} &amp; \texttt{MASS} &amp; \texttt{fitdistr} \\
\text{Linear Regression Models} &amp; \texttt{stats} &amp; \texttt{lm} \\
\text{Generalized Linear Models} &amp; \texttt{stats} &amp; \texttt{glm} \\
\text{Regression Trees} &amp; \texttt{rpart} &amp; \texttt{rpart} \\
\text{Survival Analysis} &amp; \texttt{survival} &amp; \texttt{survfit} \\
\hline
\end{array}
\end{matrix}
\]</span> <a href="#tab:13.5">Table 13.5</a>: Some R functions for data analysis.</p>
<p><a href="#tab:13.5">Table 13.5</a> lists a few R functions for different data analysis tasks. Readers can read the R documentation for examples of using these functions. There are also other R functions from other packages to do similar things. However, the functions listed in this table provide good start points for readers to conduct data analysis in R. For analyzing large datasets in R in an efficient way, readers are referred to <span class="citation">Daroczi (<a href="#ref-daroczi2015">2015</a>)</span>.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">13.5</span> Summary</h2>
<p>In this chapter, we gave a high-level overview of data analysis by introducing data types, data structures, data storages, data sources, data analysis processes, and data analysis techniques. In particular, we presented various aspects of data analysis. In addition, we provided several websites where readers can obtain real-world datasets to horn their data analysis skills. We also listed some R packages and functions that can be used to perform various data analysis tasks.</p>
</div>
<div id="DS:further-reading-and-resources" class="section level2">
<h2><span class="header-section-number">13.6</span> Further Resources and Contributors</h2>
<div id="contributor-2" class="section level4 unnumbered">
<h4>Contributor</h4>
<ul>
<li><strong>Guojun Gan</strong>, University of Connecticut, is the principal author of the initial version of this chapter. Email: <a href="mailto:guojun.gan@uconn.edu">guojun.gan@uconn.edu</a> for chapter comments and suggested improvements.</li>
<li>Chapter reviewers include: Min Ji, Toby White.</li>
</ul>

</div>
</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references">
<div id="ref-abbott2014">
<p>Abbott, Dean. 2014. <em>Applied Predictive Analytics: Principles and Techniques for the Professional Data Analyst</em>. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-abdullah2013data">
<p>Abdullah, Mohammad F., and Kamsuriah Ahmad. 2013. “The Mapping Process of Unstructured Data to Structured Data.” In <em>2013 International Conference on Research and Innovation in Information Systems (Icriis)</em>, 151–55.</p>
</div>
<div id="ref-aggarwal2015">
<p>Aggarwal, Charu C. 2015. <em>Data Mining: The Textbook</em>. New York, NY: Springer.</p>
</div>
<div id="ref-albers2017">
<p>Albers, Michael J. 2017. <em>Introduction to Quantitative Data Analysis in the Behavioral and Social Sciences</em>. Hoboken, NJ: John Wiley &amp; Sons, Inc.</p>
</div>
<div id="ref-bandyo2011">
<p>Bandyopadhyay, Prasanta S., and Malcolm R. Forster, eds. 2011. <em>Philosophy of Statistics</em>. Handbook of the Philosophy of Science 7. North Holland.</p>
</div>
<div id="ref-bishop2007">
<p>Bishop, Christopher M. 2007. <em>Pattern Recognition and Machine Learning</em>. New York, NY: Springer.</p>
</div>
<div id="ref-bluman2012">
<p>Bluman, Allan. 2012. <em>Elementary Statistics: A Step by Step Approach</em>. New York, NY: McGraw-Hill.</p>
</div>
<div id="ref-breiman2001modeling">
<p>Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” <em>Statistical Science</em> 16 (3): 199–231.</p>
</div>
<div id="ref-breiman1984">
<p>Breiman, Leo, Jerome Friedman, Charles J. Stone, and R.A. Olshen. 1984. <em>Classification and Regression Trees</em>. Raton Boca, FL: Chapman; Hall/CRC.</p>
</div>
<div id="ref-buttrey2017">
<p>Buttrey, Samuel E., and Lyn R. Whitaker. 2017. <em>A Data Scientist’s Guide to Acquiring, Cleaning, and Managing Data in R</em>. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-chen2014b">
<p>Chen, Min, Shiwen Mao, Yin Zhang, and Victor CM Leung. 2014. <em>Big Data: Related Technologies, Challenges and Future Prospects</em>. New York, NY: Springer.</p>
</div>
<div id="ref-clarke2009">
<p>Clarke, Bertrand, Ernest Fokoue, and Hao Helen Zhang. 2009. <em>Principles and Theory for Data Mining and Machine Learning</em>. New York, NY: Springer-Verlag.</p>
</div>
<div id="ref-daroczi2015">
<p>Daroczi, Gergely. 2015. <em>Mastering Data Analysis with R</em>. Birmingham, UK: Packt Publishing.</p>
</div>
<div id="ref-forte2015">
<p>Forte, Rui Miguel. 2015. <em>Mastering Predictive Analytics with R</em>. Birmingham, UK: Packt Publishing.</p>
</div>
<div id="ref-frees2009">
<p>Frees, Edward W. 2009b. <em>Regression Modeling with Actuarial and Financial Applications</em>. Cambridge University Press.</p>
</div>
<div id="ref-gan2011">
<p>Gan, Guojun. 2011. <em>Data Clustering in C++: An Object-Oriented Approach</em>. Data Mining and Knowledge Discovery Series. Boca Raton, FL, USA: Chapman &amp; Hall/CRC Press. doi:<a href="https://doi.org/10.1201/b10814">10.1201/b10814</a>.</p>
</div>
<div id="ref-gan2007">
<p>Gan, Guojun, Chaoqun Ma, and Jianhong Wu. 2007. <em>Data Clustering: Theory, Algorithms, and Applications</em>. Philadelphia, PA: SIAM Press. doi:<a href="https://doi.org/10.1137/1.9780898718348">10.1137/1.9780898718348</a>.</p>
</div>
<div id="ref-gelman2004eda">
<p>Gelman, Andrew. 2004. “Exploratory Data Analysis for Complex Models.” <em>Journal of Computational and Graphical Statistics</em> 13 (4): 755–79.</p>
</div>
<div id="ref-good1983data">
<p>Good, I. J. 1983. “The Philosophy of Exploratory Data Analysis.” <em>Philosophy of Science</em> 50 (2): 283–95.</p>
</div>
<div id="ref-hashem2015bigdata">
<p>Hashem, Ibrahim Abaker Targio, Ibrar Yaqoob, Nor Badrul Anuar, Salimah Mokhtar, Abdullah Gani, and Samee Ullah Khan. 2015. “The Rise of ‘Big Data’ on Cloud Computing: Review and Open Research Issues.” <em>Information Systems</em> 47: 98–115.</p>
</div>
<div id="ref-hox2005data">
<p>Hox, Joop J., and Hennie R. Boeije. 2005. “Data Collection, Primary Versus Secondary.” In <em>Encyclopedia of Social Measurement</em>, 593–99. Elsevier.</p>
</div>
<div id="ref-igual2017">
<p>Igual, Laura, and Santi Segu. 2017. <em>Introduction to Data Science. a Python Approach to Concepts, Techniques and Applications</em>. New York, NY: Springer.</p>
</div>
<div id="ref-inmon2014">
<p>Inmon, W.H., and Dan Linstedt. 2014. <em>Data Architecture: A Primer for the Data Scientist: Big Data, Data Warehouse and Data Vault</em>. Cambridge, MA: Morgan Kaufmann.</p>
</div>
<div id="ref-janert2010">
<p>Janert, Philipp K. 2010. <em>Data Analysis with Open Source Tools</em>. Sebastopol, CA: O’Reilly Media.</p>
</div>
<div id="ref-dejong2008">
<p>Jong, Piet de, and Gillian Z. Heller. 2008. <em>Generalized Linear Models for Insurance Data</em>. Cambridge, UK: Cambridge University Press.</p>
</div>
<div id="ref-judd2017">
<p>Judd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017. <em>Data Analysis. a Model Comparison Approach to Regression, ANOVA and Beyond</em>. 3rd ed. New York, NY: Routledge.</p>
</div>
<div id="ref-kubat2017">
<p>Kubat, Miroslav. 2017. <em>An Introduction to Machine Learning</em>. 2nd ed. New York, NY: Springer.</p>
</div>
<div id="ref-mailund2017">
<p>Mailund, Thomas. 2017. <em>Beginning Data Science in R: Data Analysis, Visualization, and Modelling for the Data Scientist</em>. Apress.</p>
</div>
<div id="ref-miles2014">
<p>Miles, Matthew, Michael Hberman, and Johnny Sdana. 2014. <em>Qualitative Data Analysis: A Methods Sourcebook</em>. 3rd ed. Thousand Oaks, CA: Sage.</p>
</div>
<div id="ref-mirkin2011">
<p>Mirkin, Boris. 2011. <em>Core Concepts in Data Analysis: Summarization, Correlation and Visualization</em>. London, UK: Springer.</p>
</div>
<div id="ref-mitchell1997">
<p>Mitchell, Tom M. 1997. <em>Machine Learning</em>. McGraw-Hill.</p>
</div>
<div id="ref-mohri2012">
<p>Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. <em>Foundations of Machine Learning</em>. Cambridge, MA: MIT Press.</p>
</div>
<div id="ref-olson2003">
<p>Olson, Jack E. 2003. <em>Data Quality: The Accuracy Dimension</em>. San Francisco, CA: Morgan Kaufmann.</p>
</div>
<div id="ref-leary2013bigdata">
<p>O’Leary, D. E. 2013. “Artificial Intelligence and Big Data.” <em>IEEE Intelligent Systems</em> 28 (2): 96–99.</p>
</div>
<div id="ref-pries2015">
<p>Pries, Kim H., and Robert Dunnigan. 2015. <em>Big Data Analytics: A Practical Guide for Managers</em>. Boca Raton, FL: CRC Press.</p>
</div>
<div id="ref-samuel1959ml">
<p>Samuel, A. L. 1959. “Some Studies in Machine Learning Using the Game of Checkers.” <em>IBM Journal of Research and Development</em> 3 (3): 210–29.</p>
</div>
<div id="ref-shmueli2010model">
<p>Shmueli, Galit. 2010. “To Explain or to Predict?” <em>Statistical Science</em> 25 (3): 289–310.</p>
</div>
<div id="ref-tukey1962data">
<p>Tukey, John W. 1962. “The Future of Data Analysis.” <em>The Annals of Mathematical Statistics</em> 33 (1). Institute of Mathematical Statistics: 1–67.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C-BonusMalus.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C-DependenceModel.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["LossDataAnalytics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
