<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Loss Data Analytics</title>
  <meta name="description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="<a href="https://github.com/openacttexts/Loss-Data-Analytics" class="uri">https://github.com/openacttexts/Loss-Data-Analytics</a>" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Loss Data Analytics" />
  
  <meta name="twitter:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="C-DataSystems.html">
<link rel="next" href="C-AppA.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
</script>

<script language="javascript">
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
</script>
<script language="javascript">
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
</script>
<script language="javascript">
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
</script>

<script language="javascript">
$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});
</script>


<script>
$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="C-Intro.html"><a href="C-Intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="C-Intro.html"><a href="C-Intro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="C-Intro.html"><a href="C-Intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.1</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.2" data-path="C-Intro.html"><a href="C-Intro.html#short-and-long-term-insurance"><i class="fa fa-check"></i><b>1.1.2</b> Short and Long-term Insurance</a></li>
<li class="chapter" data-level="1.1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="C-Intro.html"><a href="C-Intro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="C-Intro.html"><a href="C-Intro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="C-Intro.html"><a href="C-Intro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="C-Intro.html"><a href="C-Intro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="C-Intro.html"><a href="C-Intro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.3.1" data-path="C-Intro.html"><a href="C-Intro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="C-Intro.html"><a href="C-Intro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="C-Intro.html"><a href="C-Intro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="C-Intro.html"><a href="C-Intro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html"><i class="fa fa-check"></i><b>2</b> Frequency Modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>2.1</b> Frequency Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Basic Frequency Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>2.2.1</b> Foundations</a></li>
<li class="chapter" data-level="2.2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>2.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> The (a, b, 0) Class</a></li>
<li class="chapter" data-level="2.4" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimating Frequency Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>2.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Frequency Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="2.7" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="2.8" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#r-code-for-plots-in-this-chapter"><i class="fa fa-check"></i><b>2.9</b> R Code for Plots in this Chapter</a></li>
<li class="chapter" data-level="2.10" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>2.10</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C-Severity.html"><a href="C-Severity.html"><i class="fa fa-check"></i><b>3</b> Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.1" data-path="C-Severity.html"><a href="C-Severity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Basic Distributional Quantities</a><ul>
<li class="chapter" data-level="3.1.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>3.1.1</b> Moments</a></li>
<li class="chapter" data-level="3.1.2" data-path="C-Severity.html"><a href="C-Severity.html#quantiles"><i class="fa fa-check"></i><b>3.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="C-Severity.html"><a href="C-Severity.html#moment-generating-function"><i class="fa fa-check"></i><b>3.1.3</b> Moment Generating Function</a></li>
<li class="chapter" data-level="3.1.4" data-path="C-Severity.html"><a href="C-Severity.html#probability-generating-function"><i class="fa fa-check"></i><b>3.1.4</b> Probability Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C-Severity.html"><a href="C-Severity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions for Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="C-Severity.html"><a href="C-Severity.html#gamma-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="C-Severity.html"><a href="C-Severity.html#pareto-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="C-Severity.html"><a href="C-Severity.html#weibull-distribution"><i class="fa fa-check"></i><b>3.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="C-Severity.html"><a href="C-Severity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>3.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C-Severity.html"><a href="C-Severity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Methods of Creating New Distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="C-Severity.html"><a href="C-Severity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="3.3.2" data-path="C-Severity.html"><a href="C-Severity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>3.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="3.3.3" data-path="C-Severity.html"><a href="C-Severity.html#raising-to-a-power"><i class="fa fa-check"></i><b>3.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="3.3.4" data-path="C-Severity.html"><a href="C-Severity.html#exponentiation"><i class="fa fa-check"></i><b>3.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="3.3.5" data-path="C-Severity.html"><a href="C-Severity.html#finite-mixtures"><i class="fa fa-check"></i><b>3.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="3.3.6" data-path="C-Severity.html"><a href="C-Severity.html#continuous-mixtures"><i class="fa fa-check"></i><b>3.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="C-Severity.html"><a href="C-Severity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>3.4</b> Coverage Modifications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="3.4.2" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="3.4.3" data-path="C-Severity.html"><a href="C-Severity.html#coinsurance"><i class="fa fa-check"></i><b>3.4.3</b> Coinsurance</a></li>
<li class="chapter" data-level="3.4.4" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C-Severity.html"><a href="C-Severity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>3.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="3.5.2" data-path="C-Severity.html"><a href="C-Severity.html#MLEGrouped"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Likelihood Estimators for Grouped Data</a></li>
<li class="chapter" data-level="3.5.3" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-censored-data"><i class="fa fa-check"></i><b>3.5.3</b> Maximum Likelihood Estimators for Censored Data</a></li>
<li class="chapter" data-level="3.5.4" data-path="C-Severity.html"><a href="C-Severity.html#maximum-likelihood-estimators-for-truncated-data"><i class="fa fa-check"></i><b>3.5.4</b> Maximum Likelihood Estimators for Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C-Severity.html"><a href="C-Severity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>3.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html"><i class="fa fa-check"></i><b>4</b> Model Selection and Estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation"><i class="fa fa-check"></i><b>4.1.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.1.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>4.1.2</b> Tools for Model Selection and Diagnostics</a></li>
<li class="chapter" data-level="4.1.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#starting-values"><i class="fa fa-check"></i><b>4.1.3</b> Starting Values</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>4.2</b> Model Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#iterative-model-selection"><i class="fa fa-check"></i><b>4.2.1</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="4.2.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-training-dataset"><i class="fa fa-check"></i><b>4.2.2</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="4.2.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>4.2.3</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="4.2.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#model-selection-based-on-cross-validation"><i class="fa fa-check"></i><b>4.2.4</b> Model Selection Based on Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>4.3</b> Estimation using Modified Data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#parametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.1</b> Parametric Estimation using Modified Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3.2</b> Nonparametric Estimation using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:BayesInference"><i class="fa fa-check"></i><b>4.4</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:IntroBayes"><i class="fa fa-check"></i><b>4.4.1</b> Introduction to Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-model"><i class="fa fa-check"></i><b>4.4.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#bayesian-inference"><i class="fa fa-check"></i><b>4.4.3</b> Bayesian Inference</a></li>
<li class="chapter" data-level="4.4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>4.4.4</b> Conjugate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#technical-supplement-a.-gini-statistic"><i class="fa fa-check"></i>Technical Supplement A. Gini Statistic</a><ul>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#ts-a.1.-the-classic-lorenz-curve"><i class="fa fa-check"></i>TS A.1. The Classic Lorenz Curve</a></li>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#ts-a.2.-ordered-lorenz-curve-and-the-gini-index"><i class="fa fa-check"></i>TS A.2. Ordered Lorenz Curve and the Gini Index</a></li>
<li class="chapter" data-level="" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#ts-a.3.-out-of-sample-validation"><i class="fa fa-check"></i>TS A.3. Out-of-Sample Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html"><i class="fa fa-check"></i><b>5</b> Aggregate Loss Models</a><ul>
<li class="chapter" data-level="5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>5.2</b> Individual Risk Model</a></li>
<li class="chapter" data-level="5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#collective-risk-model"><i class="fa fa-check"></i><b>5.3</b> Collective Risk Model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>5.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>5.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="5.3.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#analytic-results"><i class="fa fa-check"></i><b>5.3.3</b> Analytic Results</a></li>
<li class="chapter" data-level="5.3.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#tweedie-distribution"><i class="fa fa-check"></i><b>5.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>5.4</b> Computing the Aggregate Claims Distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>5.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="5.4.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#simulation"><i class="fa fa-check"></i><b>5.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>5.5</b> Effects of Coverage Modifications</a><ul>
<li class="chapter" data-level="5.5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>5.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="5.5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>5.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="5.5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>5.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>5.6</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#technical-supplement-b.-aggregate-loss-models"><i class="fa fa-check"></i>Technical Supplement B. Aggregate Loss Models</a><ul>
<li class="chapter" data-level="" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#ts-b.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS B.1. Individual Risk Model Properties</a></li>
<li><a href="C-AggLossModels.html#ts-b.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it">TS B.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li><a href="C-AggLossModels.html#ts-b.3.-example-5.3.8-moment-generating-function-of-aggregate-loss-s_n">TS B.3. Example 5.3.8 Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="C-Simulation.html"><a href="C-Simulation.html"><i class="fa fa-check"></i><b>6</b> Simulation</a><ul>
<li class="chapter" data-level="6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>6.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="6.2" data-path="C-Simulation.html"><a href="C-Simulation.html#inverse-transform"><i class="fa fa-check"></i><b>6.2</b> Inverse Transform</a></li>
<li class="chapter" data-level="6.3" data-path="C-Simulation.html"><a href="C-Simulation.html#how-many-simulated-values"><i class="fa fa-check"></i><b>6.3</b> How Many Simulated Values?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C-PremCalc.html"><a href="C-PremCalc.html"><i class="fa fa-check"></i><b>7</b> Premium Calculation Fundamentals</a></li>
<li class="chapter" data-level="8" data-path="C-RiskClass.html"><a href="C-RiskClass.html"><i class="fa fa-check"></i><b>8</b> Risk Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>8.2</b> Poisson Regression Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>8.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="8.2.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>8.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>8.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="8.2.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#exercises-4"><i class="fa fa-check"></i><b>8.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>8.3</b> Categorical Variables and Multiplicative Tariff</a><ul>
<li class="chapter" data-level="8.3.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>8.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="8.3.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>8.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="8.3.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>8.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="8.3.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>8.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>8.4</b> Contributors and Further Resources</a></li>
<li class="chapter" data-level="8.5" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:mle-Pois-reg"><i class="fa fa-check"></i><b>8.5</b> Technical Supplement – Estimating Poisson Regression Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C-Credibility.html"><a href="C-Credibility.html"><i class="fa fa-check"></i><b>9</b> Experience Rating Using Credibility Theory</a><ul>
<li class="chapter" data-level="9.1" data-path="C-Credibility.html"><a href="C-Credibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>9.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="9.2" data-path="C-Credibility.html"><a href="C-Credibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.2</b> Limited Fluctuation Credibility</a><ul>
<li class="chapter" data-level="9.2.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:frequency"><i class="fa fa-check"></i><b>9.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="9.2.2" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>9.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="9.2.3" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>9.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="9.2.4" data-path="C-Credibility.html"><a href="C-Credibility.html#partial-credibility"><i class="fa fa-check"></i><b>9.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-credibility"><i class="fa fa-check"></i><b>9.3</b> Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.3.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>9.3.1</b> Credibility Z, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="C-Credibility.html"><a href="C-Credibility.html#buhlmann-straub-credibility"><i class="fa fa-check"></i><b>9.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="9.5" data-path="C-Credibility.html"><a href="C-Credibility.html#bayesian-inference-and-buhlmann"><i class="fa fa-check"></i><b>9.5</b> Bayesian Inference and Bühlmann</a><ul>
<li class="chapter" data-level="9.5.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:Gamma-Poisson"><i class="fa fa-check"></i><b>9.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="9.5.2" data-path="C-Credibility.html"><a href="C-Credibility.html#exact-credibility"><i class="fa fa-check"></i><b>9.5.2</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="C-Credibility.html"><a href="C-Credibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>9.6</b> Estimating Credibility Parameters</a><ul>
<li class="chapter" data-level="9.6.1" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="9.6.2" data-path="C-Credibility.html"><a href="C-Credibility.html#nonparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.2</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.3" data-path="C-Credibility.html"><a href="C-Credibility.html#semiparametric-estimation-for-buhlmann-and-buhlmann-straub-models"><i class="fa fa-check"></i><b>9.6.3</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.4" data-path="C-Credibility.html"><a href="C-Credibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>9.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="C-Credibility.html"><a href="C-Credibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C-PortMgt.html"><a href="C-PortMgt.html"><i class="fa fa-check"></i><b>10</b> Insurance Portfolio Management including Reinsurance</a><ul>
<li class="chapter" data-level="" data-path="C-PortMgt.html"><a href="C-PortMgt.html#overview"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="10.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Tails"><i class="fa fa-check"></i><b>10.1</b> Tails of Distributions</a><ul>
<li class="chapter" data-level="10.1.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>10.1.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="10.1.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>10.1.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>10.2</b> Risk Measures</a><ul>
<li class="chapter" data-level="10.2.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>10.2.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="10.2.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>10.2.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="10.2.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>10.2.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>10.3</b> Reinsurance</a><ul>
<li class="chapter" data-level="10.3.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>10.3.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.3.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>10.3.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="10.3.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>10.3.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C-LossReserves.html"><a href="C-LossReserves.html"><i class="fa fa-check"></i><b>11</b> Loss Reserving</a></li>
<li class="chapter" data-level="12" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html"><i class="fa fa-check"></i><b>12</b> Experience Rating using Bonus-Malus</a></li>
<li class="chapter" data-level="13" data-path="C-DataSystems.html"><a href="C-DataSystems.html"><i class="fa fa-check"></i><b>13</b> Data Systems</a><ul>
<li class="chapter" data-level="13.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data"><i class="fa fa-check"></i><b>13.1</b> Data</a><ul>
<li class="chapter" data-level="13.1.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-types-and-sources"><i class="fa fa-check"></i><b>13.1.1</b> Data Types and Sources</a></li>
<li class="chapter" data-level="13.1.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-structures-and-storage"><i class="fa fa-check"></i><b>13.1.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="13.1.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-quality"><i class="fa fa-check"></i><b>13.1.3</b> Data Quality</a></li>
<li class="chapter" data-level="13.1.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-cleaning"><i class="fa fa-check"></i><b>13.1.4</b> Data Cleaning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-preliminary"><i class="fa fa-check"></i><b>13.2</b> Data Analysis Preliminary</a><ul>
<li class="chapter" data-level="13.2.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:process"><i class="fa fa-check"></i><b>13.2.1</b> Data Analysis Process</a></li>
<li class="chapter" data-level="13.2.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-versus-confirmatory"><i class="fa fa-check"></i><b>13.2.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="13.2.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>13.2.3</b> Supervised versus Unsupervised</a></li>
<li class="chapter" data-level="13.2.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#parametric-versus-nonparametric"><i class="fa fa-check"></i><b>13.2.4</b> Parametric versus Nonparametric</a></li>
<li class="chapter" data-level="13.2.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:expred"><i class="fa fa-check"></i><b>13.2.5</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="13.2.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-modeling-versus-algorithmic-modeling"><i class="fa fa-check"></i><b>13.2.6</b> Data Modeling versus Algorithmic Modeling</a></li>
<li class="chapter" data-level="13.2.7" data-path="C-DataSystems.html"><a href="C-DataSystems.html#big-data-analysis"><i class="fa fa-check"></i><b>13.2.7</b> Big Data Analysis</a></li>
<li class="chapter" data-level="13.2.8" data-path="C-DataSystems.html"><a href="C-DataSystems.html#reproducible-analysis"><i class="fa fa-check"></i><b>13.2.8</b> Reproducible Analysis</a></li>
<li class="chapter" data-level="13.2.9" data-path="C-DataSystems.html"><a href="C-DataSystems.html#ethical-issues"><i class="fa fa-check"></i><b>13.2.9</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-techniques"><i class="fa fa-check"></i><b>13.3</b> Data Analysis Techniques</a><ul>
<li class="chapter" data-level="13.3.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-techniques"><i class="fa fa-check"></i><b>13.3.1</b> Exploratory Techniques</a></li>
<li class="chapter" data-level="13.3.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#descriptive-statistics"><i class="fa fa-check"></i><b>13.3.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="13.3.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#cluster-analysis"><i class="fa fa-check"></i><b>13.3.3</b> Cluster Analysis</a></li>
<li class="chapter" data-level="13.3.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#confirmatory-techniques"><i class="fa fa-check"></i><b>13.3.4</b> Confirmatory Techniques</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#some-r-functions"><i class="fa fa-check"></i><b>13.4</b> Some R Functions</a></li>
<li class="chapter" data-level="13.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html"><i class="fa fa-check"></i><b>14</b> Dependence Modeling</a><ul>
<li class="chapter" data-level="14.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:VarTypes"><i class="fa fa-check"></i><b>14.1</b> Variable Types</a><ul>
<li class="chapter" data-level="14.1.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuaVar"><i class="fa fa-check"></i><b>14.1.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuanVar"><i class="fa fa-check"></i><b>14.1.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>14.1.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>14.2</b> Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>14.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>14.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>14.2.3</b> Nominal Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>14.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="14.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>14.4</b> Application Using Copulas</a><ul>
<li class="chapter" data-level="14.4.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#data-description"><i class="fa fa-check"></i><b>14.4.1</b> Data Description</a></li>
<li class="chapter" data-level="14.4.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>14.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="14.4.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>14.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="14.4.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>14.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>14.5</b> Types of Copulas</a><ul>
<li class="chapter" data-level="14.5.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#elliptical-copulas"><i class="fa fa-check"></i><b>14.5.1</b> Elliptical Copulas</a></li>
<li class="chapter" data-level="14.5.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#archimedian-copulas"><i class="fa fa-check"></i><b>14.5.2</b> Archimedian Copulas</a></li>
<li class="chapter" data-level="14.5.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>14.5.3</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>14.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="14.7" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>14.7</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#technical-supplement-a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>Technical Supplement A. Other Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#a.1.-blomqvists-beta"><i class="fa fa-check"></i>A.1. Blomqvist’s Beta</a></li>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#a.2.-nonparametric-approach-using-spearman-correlation-with-tied-ranks"><i class="fa fa-check"></i>A.2. Nonparametric Approach Using Spearman Correlation with Tied Ranks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C-AppA.html"><a href="C-AppA.html"><i class="fa fa-check"></i><b>15</b> Appendix A: Review of Statistical Inference</a><ul>
<li class="chapter" data-level="15.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>15.1</b> Basic Concepts</a><ul>
<li class="chapter" data-level="15.1.1" data-path="C-AppA.html"><a href="C-AppA.html#random-sampling"><i class="fa fa-check"></i><b>15.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="15.1.2" data-path="C-AppA.html"><a href="C-AppA.html#sampling-distribution"><i class="fa fa-check"></i><b>15.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="15.1.3" data-path="C-AppA.html"><a href="C-AppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>15.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>15.2</b> Point Estimation and Properties</a><ul>
<li class="chapter" data-level="15.2.1" data-path="C-AppA.html"><a href="C-AppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>15.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="15.2.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>15.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>15.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="15.3.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>15.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="15.3.2" data-path="C-AppA.html"><a href="C-AppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>15.3.2</b> Large-sample Properties of MLE</a></li>
<li class="chapter" data-level="15.3.3" data-path="C-AppA.html"><a href="C-AppA.html#confidence-interval"><i class="fa fa-check"></i><b>15.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>15.4</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="15.4.1" data-path="C-AppA.html"><a href="C-AppA.html#basic-concepts"><i class="fa fa-check"></i><b>15.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="15.4.2" data-path="C-AppA.html"><a href="C-AppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>15.4.2</b> Student-<span class="math inline">\(t\)</span> test based on MLE</a></li>
<li class="chapter" data-level="15.4.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>15.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="15.4.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>15.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C-AppB.html"><a href="C-AppB.html"><i class="fa fa-check"></i><b>16</b> Appendix B: Iterated Expectations</a><ul>
<li class="chapter" data-level="16.1" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>16.1</b> Conditional Distribution and Conditional Expectation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="C-AppB.html"><a href="C-AppB.html#conditional-distribution"><i class="fa fa-check"></i><b>16.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="16.1.2" data-path="C-AppB.html"><a href="C-AppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>16.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>16.2</b> Iterated Expectations and Total Variance</a><ul>
<li class="chapter" data-level="16.2.1" data-path="C-AppB.html"><a href="C-AppB.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>16.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="16.2.2" data-path="C-AppB.html"><a href="C-AppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>16.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="C-AppB.html"><a href="C-AppB.html#application"><i class="fa fa-check"></i><b>16.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>16.3</b> Conjugate Distributions</a><ul>
<li class="chapter" data-level="16.3.1" data-path="C-AppB.html"><a href="C-AppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>16.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="16.3.2" data-path="C-AppB.html"><a href="C-AppB.html#conjugate-distributions"><i class="fa fa-check"></i><b>16.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C-AppC.html"><a href="C-AppC.html"><i class="fa fa-check"></i><b>17</b> Appendix C: Maximum Likelihood Theory</a><ul>
<li class="chapter" data-level="17.1" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>17.1</b> Likelihood Function</a><ul>
<li class="chapter" data-level="17.1.1" data-path="C-AppC.html"><a href="C-AppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="17.1.2" data-path="C-AppC.html"><a href="C-AppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>17.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>17.2</b> Maximum Likelihood Estimators</a><ul>
<li class="chapter" data-level="17.2.1" data-path="C-AppC.html"><a href="C-AppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>17.2.1</b> Definition and Derivation of MLE</a></li>
<li class="chapter" data-level="17.2.2" data-path="C-AppC.html"><a href="C-AppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>17.2.2</b> Asymptotic Properties of MLE</a></li>
<li class="chapter" data-level="17.2.3" data-path="C-AppC.html"><a href="C-AppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>17.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>17.3</b> Statistical Inference Based on Maximum Likelhood Estimation</a><ul>
<li class="chapter" data-level="17.3.1" data-path="C-AppC.html"><a href="C-AppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>17.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="17.3.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>17.3.2</b> MLE and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://openacttexts.github.io/Loss-Data-Analytics/" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C:DependenceModel" class="section level1">
<h1><span class="header-section-number">Chapter 14</span> Dependence Modeling</h1>
<p><em>Chapter Preview</em>. In practice, there are many types of variables that one encounter and the first step in dependence modeling is identifying the type of variable you are dealing with to help direct you to the appropriate technique.This chapter introduces readers to variable types and techniques for modeling dependence or association of multivariate distributions. Section <a href="C-DependenceModel.html#S:VarTypes">14.1</a> provides an overview of the types of variables. Section <a href="C-DependenceModel.html#S:Measures">14.2</a> then elaborates basic measures for modeling the dependence between variables.</p>
<p>Section <a href="C-DependenceModel.html#S:Copula">14.3</a> introduces a novel approach to modeling dependence using Copulas which is reinforced with practical illustrations in Section <a href="C-DependenceModel.html#S:CopAppl">14.4</a>. The types of Copula families and basic properties of Copula functions is explained Section <a href="C-DependenceModel.html#S:CopTyp">14.5</a>. The chapter concludes by explaining why the study of dependence modeling is important in Section <a href="C-DependenceModel.html#S:CopImp">14.6</a>.</p>
<div id="S:VarTypes" class="section level2">
<h2><span class="header-section-number">14.1</span> Variable Types</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Classify variables as qualitative or quantitative.</li>
<li>Describe multivariate variables.</li>
</ul>
<hr />
<p>People, firms, and other entities that we want to understand are described in a dataset by numerical characteristics. As these characteristics vary by entity, they are commonly known as <em>variables</em>. To manage insurance systems, it will be critical to understand the distribution of each variable and how they are associated with one another. It is common for data sets to have many variables (high dimensional) and so it useful to begin by classifying them into different types. As will be seen, these classifications are not strict; there is overlap among the groups. Nonetheless, the grouping summarized in <a href="#tab:14.1">Table 14.1</a> and explained in the remainder of this section provide a solid first step in framing a data set.</p>
<p><a id=tab:14.1></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|l} \hline
\textbf{Variable Type} &amp; \textbf{Example} \\\hline
Qualitative &amp;            \\
    \text{Binary} &amp;        \text{Sex} \\
\text{Categorical (Unordered, Nominal)} &amp; \text{Territory (e.g., state/province) in which an insured resides} \\
\text{Ordered Category (Ordinal)} &amp; \text{Claimant satisfaction (five point scale ranging from 1=dissatisfied} \\
&amp; ~~~ \text{to 5 =satisfied)} \\\hline
Quantitative &amp;            \\
\text{Continuous} &amp; \text{Policyholder&#39;s age, weight, income} \\
  \text{Discrete} &amp; \text{Amount of deductible} \\
\text{Count} &amp; \text{Number of insurance claims} \\
\text{Combinations of}  &amp; \text{Policy losses, mixture of 0&#39;s (for no loss)}  \\
~~~ \text{Discrete and Continuous} &amp; ~~~\text{and positive claim amount} \\
\text{Interval Variable} &amp; \text{Driver Age: 16-24 (young), 25-54 (intermediate),}  \\
&amp; ~~~\text{55 and over (senior)} \\
\text{Circular Data} &amp; \text{Time of day measures of customer arrival} \\ \hline
Multivariate ~ Variable &amp;            \\
\text{High Dimensional Data} &amp; \text{Characteristics of a firm purchasing worker&#39;s compensation} \\
&amp; ~~~\text{insurance (location of plants, industry, number of employees,} \\
&amp;~~~\text{and so on)} \\
\text{Spatial Data} &amp; \text{Longitude/latitude of the location an insurance hailstorm claim} \\
\text{Missing Data} &amp; \text{Policyholder&#39;s age (continuous/interval) and &quot;-99&quot; for} \\
&amp;~~~ \text{&quot;not reported,&quot; that is, missing} \\
\text{Censored and Truncated Data} &amp; \text{Amount of insurance claims in excess of a deductible} \\
\text{Aggregate Claims} &amp; \text{Losses recorded for each claim in a motor vehicle policy.} \\
\text{Stochastic Process Realizations} &amp; \text{The time and amount of each occurrence of an insured loss} \\ \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.1">Table 14.1</a> : Variable types</p>
<p>In data analysis, it is important to understand what type of variable you are working with. For example, Consider a pair of random variables <em>(Coverage,Claim)</em> from the LGPIF data introduced in chapter 1 as displayed in Figure <a href="C-DependenceModel.html#fig:IntroPlot">14.1</a> below. We would like to know whether the distribution of <em>Coverage</em> depends on the distribution of <em>Claim</em> or whether they are statistically independent. We would also want to know how the <em>Claim</em> distribution depends on the <em>EntityType</em> variable. Because the <em>EntityType</em> variable belongs to a different class of variables, modeling the dependence between <em>Claim</em> and <em>Coverage</em> may require a different technique from that of <em>Claim</em> and <em>EntityType</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:IntroPlot"></span>
<img src="LossDataAnalytics_files/figure-html/IntroPlot-1.png" alt="Scatter plot of *(Coverage,Claim)* from LGPIF data" width="100%" />
<p class="caption">
Figure 14.1: Scatter plot of <em>(Coverage,Claim)</em> from LGPIF data
</p>
</div>
<div id="S:QuaVar" class="section level3">
<h3><span class="header-section-number">14.1.1</span> Qualitative Variables</h3>
<hr />
<p>In this sub-section, you learn how to:</p>
<ul>
<li>Classify qualitative variables as nominal or ordinal</li>
<li>Describe binary variable</li>
</ul>
<hr />
<p>A <em>qualitative</em>, or <em>categorical</em>, variable is one for which the measurement denotes membership in a set of groups, or categories. For example, if you were coding which area of the country an insured resides, you might use a 1 for the northern part, 2 for southern, and 3 for everything else. This location variable is an example of a <em>nominal</em> variable, one for which the levels have no natural ordering. Any analysis of nominal variables should not depend on the labeling of the categories. For example, instead of using a 1,2,3 for north, south, other, I should arrive at the same set of summary statistics if I used a 2,1,3 coding instead, interchanging north and south.</p>
<p>In contrast, an <em>ordinal</em> variable is a type of categorical variable for which an ordering does exist. For example, with a survey to see how satisfied customers are with our claims servicing department, we might use a five point scale that ranges from 1 meaning dissatisfied to a 5 meaning satisfied. Ordinal variables provide a clear ordering of levels of a variable but the amount of separation between levels is unknown.</p>
<p>A <em>binary</em> variable is a special type of categorical variable where there are only two categories commonly taken to be a 0 and a 1. For example, we might code a variable in a dataset to be a 1 if an insured is female and a 0 if male.</p>
</div>
<div id="S:QuanVar" class="section level3">
<h3><span class="header-section-number">14.1.2</span> Quantitative Variables</h3>
<hr />
<p>In this sub-section, you learn how to:</p>
<ul>
<li>Differentiate between continuous and discrete variable</li>
<li>Use a combination of continuous and discrete variable</li>
<li>Describe circular data</li>
</ul>
<hr />
<p>Unlike a qualitative variable, a quantitative variable is one in which numerical level is a realization from some scale so that the distance between any two levels of the scale takes on meaning. A <em>continuous variable</em> is one that can take on any value within a finite interval. For example, it is common to represent a policyholder’s age, weight, or income, as a continuous variable. In contrast, a <em>discrete variable</em> is one that takes on only a finite number of values in any finite interval. Like an ordinal variable, these represent distinct categories that are ordered. Unlike an ordinal variable, the numerical difference between levels takes on economic meaning. A special type of discrete variable is a <em>count variable</em>, one with values on the nonnegative integers. For example, we will be particularly interested in the number of claims arising from a policy during a given period.</p>
<p>Some variables are inherently a <em>combination of discrete and continuous</em> components. For example, when we analyze the insured loss of a policyholder, we will encounter a discrete outcome at zero, representing no insured loss, and a continuous amount for positive outcomes, representing the amount of the insured loss. Another interesting variation is an <em>interval variable</em>, one that gives a range of possible outcomes.</p>
<p><em>Circular data</em> represent an interesting category typically not analyzed by insurers. As an example of circular data, suppose that you monitor calls to your customer service center and would like to know when is the peak time of the day for calls to arrive. In this context, one can think about the time of the day as a variable with realizations on a circle, e.g., imagine an analog picture of a clock. For circular data, the distance between observations at 00:15 and 00:45 are just as close as observations 23:45 and 00:15 (here, we use the convention <em>HH:MM</em> means hours and minutes).</p>
</div>
<div id="multivariate-variables" class="section level3">
<h3><span class="header-section-number">14.1.3</span> Multivariate Variables</h3>
<hr />
<p>In this sub-section, you learn how to:</p>
<ul>
<li>Differentiate between univariate and multivariate data</li>
<li>Handle missing variables</li>
</ul>
<hr />
<p>Insurance data typically are <em>multivariate</em> in the sense that we can take many measurements on a single entity. For example, when studying losses associated with a firm’s worker’s compensation plan, we might want to know the location of its manufacturing plants, the industry in which it operates, the number of employees, and so forth. The usual strategy for analyzing multivariate data is to begin by examining each variable in isolation of the others. This is known as a <em>univariate</em> approach.</p>
<p>In contrast, for some variables, it makes little sense to only look at one dimensional aspects. For example, insurers typically organize <em>spatial</em> data by longitude and latitude to analyze the location of weather related insurance claims due hailstorms. Having only a single number, either longitude or latitude, provides little information in understanding geographical location.</p>
<p>Another special case of a multivariate variable, less obvious, involves coding for <em>missing data</em>. When data are missing, it is better to think about the variable as two dimensions, one to indicate whether or not the variable is reported and the second providing the age (if reported). In the same way, insurance data are commonly <em>censored</em> and <em>truncated</em>. We refer you to Chapter 4 for more on censored and truncated data. <em>Aggregate claims</em> can also be coded as another special type of multivariate variable. We refer you to Chapter 5 for more Aggregate claims.</p>
<p>Perhaps the most complicated type of multivariate variable is a <em>realization of a stochastic process</em>. You will recall that a stochastic process is little more than a collection of random variables. For example, in insurance, we might think about the times that claims arrive to an insurance company in a one year time horizon. This is a high dimensional variable that theoretically is infinite dimensional. Special techniques are required to understand realizations of stochastic processes that will not be addressed here.</p>
</div>
</div>
<div id="S:Measures" class="section level2">
<h2><span class="header-section-number">14.2</span> Classic Measures of Scalar Associations</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Estimate correlation using Pearson method</li>
<li>Use rank based measures like Spearman, Kendall to estimate correlation</li>
<li>Measure dependence using odds ratio,Pearson chi-square and likelihood ratio test statistic</li>
<li>Use normal-based correlations to quantify associations involving ordinal variables</li>
</ul>
<hr />
<div id="association-measures-for-quantitative-variables" class="section level3">
<h3><span class="header-section-number">14.2.1</span> Association Measures for Quantitative Variables</h3>
<p>For this section, consider a pair of random variables <span class="math inline">\((X,Y)\)</span> having joint distribution function <span class="math inline">\(F(\cdot)\)</span> and a random sample <span class="math inline">\((X_i,Y_i), i=1, \ldots, n\)</span>. For the continuous case, suppose that <span class="math inline">\(F(\cdot)\)</span> is absolutely continuous with absolutely continuous marginals.</p>
<div id="pearson-correlation" class="section level4">
<h4><span class="header-section-number">14.2.1.1</span> Pearson Correlation</h4>
Define the sample covariance function <span class="math inline">\(Cov(X,Y) = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})\)</span>, where <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span> are the sample means of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively. Then, the product-moment (Pearson) correlation can be written as
<span class="math display">\[\begin{equation*}
r = \frac{Cov(X,Y)}{\sqrt{Cov(X,X) Cov(Y,Y)}}.
\end{equation*}\]</span>
<p>The correlation statistic <span class="math inline">\(r\)</span> is widely used to capture association between random variables. It is a (nonparametric) estimator of the correlation parameter <span class="math inline">\(\rho\)</span>, defined to be the covariance divided by the product of standard deviations. In this sense, it captures association for any pair of random variables.</p>
<p>This statistic has several important features. Unlike regression estimators, it is symmetric between random variables, so the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> equals the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. It is unchanged by linear transformations of random variables (up to sign changes) so that we can multiply random variables or add constants as is helpful for interpretation. The range of the statistic is <span class="math inline">\([-1,1]\)</span> which does not depend on the distribution of either <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>.</p>
<p>Further, in the case of independence, the correlation coefficient <span class="math inline">\(r\)</span> is 0. However, it is well known that zero correlation does not imply independence, except for normally distributed random variables. The correlation statistic <span class="math inline">\(r\)</span> is also a (maximum likelihood) estimator of the association parameter for bivariate normal distribution. So, for normally distributed data, the correlation statistic <span class="math inline">\(r\)</span> can be used to assess independence. For additional interpretations of this well-known statistic, readers will enjoy <span class="citation">(Lee Rodgers and Nicewander <a href="#ref-lee1988thirteen">1998</a>)</span>.</p>
<p>You can obtain the correlation statistic <span class="math inline">\(r\)</span> using the <code>cor()</code> function in <code>R</code> and selecting the <code>pearson</code> method. This is demonstrated below by using the <em>Coverage</em> rating variable in millions of dollars and <em>Claim</em> amount variable in dollars from the LGPIF data introduced in chapter 1.</p>
<h5 style="text-align: center;">
<a id="display.pearson.1" href="javascript:togglecode
('display.pearson.2','display.pearson.1');"><i><strong>R Code for Pearson Correlation Statistic</strong></i></a>
</h5>
<div id="display.pearson.2" style="display: none">
<pre><code>### Pearson correlation between Claim and Coverage ###
r&lt;-cor(Claim,Coverage, method = c(&quot;pearson&quot;))
round(r,2)

Output:
[1] 0.31

### Pearson correlation between Claim and log(Coverage) ###
r&lt;-cor(Claim,log(Coverage), method = c(&quot;pearson&quot;))
round(r,2)

Output:
[1] 0.1</code></pre>
</div>
<p>From <code>R</code> output above, <span class="math inline">\(r=0.31\)</span> , which indicates a positive association between <em>Claim</em> and <em>Coverage</em>. This means that as the coverage amount of a policy increases we expect claim to increase.</p>
</div>
</div>
<div id="rank-based-measures" class="section level3">
<h3><span class="header-section-number">14.2.2</span> Rank Based Measures</h3>
<div id="spearmans-rho" class="section level4">
<h4><span class="header-section-number">14.2.2.1</span> Spearman’s Rho</h4>
<p>The Pearson correlation coefficient does have the drawback that it is not invariant to nonlinear transforms of the data. For example, the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(\ln Y\)</span> can be quite different from the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. As we see from the <code>R</code> code for Pearson correlation statistic above, the correlation statistic <span class="math inline">\(r\)</span> between <em>Coverage</em> rating variable in logarithmic millions of dollars and <em>Claim</em> amounts variable in dollars is <span class="math inline">\(0.1\)</span> as compared to <span class="math inline">\(0.31\)</span> when we calculate the correlation between <em>Coverage</em> rating variable in millions of dollars and <em>Claim</em> amounts variable in dollars. This limitation is one reason for considering alternative statistics.</p>
<p>Alternative measures of correlation are based on ranks of the data. Let <span class="math inline">\(R(X_j)\)</span> denote the rank of <span class="math inline">\(X_j\)</span> from the sample <span class="math inline">\(X_1, \ldots, X_n\)</span> and similarly for <span class="math inline">\(R(Y_j)\)</span>. Let <span class="math inline">\(R(X) = \left(R(X_1), \ldots, R(X_n)\right)&#39;\)</span> denote the vector of ranks, and similarly for <span class="math inline">\(R(Y)\)</span>. For example, if <span class="math inline">\(n=3\)</span> and <span class="math inline">\(X=(24, 13, 109)\)</span>, then <span class="math inline">\(R(X)=(2,1,3)\)</span>. A comprehensive introduction of rank statistics can be found in, for example, <span class="citation">(Hettmansperger <a href="#ref-hettmansperger1984statistical">1984</a>)</span>. Also, ranks can be used to obtain the empirical distribution function, refer to section 4.1.1 for more on the empirical distribution function.</p>
With this, the correlation measure of <span class="citation">(Spearman <a href="#ref-spearman1904proof">1904</a>)</span> is simply the product-moment correlation computed on the ranks:
<span class="math display">\[\begin{equation*}
r_S = \frac{Cov(R(X),R(Y))}{\sqrt{Cov(R(X),R(X))Cov(R(Y),R(Y))}}
= \frac{Cov(R(X),R(Y))}{(n^2-1)/12} .
\end{equation*}\]</span>
<p>You can obtain the Spearman correlation statistic <span class="math inline">\(r_S\)</span> using the <code>cor()</code> function in <code>R</code> and selecting the <code>spearman</code> method. From below, the Spearman correlation between the <em>Coverage</em> rating variable in millions of dollars and <em>Claim</em> amount variable in dollars is <span class="math inline">\(0.41\)</span>.</p>
<h5 style="text-align: center;">
<a id="display.spearman.1" href="javascript:togglecode
('display.spearman.2','display.spearman.1');"><i><strong>R Code for Spearman Correlation Statistic</strong></i></a>
</h5>
<div id="display.spearman.2" style="display: none">
<pre><code>### Spearman correlation between Claim and Coverage ###
rs&lt;-cor(Claim,Coverage, method = c(&quot;spearman&quot;))
round(rs,2)

Output:
[1] 0.41

### Spearman correlation between Claim and log(Coverage) ###
rs&lt;-cor(Claim,log(Coverage), method = c(&quot;spearman&quot;))
round(rs,2)

Output:
[1] 0.41</code></pre>
</div>
<p>To show that the Spearman correlation statistic is invariate under strictly increasing transformations , from the <code>R</code> Code for Spearman correlation statistic above, <span class="math inline">\(r_S=0.41\)</span> between the <em>Coverage</em> rating variable in logarithmic millions of dollars and <em>Claim</em> amount variable in dollars.</p>
</div>
<div id="kendalls-tau" class="section level4">
<h4><span class="header-section-number">14.2.2.2</span> Kendall’s Tau</h4>
<p>An alternative measure that uses ranks is based on the concept of <em>concordance</em>. An observation pair <span class="math inline">\((X,Y)\)</span> is said to be concordant (discordant) if the observation with a larger value of <span class="math inline">\(X\)</span> has also the larger (smaller) value of <span class="math inline">\(Y\)</span>. Then <span class="math inline">\(\Pr(concordance) = \Pr[ (X_1-X_2)(Y_1-Y_2) &gt;0 ]\)</span> , <span class="math inline">\(\Pr(discordance) = \Pr[ (X_1-X_2)(Y_1-Y_2) &lt;0 ]\)</span> and</p>
<span class="math display">\[\begin{eqnarray*}
\tau(X,Y)= \Pr(concordance) - \Pr(discordance) = 2\Pr(concordance) - 1 + \Pr(tie).
\end{eqnarray*}\]</span>
<p>To estimate this, the pairs <span class="math inline">\((X_i,Y_i)\)</span> and <span class="math inline">\((X_j,Y_j)\)</span> are said to be concordant if the product <span class="math inline">\(sgn(X_j-X_i)sgn(Y_j-Y_i)\)</span> equals 1 and discordant if the product equals -1. Here, <span class="math inline">\(sgn(x)=1,0,-1\)</span> as <span class="math inline">\(x&gt;0\)</span>, <span class="math inline">\(x=0\)</span>, <span class="math inline">\(x&lt;0\)</span>, respectively. With this, we can express the association measure of <span class="citation">(Kendall <a href="#ref-kendall1938new">1938</a>)</span>, known as <em>Kendall’s tau</em>, as</p>
<span class="math display">\[\begin{equation*}
\begin{array}{rl}
\tau &amp;= \frac{2}{n(n-1)} \sum_{i&lt;j}sgn(X_j-X_i)sgn(Y_j-Y_i)\\
&amp;= \frac{2}{n(n-1)} \sum_{i&lt;j}sgn(R(X_j)-R(X_i))sgn(R(Y_j)-R(Y_i)) 
\end{array}.
\end{equation*}\]</span>
<p>Interestingly, <span class="citation">(Hougaard <a href="#ref-hougaard2000analysis">2000</a>)</span>, page 137, attributes the original discovery of this statistic to <span class="citation">(Fechner <a href="#ref-fechnerkollektivmasslehre">1897</a>)</span>, noting that Kendall’s discovery was independent and more complete than the original work.</p>
<p>You can obtain the Kendall’s tau, using the <code>cor()</code> function in <code>R</code> and selecting the <code>kendall</code> method. From below, <span class="math inline">\(\tau=0.32\)</span> between the <em>Coverage</em> rating variable in millions of dollars and <em>Claim</em> amount variable in dollars.</p>
<h5 style="text-align: center;">
<a id="display.kendall.1" href="javascript:togglecode
('display.kendall.2','display.kendall.1');"><i><strong>R Code for Kendall’s Tau</strong></i></a>
</h5>
<div id="display.kendall.2" style="display: none">
<pre><code>### Kendall&#39;s tau correlation between Claim and Coverage ###
tau&lt;-cor(Claim,Coverage, method = c(&quot;kendall&quot;))
round(tau,2)

Output:
[1]  0.32

### Kendall&#39;s tau correlation between Claim and log(Coverage) ###
tau&lt;-cor(Claim,log(Coverage), method = c(&quot;kendall&quot;))
round(tau,2)

Output:
[1] 0.32</code></pre>
</div>
<p>Also,to show that the Kendall’s tau is invariate under strictly increasing transformations , <span class="math inline">\(\tau=0.32\)</span> between the <em>Coverage</em> rating variable in logarithmic millions of dollars and <em>Claim</em> amount variable in dollars.</p>
</div>
</div>
<div id="nominal-variables" class="section level3">
<h3><span class="header-section-number">14.2.3</span> Nominal Variables</h3>
<div id="bernoulli-variables" class="section level4">
<h4><span class="header-section-number">14.2.3.1</span> Bernoulli Variables</h4>
To see why dependence measures for continuous variables may not be the best for discrete variables, let us focus on the case of Bernoulli variables that take on simple binary outcomes, 0 and 1. For notation, let <span class="math inline">\(\pi_{jk} = \Pr(X=j, Y=k)\)</span> for <span class="math inline">\(j,k=0,1\)</span> and let <span class="math inline">\(\pi_X=\Pr(X=1)\)</span> and similarly for <span class="math inline">\(\pi_Y\)</span>. Then, the population version of the product-moment (Pearson) correlation can be easily seen to be
<span class="math display">\[\begin{eqnarray*}
\rho = \frac{\pi_{11} - \pi_X \pi_Y}{\sqrt{\pi_X(1-\pi_X)\pi_Y(1-\pi_Y)}} .
\end{eqnarray*}\]</span>
Unlike the case for continuous data, it is not possible for this measure to achieve the limiting boundaries of the interval <span class="math inline">\([-1,1]\)</span>. To see this, students of probability may recall the Fr<span class="math inline">\(\acute{e}\)</span>chet-H<span class="math inline">\(\ddot{o}\)</span>effding bounds for a joint distribution that turn out to be <span class="math inline">\(\max\{0, \pi_X+\pi_Y-1\} \le \pi_{11} \le \min\{\pi_X,\pi_Y\}\)</span> for this joint probability. This limit on the joint probability imposes an additional restriction on the Pearson correlation. As an illustration, assume equal probabilities <span class="math inline">\(\pi_X =\pi_Y = \pi &gt; 1/2\)</span>. Then, the lower bound is
<span class="math display">\[\begin{eqnarray*}
\frac{2\pi - 1 - \pi^2}{\pi(1-\pi)} = -\frac{1-\pi}{\pi} .
\end{eqnarray*}\]</span>
<p>For example, if <span class="math inline">\(\pi=0.8\)</span>, then the smallest that the Pearson correlation could be is -0.25. More generally, there are bounds on <span class="math inline">\(\rho\)</span> that depend on <span class="math inline">\(\pi_X\)</span> and <span class="math inline">\(\pi_Y\)</span> that make it difficult to interpret this measure.</p>
<p>As noted by <span class="citation">(Bishop, Fienberg, and Holland <a href="#ref-bishop1975discrete">1975</a>)</span> (page 382), squaring this correlation coefficient yields the Pearson chi-square statistic. Despite the boundary problems described above, this feature makes the Pearson correlation coefficient a good choice for describing dependence with binary data. The other is the odds ratio, described as follows.</p>
As an alternative measure for Bernoulli variables, the <em>odds ratio</em> is given by
<span class="math display">\[\begin{eqnarray*}
OR(\pi_{11}) = \frac{\pi_{11} \pi_{00}}{\pi_{01} \pi_{10}} = \frac{\pi_{11} \left( 1+\pi_{11}-\pi_1 -\pi_2\right)}{(\pi_1-\pi_{11})(\pi_2- \pi_{11})} .
\end{eqnarray*}\]</span>
<p>Pleasant calculations show that <span class="math inline">\(OR(z)\)</span> is <span class="math inline">\(0\)</span> at the lower Fr<span class="math inline">\(\acute{e}\)</span>chet-H<span class="math inline">\(\ddot{o}\)</span>effding bound <span class="math inline">\(z= \max\{0, \pi_1+\pi_2-1\}\)</span> and is <span class="math inline">\(\infty\)</span> at the upper bound <span class="math inline">\(z=\min\{\pi_1,\pi_2\}\)</span>. Thus, the bounds on this measure do not depend on the marginal probabilities <span class="math inline">\(\pi_X\)</span> and <span class="math inline">\(\pi_Y\)</span>, making it easier to interpret this measure.</p>
<p>As noted by <span class="citation">(Yule <a href="#ref-yule1900association">1900</a>)</span>, odds ratios are invariant to the labeling of 0 and 1. Further, they are invariant to the marginals in the sense that one can rescale <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> by positive constants and the odds ratio remains unchanged. Specifically, suppose that <span class="math inline">\(a_i\)</span>, <span class="math inline">\(b_j\)</span> are sets of positive constants and that</p>
<span class="math display">\[\begin{eqnarray*}
\pi_{ij}^{new} &amp;=&amp; a_i b_j \pi_{ij}
\end{eqnarray*}\]</span>
and <span class="math inline">\(\sum_{ij} \pi_{ij}^{new}=1.\)</span> Then,
<span class="math display">\[\begin{eqnarray*}
OR^{new} = \frac{(a_1 b_1 \pi_{11})( a_0 b_0 \pi_{00})}{(a_0 b_1 \pi_{01})( a_1 b_0\pi_{10})}
= \frac{\pi_{11} \pi_{00}}{\pi_{01} \pi_{10}} =OR^{old} .
\end{eqnarray*}\]</span>
For additional help with interpretation, Yule proposed two transforms for the odds ratio, the first in <span class="citation">(Yule <a href="#ref-yule1900association">1900</a>)</span>,
<span class="math display">\[\begin{eqnarray*}
\frac{OR-1}{OR+1},
\end{eqnarray*}\]</span>
and the second in <span class="citation">(Yule <a href="#ref-yule1912methods">1912</a>)</span>,
<span class="math display">\[\begin{eqnarray*}
\frac{\sqrt{OR}-1}{\sqrt{OR}+1}.
\end{eqnarray*}\]</span>
<p>Although these statistics provide the same information as is the original odds ration <span class="math inline">\(OR\)</span>, they have the advantage of taking values in the interval <span class="math inline">\([-1,1]\)</span>, making them easier to interpret.</p>
<p>In a later section, we will also see that the marginal distributions have no effect on the Fr<span class="math inline">\(\acute{e}\)</span>chet-H<span class="math inline">\(\ddot{o}\)</span>effding of the tetrachoric correlation, another measure of association, see also, <span class="citation">(Joe <a href="#ref-joe2014dependence">2014</a>)</span>, page 48.</p>
<p><a id=tab:14.2></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|rr|r} 
    \hline
                  &amp; \text{Fire5} &amp; &amp; \\
\text{NoClaimCredit} &amp; 0     &amp; 1     &amp; \text{Total} \\
  \hline
           0  &amp; 1611  &amp; 2175  &amp; 3786 \\
           1  &amp; 897   &amp; 956   &amp; 1853 \\
    \hline
    \text{Total}    &amp; 2508  &amp; 3131  &amp; 5639 \\
   \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.2">Table 14.2</a> : 2 <span class="math inline">\(\times\)</span> 2 table of counts for <em>Fire5</em> and <em>NoClaimCredit</em></p>
<p>From <a href="#tab:14.2">Table 14.2</a>, <span class="math inline">\(OR(\pi_{11})=\frac{1611(956)}{897(2175)}=0.79\)</span>. You can obtain the <span class="math inline">\(OR(\pi_{11})\)</span>, using the <code>oddsratio()</code> function from the <code>epitools</code> library in <code>R</code>. From the output below, <span class="math inline">\(OR(\pi_{11})=0.79\)</span> for the binary variables <em>NoClaimCredit</em> and <em>Fier5</em> from the LGPIF data.</p>
<h5 style="text-align: center;">
<a id="display.wald.1" href="javascript:togglecode
('display.wald.2','display.wald.1');"><i><strong>R Code for Odds Ratios</strong></i></a>
</h5>
<div id="display.wald.2" style="display: none">
<pre><code>library(epitools)
oddsratio(NoClaimCredit, Fire5,method = c(&quot;wald&quot;))$measure

Output:
[1]  0.79</code></pre>
</div>
</div>
<div id="categorical-variables" class="section level4">
<h4><span class="header-section-number">14.2.3.2</span> Categorical Variables</h4>
More generally, let <span class="math inline">\((X,Y)\)</span> be a bivariate pair having <span class="math inline">\(ncat_X\)</span> and <span class="math inline">\(ncat_Y\)</span> numbers of categories, respectively. For a two-way table of counts, let <span class="math inline">\(n_{jk}\)</span> be the number in the <span class="math inline">\(j\)</span>th row, <span class="math inline">\(k\)</span> column. Let <span class="math inline">\(n_{j\cdot}\)</span> be the row margin total and <span class="math inline">\(n_{\cdot k}\)</span> be the column margin total. Define Pearson chi-square statistic as
<span class="math display">\[\begin{eqnarray*}
chi^2 = \sum_{jk} \frac{(n_{jk}- n_{j\cdot}n_{\cdot k}/n)^2}{n_{j\cdot}n_{\cdot k}/n} .
\end{eqnarray*}\]</span>
The likelihood ratio test statistic is
<span class="math display">\[\begin{eqnarray*}
G^2 = 2 \sum_{jk} n_{jk} \ln\frac{n_{jk}}{n_{j\cdot}n_{\cdot k}/n} .
\end{eqnarray*}\]</span>
<p>Under the assumption of independence, both <span class="math inline">\(chi^2\)</span> and <span class="math inline">\(G^2\)</span> have an asymptotic chi-square distribution with <span class="math inline">\((ncat_X-1)(ncat_Y-1)\)</span> degrees of freedom.</p>
To help see what these statistics are estimating, let <span class="math inline">\(\pi_{jk} = \Pr(X=j, Y=k)\)</span> and let <span class="math inline">\(\pi_{X,j}=\Pr(X=j)\)</span> and similarly for <span class="math inline">\(\pi_{Y,k}\)</span>. Assuming that <span class="math inline">\(n_{jk}/n \approx \pi_{jk}\)</span> for large <span class="math inline">\(n\)</span> and similarly for the marginal probabilities, we have
<span class="math display">\[\begin{eqnarray*}
\frac{chi^2}{n} \approx \sum_{jk} \frac{(\pi_{jk}- \pi_{X,j}\pi_{Y,k})^2}{\pi_{X,j}\pi_{Y,k}}
\end{eqnarray*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
\frac{G^2}{n} \approx 2 \sum_{jk} \pi_{jk} \ln\frac{\pi_{jk}}{\pi_{X,j}\pi_{Y,k}} .
\end{eqnarray*}\]</span>
<p>Under the null hypothesis of independence, we have <span class="math inline">\(\pi_{jk} =\pi_{X,j}\pi_{Y,k}\)</span> and it is clear from these approximations that we anticipate that these statistics will be small under this hypothesis.</p>
<p>Classical approaches, as described in <span class="citation">(Bishop, Fienberg, and Holland <a href="#ref-bishop1975discrete">1975</a>)</span> (page 374), distinguish between tests of independence and measures of associations. The former are designed to detect whether a relationship exists whereas the latter are meant to assess the type and extent of a relationship. We acknowledge these differing purposes but also less concerned with this distinction for actuarial applications.</p>
<p><a id=tab:14.3></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
                  &amp; \text{NoClaimCredit} &amp;  \\
       \text{EntityType} &amp; 0     &amp; 1      \\
  \hline
            \text{City}    &amp; 644  &amp; 149 \\
          \text{County}    &amp; 310  &amp;  18 \\
            \text{Misc}    &amp; 336  &amp; 273 \\
          \text{School}    &amp; 1103 &amp; 494 \\
           \text{Town}     &amp; 492  &amp; 479 \\
         \text{Village}    &amp; 901  &amp; 440 \\
   \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.3">Table 14.3</a> : Two-way table of counts for <em>EntityType</em> and <em>NoClaimCredit</em></p>
<p>You can obtain the Pearson chi-square statistic, using the <code>chisq.test()</code> function from the <code>MASS</code> library in <code>R</code>. Here, we test whether the <em>EntityType</em> variable is independent of <em>NoClaimCredit</em> variable using <a href="#tab:14.3">Table 14.3</a>.</p>
<h5 style="text-align: center;">
<a id="display.chi.1" href="javascript:togglecode
('display.chi.2','display.chi.1');"><i><strong>R Code for Pearson Chi-square Statistic</strong></i></a>
</h5>
<div id="display.chi.2" style="display: none">
<pre><code>library(MASS)
table = table(EntityType, NoClaimCredit)
chisq.test(table)


Output:
------------------------------------
 Test statistic   df     P value    
---------------- ---- --------------
     344.2        5   3.15e-72 * * *
------------------------------------

Table: Pearson&#39;s Chi-squared test</code></pre>
</div>
<p>As the p-value is less than the .05 significance level, we reject the null hypothesis that the <em>EntityType</em> is independent of <em>NoClaimCredit</em>.</p>
<p>Furthermore, you can obtain the likelihood ratio test statistic , using the <code>likelihood.test()</code> function from the <code>Deducer</code> library in <code>R</code>. From below, we test whether the <em>EntityType</em> variable is independent of <em>NoClaimCredit</em> variable from the LGPIF data. Same conclusion is drawn as the Pearson chi-square test.</p>
<h5 style="text-align: center;">
<a id="display.lik.1" href="javascript:togglecode
('display.lik.2','display.lik.1');"><i><strong>R Code for Likelihood Ratio Test Statistic</strong></i></a>
</h5>
<div id="display.lik.2" style="display: none">
<pre><code>library(Deducer)
likelihood.test(EntityType, NoClaimCredit)

Output:
-----------------------------------------
 Test statistic   X-squared df   P value 
---------------- -------------- ---------
     378.7             5         0 * * * 
-----------------------------------------

Table: Log likelihood ratio (G-test) test of independence without correction
</code></pre>
</div>
</div>
<div id="ordinal-variables" class="section level4">
<h4><span class="header-section-number">14.2.3.3</span> Ordinal Variables</h4>
<p>As the analyst moves from the continuous to the nominal scale, there are two main sources of loss of information <span class="citation">(Bishop, Fienberg, and Holland <a href="#ref-bishop1975discrete">1975</a>)</span> (page 343). The first is breaking the precise continuous measurements into groups. The second is losing the ordering of the groups. So, it is sensible to describe what we can do with variables that in discrete groups but where the ordering is known.</p>
<p>As described in Section <a href="C-DependenceModel.html#S:QuaVar">14.1.1</a>, ordinal variables provide a clear ordering of levels of a variable but distances between levels are unknown. Associations have traditionally been quantified parametrically using normal-based correlations and nonparametrically using Spearman correlations with tied ranks.</p>
</div>
<div id="parametric-approach-using-normal-based-correlations" class="section level4">
<h4><span class="header-section-number">14.2.3.4</span> Parametric Approach Using Normal Based Correlations</h4>
<p>Refer to page 60, Section 2.12.7 of <span class="citation">(Joe <a href="#ref-joe2014dependence">2014</a>)</span>. Let <span class="math inline">\((y_1,y_2)\)</span> be a bivariate pair with discrete values on <span class="math inline">\(m_1, \ldots, m_2\)</span>. For a two-way table of ordinal counts, let <span class="math inline">\(n_{st}\)</span> be the number in the <span class="math inline">\(s\)</span>th row, <span class="math inline">\(t\)</span> column. Let <span class="math inline">\((n_{m_1*}, \ldots, n_{m_2*})\)</span> be the row margin total and <span class="math inline">\((n_{*m_1}, \ldots, n_{*m_2})\)</span> be the column margin total.</p>
<p>Let <span class="math inline">\(\hat{\xi}_{1s} = \Phi^{-1}((n_{m_1}+\cdots+n_{s*})/n)\)</span> for <span class="math inline">\(s=m_1, \ldots, m_2\)</span> be a cutpoint and similarly for <span class="math inline">\(\hat{\xi}_{2t}\)</span>. The <em>polychoric</em> correlation, based on a two-step estimation procedure, is</p>
<span class="math display">\[\begin{eqnarray*}
\begin{array}{cr}
  \hat{\rho_N} &amp;=\text{argmax}_{\rho}
  \sum_{s=m_1}^{m_2} \sum_{t=m_1}^{m_2} n_{st} \log\left\{
    \Phi_2(\hat{\xi}_{1s}, \hat{\xi}_{2t};\rho)
    -\Phi_2(\hat{\xi}_{1,s-1}, \hat{\xi}_{2t};\rho) \right.\\
   &amp; \left. -\Phi_2(\hat{\xi}_{1s}, \hat{\xi}_{2,t-1};\rho)
    +\Phi_2(\hat{\xi}_{1,s-1}, \hat{\xi}_{2,t-1};\rho)
    \right\}
\end{array}
\end{eqnarray*}\]</span>
<p>It is called a tetrachoric correlation for binary variables.</p>
<p><a id=tab:14.4></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
                  &amp; \text{NoClaimCredit} &amp;  \\
\text{AlarmCredit} &amp; 0     &amp; 1      \\
  \hline
          1  &amp; 1669  &amp;  942   \\
          2  &amp;    121 &amp;  118 \\
          3  &amp;  195  &amp;   132 \\
          4 &amp;  1801  &amp;   661 \\
   \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.4">Table 14.4</a> : Two-way table of counts for <em>AlarmCredit</em> and <em>NoClaimCredit</em></p>
<p>You can obtain the polychoric or tetrachoric correlation using the <code>polychoric()</code> or <code>tetrachoric()</code> function from the <code>psych</code> library in <code>R</code>. The polychoric correlation is illustrated using <a href="#tab:14.4">Table 14.4</a>. <span class="math inline">\(\hat{\rho_N}=-0.14\)</span>, which means that there is a negative relationship between <em>AlarmCredit</em> and <em>NoClaimCredit</em>.</p>
<h5 style="text-align: center;">
<a id="display.poly.1" href="javascript:togglecode
('display.poly.2','display.poly.1');"><i><strong>R Code for Polychoric Correlation</strong></i></a>
</h5>
<div id="display.poly.2" style="display: none">
<pre><code>library(psych)
AlarmCredit&lt;-as.numeric(ifelse(Insample$AC00==1,&quot;1&quot;,
                   ifelse(Insample$AC05==1,&quot;2&quot;,
                          ifelse(Insample$AC10==1,&quot;3&quot;,
                                 ifelse(Insample$AC15==1,&quot;4&quot;,0)))))
x &lt;- table(AlarmCredit,NoClaimCredit)
rhoN&lt;-polychoric(x,correct=FALSE)$rho
round(rhoN,2)

Output:
[1] -0.14</code></pre>
</div>
</div>
<div id="interval-variables" class="section level4">
<h4><span class="header-section-number">14.2.3.5</span> Interval Variables</h4>
<p>As described in Section <a href="C-DependenceModel.html#S:QuanVar">14.1.2</a>, interval variables provide a clear ordering of levels of a variable and the numerical distance between any two levels of the scale can be readily interpretable. For example, a claims count variable is an interval variable.</p>
<p>For measuring association, both the continuous variable and ordinal variable approaches make sense. The former takes advantage of knowledge of the ordering although assumes continuity. The latter does not rely on the continuity but also does not make use of the information given by the distance between scales.</p>
<p>For applications, one type is a count variable, a random variable on the discrete integers. Another is a mixture variable, on that has discrete and continuous components.</p>
</div>
<div id="discrete-and-continuous-variables" class="section level4">
<h4><span class="header-section-number">14.2.3.6</span> Discrete and Continuous Variables</h4>
<p>The polyserial correlation is defined similarly, when one variable (<span class="math inline">\(y_1\)</span>) is continuous and the other (<span class="math inline">\(y_2\)</span>) ordinal. Define <span class="math inline">\(z\)</span> to be the normal score of <span class="math inline">\(y_1\)</span>. The polyserial correlation is</p>
<span class="math display">\[\begin{eqnarray*}
\hat{\rho_N} = \text{argmax}_{\rho}
\sum_{i=1}^n \log\left\{ \phi(z_{i1})\left[
\Phi(\frac{\hat{\xi}_{2,y_{i2}} - \rho z_{i1}}
{(1-\rho^2)^{1/2}})
-\Phi(\frac{\hat{\xi}_{2,y_{i2-1}} - \rho z_{i1}}
{(1-\rho^2)^{1/2}})
\right]
\right\}
\end{eqnarray*}\]</span>
<p>The biserial correlation is defined similarly, when one variable is continuous and the other binary.</p>
<p><a id=tab:14.5></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|r|r} 
    \hline
\text{NoClaimCredit} &amp; \text{Mean}     &amp;\text{Total}       \\
 &amp; \text{Claim}     &amp;\text{Claim}       \\
  \hline
          0  &amp; 22,505  &amp;  85,200,483   \\
          1  &amp;    6,629 &amp;  12,282,618 \\
   \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.5">Table 14.5</a> : Summary of <em>Claim</em> by <em>NoClaimCredit</em></p>
<p>You can obtain the polyserial or biserial correlation using the <code>polyserial()</code> or <code>biserial()</code> function from the <code>psych</code> library in <code>R</code>. <a href="#tab:14.5">Table 14.5</a> gives the summary of <em>Claim</em> by <em>NoClaimCredit</em> and the biserial correlation is illustrated using <code>R</code> code below. The <span class="math inline">\(\hat{\rho_N}=-0.04\)</span> which means that there is a negative correlation between <em>Claim</em> and <em>NoClaimCredit</em>.</p>
<h5 style="text-align: center;">
<a id="display.bis.1" href="javascript:togglecode
('display.bis.2','display.bis.1');"><i><strong>R Code for Biserial Correlation </strong></i></a>
</h5>
<div id="display.bis.2" style="display: none">
<pre><code>library(psych)
rhoN&lt;-biserial(Claim,NoClaimCredit)
round(rhoN,2)

Output:
[1] -0.04</code></pre>
</div>
</div>
</div>
</div>
<div id="S:Copula" class="section level2">
<h2><span class="header-section-number">14.3</span> Introduction to Copulas</h2>
<p>Copula functions are widely used in statistics and actuarial science literature for dependency modeling.</p>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Describe a multivariate distribution function in terms of a copula function.</li>
</ul>
<hr />
<p>A <span class="math inline">\(copula\)</span> is a multivariate distribution function with uniform marginals. Specifically, let <span class="math inline">\(U_1, \ldots, U_p\)</span> be <span class="math inline">\(p\)</span> uniform random variables on <span class="math inline">\((0,1)\)</span>. Their distribution function <span class="math display">\[{C}(u_1, \ldots, u_p) = \Pr(U_1 \leq u_1, \ldots, U_p \leq u_p),\]</span></p>
<p>is a copula. We seek to use copulas in applications that are based on more than just uniformly distributed data. Thus, consider arbitrary marginal distribution functions <span class="math inline">\({F}_1(y_1)\)</span>,…,<span class="math inline">\({F}_p(y_p)\)</span>. Then, we can define a multivariate distribution function using the copula such that <span class="math display">\[{F}(y_1, \ldots, y_p)= {C}({F}_1(y_1), \ldots,
{F}_p(y_p)).\]</span></p>
<p>Here, <span class="math inline">\(F\)</span> is a multivariate distribution function in this equation. Sklar (1959) showed that <span class="math inline">\(any\)</span> multivariate distribution function <span class="math inline">\(F\)</span>, can be written in the form of this equation, that is, using a copula representation.</p>
<p>Sklar also showed that, if the marginal distributions are continuous, then there is a unique copula representation. In this chapter we focus on copula modeling with continuous variables. For discrete case, readers can see <span class="citation">(Joe <a href="#ref-joe2014dependence">2014</a>)</span> and <span class="citation">(Genest and Nešlohva <a href="#ref-genest2007methods">2007</a>)</span>.</p>
<p>For bivariate case, <span class="math inline">\(p=2\)</span> , the distribution function of two random variables can be written by the bivariate copula function: <span class="math display">\[{C}(u_1, \, u_2) = \Pr(U_1 \leq u_1, \, U_2 \leq
u_2),\]</span></p>
<p><span class="math display">\[{F}(y_1, \, y_2)= {C}({F}_1(y_1), \,
{F}_p(y_2)).\]</span></p>
<p>To give an example for bivariate copula, we can look at Frank’s (1979) copula. The equation is</p>
<p><span class="math display">\[{C}(u_1,u_2) = \frac{1}{\theta} \ln \left( 1+ \frac{ (\exp(\theta
u_1) -1)(\exp(\theta u_2) -1)} {\exp(\theta) -1} \right).\]</span></p>
<p>This is a bivariate distribution function with its domain on the unit square <span class="math inline">\([0,1]^2.\)</span> Here <span class="math inline">\(\theta\)</span> is dependence parameter and the range of dependence is controlled by the parameter <span class="math inline">\(\theta\)</span>. Positive association increases as <span class="math inline">\(\theta\)</span> increases and this positive association can be summarized with Spearman’s rho (<span class="math inline">\(\rho\)</span>) and Kendall’s tau (<span class="math inline">\(\tau\)</span>). Frank’s copula is one of the commonly used copula functions in the copula literature. We will see other copula functions in Section <a href="C-DependenceModel.html#S:CopTyp">14.5</a>.</p>
</div>
<div id="S:CopAppl" class="section level2">
<h2><span class="header-section-number">14.4</span> Application Using Copulas</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Discover dependence structure between random variables</li>
<li>Model the dependence with a copula function</li>
</ul>
<hr />
<p>This section analyzes the insurance <em>losses </em> and <em>expenses </em> data with the statistical programming <code>R</code>. This data set was introduced in <span class="citation">Frees and Valdez (<a href="#ref-frees1998understanding">1998</a>)</span> and is now readily available in the <code>copula</code> package. The model fitting process is started by marginal modeling of two variables (<span class="math inline">\(loss\)</span> and <span class="math inline">\(expense\)</span>). Then we model the joint distribution of these marginal outcomes.</p>
<div id="data-description" class="section level3">
<h3><span class="header-section-number">14.4.1</span> Data Description</h3>
<p>We start with getting a sample (<span class="math inline">\(n = 1500\)</span>) from the whole data. We consider first two variables of the data; <em>losses</em> and <em>expenses</em>.</p>
<ul>
<li><em>losses </em>: general liability claims from Insurance Services Office, Inc. (ISO)</li>
<li><em>expenses </em>: ALAE, specifically attributable to the settlement of individual claims (e.g. lawyer’s fees, claims investigation expenses)</li>
</ul>
<p>To visualize the relationship between <em>losses </em> and <em>expenses </em> (ALAE), scatterplots in figure <a href="C-DependenceModel.html#fig:Scatter">14.2</a> are created on the real dollar scale and on the log scale.</p>
<div class="figure"><span id="fig:Scatter"></span>
<img src="LossDataAnalytics_files/figure-html/Scatter-1.png" alt="Scatter plot of Loss and ALAE" width="672" />
<p class="caption">
Figure 14.2: Scatter plot of Loss and ALAE
</p>
</div>
<h5 style="text-align: center;">
<a id="display.ScaHis.1" href="javascript:togglecode
('display.ScaHis.2','display.ScaHis.1');"><i><strong>R Code for Scatterplots</strong></i></a>
</h5>
<div id="display.ScaHis.2" style="display: none">
<pre><code>library(copula)
data(loss) # loss data
Lossdata &lt;- loss 
attach(Lossdata) 
loss &lt;- Lossdata$loss
par(mfrow=c(1, 2))
plot(loss,alae, cex=.5) # real dollar scale
plot(log(loss),log(alae),cex=.5) # log scale
par(mfrow=c(1, 2))
</code></pre>
</div>
</div>
<div id="marginal-models" class="section level3">
<h3><span class="header-section-number">14.4.2</span> Marginal Models</h3>
<p>We first examine the marginal distributions of <em>losses </em> and <em>expenses </em> before going through the joint modeling. The histograms show that both <em>losses </em> and <em>expenses </em> are right-skewed and fat-tailed.</p>
<p>For marginal distributions of losses and expenses, we consider a Pareto-type distribution, namely a Pareto type II with distribution function</p>
<p><span class="math display">\[ F(y)=1- \left( 1 + \frac{y}{\theta} \right) ^{-\alpha},\]</span> where <span class="math inline">\(\theta\)</span> is the scale parameter and <span class="math inline">\(\alpha\)</span> is the shape parameter.</p>
<p>The marginal distributions of losses and expenses are fitted with maximum likelihood. Specifically, we use the <span class="math inline">\(vglm\)</span> function from the <code>R VGAM</code> package. Firstly, we fit the marginal distribution of <em>expenses </em>.</p>
<h5 style="text-align: center;">
<a id="display.Reg.1" href="javascript:togglecode
('display.Reg.2','display.Reg.1');"><i><strong>R Code for Pareto Fitting</strong></i></a>
</h5>
<div id="display.Reg.2" style="display: none">
<pre><code>library(VGAM)

fit = vglm(alae ~ 1, paretoII(location=0, lscale=&quot;loge&quot;, lshape=&quot;loge&quot;)) # fit the model by vlgm function
coef(fit, matrix=TRUE) # extract fitted model coefficients, matrix=TRUE gives logarithm of estimated parameters instead of default normal scale estimates
Coef(fit)

Output: 
               loge(scale) loge(shape)
 (Intercept)     9.624673   0.7988753
 
                  scale        shape 
 (Intercept)  15133.603598     2.223039 </code></pre>
</div>
<p>We repeat this procedure to fit the marginal distribution of the <em>loss</em> variable. Because the loss data also seems right-skewed and heavy-tail data, we also model the marginal distribution with Pareto II distribution.</p>
<h5 style="text-align: center;">
<a id="display.ParFit.1" href="javascript:togglecode
('display.ParFit.2','display.ParFit.1');"><i><strong>R Code for Pareto Fitting </strong></i></a>
</h5>
<div id="display.ParFit.2" style="display: none">
<pre><code>fitloss = vglm(loss ~ 1, paretoII, trace=TRUE)
Coef(fit)
summary(fit)


Output: 
       scale        shape 
15133.603598     2.223039 
</code></pre>
</div>
<p>To visualize the fitted distribution of <em>expenses </em> and <em>loss</em> variables, we use the estimated parameters and plot the corresponding distribution function and density function. For more details on marginal model selection, see Chapter <a href="C-ModelSelection.html#C:ModelSelection">4</a>.</p>
</div>
<div id="probability-integral-transformation" class="section level3">
<h3><span class="header-section-number">14.4.3</span> Probability Integral Transformation</h3>
<p>The <em>probability integral transformation </em> shows that any continuous variable can be mapped to a <span class="math inline">\(U(0,1)\)</span> random variable via its distribution function.</p>
<p>Given the fitted Pareto II distribution, the variable <em>expenses</em> is transformed to the variable <span class="math inline">\(u_1\)</span>, which follows a uniform distribution on <span class="math inline">\([0,1]\)</span>:</p>
<p><span class="math display">\[u_1 = 1 - \left( 1 + \frac{ALAE}{\hat{\theta}} \right)^{-\hat{\alpha}}.\]</span></p>
<p>After applying the probability integral transformation to <em>expenses </em> variable, we plot the histogram of <em>Transformed Alae </em> in Figure <a href="C-DependenceModel.html#fig:Hist">14.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:Hist"></span>
<img src="LossDataAnalytics_files/figure-html/Hist-1.png" alt="Histogram of Transformed Alae" width="384" />
<p class="caption">
Figure 14.3: Histogram of Transformed Alae
</p>
</div>
<p>After fitting process,the variable <em>loss</em> is also transformed to the variable <span class="math inline">\(u_2\)</span>, which follows a uniform distribution on <span class="math inline">\([0,1]\)</span>. We plot the histogram of <em>Transformed Loss </em>. As an alternative, the variable <em>loss</em> is transformed to <span class="math inline">\(normal\)</span> <span class="math inline">\(scores\)</span> with the quantile function of standard normal distribution. As we see in Figure <a href="C-DependenceModel.html#fig:Hist2">14.4</a>, normal scores of the variable <em>loss</em> are approximately marginally standard normal.</p>
<div class="figure"><span id="fig:Hist2"></span>
<img src="LossDataAnalytics_files/figure-html/Hist2-1.png" alt="Histogram of Transformed Loss. The left-hand panel shows the distribution of probability integral transformed losses. The right-hand panel shows the distribution for the corresponding normal scores." width="672" />
<p class="caption">
Figure 14.4: Histogram of Transformed Loss. The left-hand panel shows the distribution of probability integral transformed losses. The right-hand panel shows the distribution for the corresponding normal scores.
</p>
</div>
<h5 style="text-align: center;">
<a id="display.transalae.1" href="javascript:togglecode
('display.transalae.2','display.transalae.1');"><i><strong>R Code for Histograms of Transformed Variables </strong></i></a>
</h5>
<div id="display.transalae.2" style="display: none">
<pre><code>u1 &lt;- 1 - (1 + (alae/b))^(-s) # or u1 &lt;- pparetoII(alae, location=0, scale=b, shape=s)
hist(u1, main = &quot;&quot;, xlab = &quot;Histogram of Transformed alae&quot;)

scaleloss &lt;- Coef(fitloss)[1]
shapeloss &lt;- Coef(fitloss)[2]
u2 &lt;- 1 - (1 + (loss/scaleloss))^(-shapeloss)
par(mfrow = c(1, 2))
hist(u2, main = &quot;&quot;, xlab = &quot;Histogram of Transformed Loss&quot;)
hist(qnorm(u2), main = &quot;&quot;, xlab = &quot;Histogram of qnorm(Loss)&quot;)</code></pre>
</div>
</div>
<div id="joint-modeling-with-copula-function" class="section level3">
<h3><span class="header-section-number">14.4.4</span> Joint Modeling with Copula Function</h3>
<p>Before jointly modeling losses and expenses, we draw the scatterplot of transformed variables <span class="math inline">\((u_1, u_2)\)</span> and the scatterplot of normal scores in Figure <a href="C-DependenceModel.html#fig:Scatter2">14.5</a>.</p>
<p>Then we calculate the Spearman’s rho between these two uniform random variables.</p>
<div class="figure"><span id="fig:Scatter2"></span>
<img src="LossDataAnalytics_files/figure-html/Scatter2-1.png" alt="Left: Scatter plot for transformed variables.  Right:Scatter plot for normal scores" width="672" />
<p class="caption">
Figure 14.5: Left: Scatter plot for transformed variables. Right:Scatter plot for normal scores
</p>
</div>
<h5 style="text-align: center;">
<a id="display.Cor.1" href="javascript:togglecode
('display.Cor.2','display.Cor.1');"><i><strong>R Code for Scatter Plots and Correlation </strong></i></a>
</h5>
<div id="display.Cor.2" style="display: none">
<pre><code>par(mfrow = c(1, 2))
plot(u1, u2, cex = 0.5, xlim = c(-0.1,1.1), ylim = c(-0.1,1.1),
     xlab = &quot;Transformed Alae&quot;, ylab = &quot;Transformed Loss&quot;)
plot(qnorm(u1), qnorm(u2))
cor(u1, u2, method = &quot;spearman&quot;)

Output: 

[1] 0.451872</code></pre>
</div>
<p>Scatter plots and Spearman’s rho correlation value (0.451) shows us there is a positive dependency between these two uniform random variables. It is more clear to see the relationship with normal scores in the second graph. To learn more details about normal scores and their applications in copula modeling, see <span class="citation">(Joe <a href="#ref-joe2014dependence">2014</a>)</span>.</p>
<p><span class="math inline">\((U_1, U_2)\)</span>, (<span class="math inline">\(U_1 = F_1(ALAE)\)</span> and <span class="math inline">\(U_2=F_2(LOSS)\)</span>), is fit to Frank’s copula with maximum likelihood method.</p>
<h5 style="text-align: center;">
<a id="display.FrankCopula.1" href="javascript:togglecode
('display.FrankCopula.2','display.FrankCopula.1');"><i><strong>R Code for Modeling with Frank Copula</strong></i></a>
</h5>
<div id="display.FrankCopula.2" style="display: none">
<pre><code>uu = cbind(u1,u2) 
frank.cop &lt;- archmCopula(&quot;frank&quot;, param= c(5), dim = 2)
fit.ml &lt;- fitCopula(frank.cop, uu, method=&quot;ml&quot;, start=c(0.4))
summary(fit.ml)


Output: 

Call: fitCopula(copula, data = data, method = &quot;ml&quot;, start = ..2)
Fit based on &quot;maximum likelihood&quot; and 1500 2-dimensional observations.
Copula: frankCopula 
param 
3.114 
The maximized loglikelihood is 172.6 
Convergence problems: code is 52 see ?optim.
Call: fitCopula(copula, data = data, method = &quot;ml&quot;, start = ..2)
Fit based on &quot;maximum likelihood&quot; and 1500 2-dimensional observations.
Frank copula, dim. d = 2 
      Estimate Std. Error
param    3.114         NA
The maximized loglikelihood is 172.6 
Convergence problems: code is 52 see ?optim.
Number of loglikelihood evaluations:
function gradient 
      45       45 </code></pre>
</div>
<p>The fitted model implies that losses and expenses are positively dependent and their dependence is significant.</p>
<p>We use the fitted parameter to update the Frank’s copula. The Spearman’s correlation corresponding to the fitted copula parameter(3.114) is calculated with the <code>rho</code> function. In this case, the Spearman’s correlation coefficient is 0.462, which is very close to the sample Spearman’s correlation coefficient; 0.452.</p>
<h5 style="text-align: center;">
<a id="display.fittedCop.1" href="javascript:togglecode
('display.fittedCop.2','display.fittedCop.1');"><i><strong>R Code for Spearman’s Correlation Using Frank’s Copula</strong></i></a>
</h5>
<div id="display.fittedCop.2" style="display: none">
<pre><code>(param = fit.ml@estimate)
frank.cop &lt;- archmCopula(&quot;frank&quot;, param= param, dim = 2)
rho(frank.cop) 

Output : 
[1] 0.4622722
</code></pre>
</div>
<p>To visualize the fitted Frank’s copula, the distribution function and density function perspective plots are drawn in Figure <a href="C-DependenceModel.html#fig:FrankCop">14.6</a>.</p>
<div class="figure"><span id="fig:FrankCop"></span>
<img src="LossDataAnalytics_files/figure-html/FrankCop-1.png" alt="Left: Plot for distribution function for Franks Copula. Right:Plot for density function for Franks Copula " width="672" />
<p class="caption">
Figure 14.6: Left: Plot for distribution function for Franks Copula. Right:Plot for density function for Franks Copula
</p>
</div>
<h5 style="text-align: center;">
<a id="display.DistriPlot.1" href="javascript:togglecode
('display.DistriPlot.2','display.DistriPlot.1');"><i><strong>R Code for Frank’s Copula Plots</strong></i></a>
</h5>
<div id="display.DistriPlot.2" style="display: none">
<pre><code>par(mar=c(3.2,3,.2,.2),mfrow=c(1,2))
persp(frank.cop, pCopula, theta=50, zlab=&quot;C(u,v)&quot;,
        xlab =&quot;u&quot;, ylab=&quot;v&quot;, cex.lab=1.3)
persp(frank.cop, dCopula, theta=0, zlab=&quot;c(u,v)&quot;,
        xlab =&quot;u&quot;, ylab=&quot;v&quot;, cex.lab=1.3)</code></pre>
</div>
<p>Frank’s copula models positive dependence for this data set, with <span class="math inline">\(\theta=3.114\)</span>. For Frank’s copula, the dependence is related to values of <span class="math inline">\(\theta\)</span>. That is:</p>
<ul>
<li><span class="math inline">\(\theta=0\)</span>: independent copula</li>
<li><span class="math inline">\(\theta&gt;0\)</span>: positive dependence</li>
<li><span class="math inline">\(\theta&lt;0\)</span>: negative dependence</li>
</ul>
</div>
</div>
<div id="S:CopTyp" class="section level2">
<h2><span class="header-section-number">14.5</span> Types of Copulas</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Define the basic families of the copula functions</li>
<li>Calculate the association coefficients by the help of copula functions</li>
</ul>
<hr />
<p>There are several families of copulas have been described in the literature. Two main families of the copula families are the <strong>Archimedian</strong> and <strong>Elliptical</strong> copulas.</p>
<div id="elliptical-copulas" class="section level3">
<h3><span class="header-section-number">14.5.1</span> Elliptical Copulas</h3>
<p>Elliptical copulas are constructed from elliptical distributions. This copula decompose (multivariate) elliptical distributions into their univariate elliptical marginal distributions by Sklar’s theorem <span class="citation">(Hofert et al. <a href="#ref-hofertelements">2018</a>)</span>.</p>
<p>Properties of elliptical copulas are typically obtained from the properties ofcorresponding elliptical distributions <span class="citation">(Hofert et al. <a href="#ref-hofertelements">2018</a>)</span>.</p>
<p>For example, the normal distribution is a special type of elliptical distribution. To introduce the elliptical class of copulas, we start with the familiar multivariate normal distribution with probability density function <span class="math display">\[\phi_N (\mathbf{z})= \frac{1}{(2 \pi)^{p/2}\sqrt{\det \boldsymbol \Sigma}}
\exp\left( -\frac{1}{2} \mathbf{z}^{\prime} \boldsymbol
\Sigma^{-1}\mathbf{z}\right).\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol \Sigma\)</span> is a correlation matrix, with ones on the diagonal. Let <span class="math inline">\(\Phi\)</span> and <span class="math inline">\(\phi\)</span> denote the standard normal distribution and density functions. We define the Gaussian (normal) copula density function as</p>
<p><span class="math display">\[{c}_N(u_1,  \ldots, u_p) = \phi_N \left(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_p) \right) \prod_{j=1}^p \frac{1}{\phi(\Phi^{-1}(u_j))}.\]</span></p>
<p>As with other copulas, the domain is the unit cube <span class="math inline">\([0,1]^p\)</span>.</p>
<p>Specifically, a <span class="math inline">\(p\)</span>-dimensional vector <span class="math inline">\({z}\)</span> has an <span class="math inline">\({elliptical}\)</span> <span class="math inline">\({distribution}\)</span> if the density can be written as <span class="math display">\[h_E (\mathbf{z})= \frac{k_p}{\sqrt{\det \boldsymbol \Sigma}}
g_p \left( \frac{1}{2} (\mathbf{z}- \boldsymbol \mu)^{\prime}
\boldsymbol \Sigma^{-1}(\mathbf{z}- \boldsymbol \mu) \right).\]</span></p>
<p>We will use elliptical distributions to generate copulas. Because copulas are concerned primarily with relationships, we may restrict our considerations to the case where <span class="math inline">\(\mu = \mathbf{0}\)</span> and <span class="math inline">\(\boldsymbol \Sigma\)</span> is a correlation matrix. With these restrictions, the marginal distributions of the multivariate elliptical copula are identical; we use <span class="math inline">\(H\)</span> to refer to this marginal distribution function and <span class="math inline">\(h\)</span> is the corresponding density. This marginal density is <span class="math inline">\(h(z) = k_1 g_1(z^2/2).\)</span></p>
<p>We are now ready to define the <span class="math inline">\(elliptical\)</span> <span class="math inline">\(copula\)</span>, a function defined on the unit cube <span class="math inline">\([0,1]^p\)</span> as</p>
<p><span class="math display">\[{c}_E(u_1,  \ldots, u_p) = h_E \left(H^{-1}(u_1), \ldots,
H^{-1}(u_p) \right) \prod_{j=1}^p \frac{1}{h(H^{-1}(u_j))}.\]</span></p>
<p>In the elliptical copula family, the function <span class="math inline">\(g_p\)</span> is known as a <em>generator</em> in that it can be used to generate alternative distributions.</p>
<p><a id=tab:14.6></a></p>
<p><span class="math display">\[
\small\begin{array}{lc}
\hline &amp; Generator \\
 Distribution &amp;  \mathrm{g}_p(x)  \\
\hline
 \text{Normal distribution} &amp;  e^{-x}\\
 \text{t-distribution with r degrees of freedom} &amp;   (1+2x/r)^{-(p+r)/2}\\
 \text{Cauchy} &amp;  (1+2x)^{-(p+1)/2}\\
\text{Logistic} &amp;  e^{-x}/(1+e^{-x})^2\\
 \text{Exponential power} &amp;   \exp(-rx^s)\\
\hline
\end{array}
\]</span></p>
<p><a href="#tab:14.6">Table 14.6</a> : Distribution and Generator Functions (<span class="math inline">\(\mathrm{g}_p(x)\)</span>) for Selected Elliptical Copulas</p>
<p>Most empirical work focuses on the normal copula and <span class="math inline">\(t\)</span>-copula. That is, <span class="math inline">\(t\)</span>-copulas are useful for modeling the dependency in the tails of bivariate distributions, especially in financial risk analysis applications.</p>
<p>The <span class="math inline">\(t\)</span>-copulas with same association parameter in varying the degrees of freedom parameter show us different tail dependency structures. For more information on about <span class="math inline">\(t\)</span>-copulas readers can see <span class="citation">(Joe <a href="#ref-joe2014dependence">2014</a>)</span>, <span class="citation">(Hofert et al. <a href="#ref-hofertelements">2018</a>)</span>.</p>
</div>
<div id="archimedian-copulas" class="section level3">
<h3><span class="header-section-number">14.5.2</span> Archimedian Copulas</h3>
<p>This class of copulas are constructed from a <span class="math inline">\(generator\)</span> function,which is <span class="math inline">\(\mathrm{g}(\cdot)\)</span> is a convex, decreasing function with domain [0,1] and range <span class="math inline">\([0, \infty)\)</span> such that <span class="math inline">\(\mathrm{g}(0)=0\)</span>. Use <span class="math inline">\(\mathrm{g}^{-1}\)</span> for the inverse function of <span class="math inline">\(\mathrm{g}\)</span>. Then the function</p>
<p><span class="math display">\[\mathrm{C}_{\mathrm{g}}(u_1, \ldots, u_p) = \mathrm{g}^{-1} \left(\mathrm{g}(u_1)+ \cdots + \mathrm{g}(u_p) \right)\]</span></p>
<p>is said to be an <em>Archimedean</em> copula. The function <span class="math inline">\(\mathrm{g}\)</span> is known as the <em>generator</em> of the copula <span class="math inline">\(\mathrm{C}_{\mathrm{g}}\)</span>.</p>
<p>For bivariate case; <span class="math inline">\(p=2\)</span> , Archimedean copula function can be written by the function</p>
<p><span class="math display">\[\mathrm{C}_{\mathrm{g}}(u_1, \, u_2) = \mathrm{g}^{-1} \left(\mathrm{g}(u_1) + \mathrm{g}(u_2) \right).\]</span></p>
<p>Some important special cases of Archimedean copulas are Frank copula, Clayton/Cook-Johnson copula, Gumbel/Hougaard copula. This copula classes are derived from different generator functions.</p>
<p>We can remember that we mentioned about Frank’s copula with details in Section <a href="C-DependenceModel.html#S:Copula">14.3</a> and in Section <a href="C-DependenceModel.html#S:CopAppl">14.4</a>. Here we will continue to express the equations for Clayton copula and Gumbel/Hougaard copula.</p>
<div id="clayton-copula" class="section level4">
<h4><span class="header-section-number">14.5.2.1</span> Clayton Copula</h4>
<p>For <span class="math inline">\(p=2\)</span>, the Clayton copula is parameterized by <span class="math inline">\(\theta \in [-1,\infty)\)</span> is defined by <span class="math display">\[C_{\theta}^C(u)=\max\{u_1^{-\theta}+u_2^{-\theta}-1,0\}^{1/\theta}, \quad u\in[0,1]^2.\]</span></p>
<p>This is a bivariate distribution function of Clayton copula defined in unit square <span class="math inline">\([0,1]^2.\)</span> The range of dependence is controlled by the parameter <span class="math inline">\(\theta\)</span> as the same as Frank copula.</p>
</div>
<div id="gumbel-hougaard-copula" class="section level4">
<h4><span class="header-section-number">14.5.2.2</span> Gumbel-Hougaard copula</h4>
<p>The Gumbel-Hougaarg copula is parametrized by <span class="math inline">\(\theta \in [1,\infty)\)</span> and defined by <span class="math display">\[C_{\theta}^{GH}(u)=\exp\left(-\left(\sum_{i=1}^2 (-\log u_i)^{\theta}\right)^{1/\theta}\right), \quad u\in[0,1]^2.\]</span></p>
<p>Readers seeking deeper background on Archimedean copulas can see <span class="citation">Joe (<a href="#ref-joe2014dependence">2014</a>)</span>, <span class="citation">Frees and Valdez (<a href="#ref-frees1998understanding">1998</a>)</span>, and <span class="citation">Genest and Mackay (<a href="#ref-genest1986bivariate">1986</a>)</span>.</p>
</div>
</div>
<div id="properties-of-copulas" class="section level3">
<h3><span class="header-section-number">14.5.3</span> Properties of Copulas</h3>
<div id="bounds-on-association" class="section level4">
<h4><span class="header-section-number">14.5.3.1</span> Bounds on Association</h4>
<p>Like all multivariate distribution functions, copulas are bounded. The Fr<span class="math inline">\(&#39;{e}\)</span>chet-Hoeffding bounds are</p>
<p><span class="math display">\[\max( u_1 +\cdots+ u_p + p -1, 0) \leq  \mathrm{C}(u_1,  \ldots, u_p) \leq \min (u_1,  \ldots,u_p).\]</span></p>
<p>To see the right-hand side of the equation, note that <span class="math display">\[\mathrm{C}(u_1,\ldots, u_p) = \Pr(U_1 \leq u_1, \ldots, U_p \leq u_p) \leq  \Pr(U_j \leq u_j)\]</span>, for <span class="math inline">\(j=1,\ldots,p\)</span>. The bound is achieved when <span class="math inline">\(U_1 = \cdots = U_p\)</span>. To see the left-hand side when <span class="math inline">\(p=2\)</span>, consider <span class="math inline">\(U_2=1-U_1\)</span>. In this case, if <span class="math inline">\(1-u_2 &lt; u_1\)</span> then <span class="math inline">\(\Pr(U_1 \leq u_1, U_2 \leq u_2) = \Pr ( 1-u_2 \leq U_1 &lt; u_1) =u_1+u_2-1.\)</span> <span class="citation">(Nelson <a href="#ref-nelsen1997introduction">1997</a>)</span></p>
<p>The product copula is <span class="math inline">\(\mathrm{C}(u_1,u_2)=u_1u_2\)</span> is the result of assuming independence between random variables.</p>
<p>The lower bound is achieved when the two random variables are perfectly negatively related (<span class="math inline">\(U_2=1-U_1\)</span>) and the upper bound is achieved when they are perfectly positively related (<span class="math inline">\(U_2=U_1\)</span>).</p>
<p>We can see The Frechet-Hoeffding bounds for two random variables in the Figure <a href="C-DependenceModel.html#fig:Bounds">14.7</a>.</p>
<div class="figure"><span id="fig:Bounds"></span>
<img src="LossDataAnalytics_files/figure-html/Bounds-1.png" alt="Perfect Positive and Perfect negative dependence plots" width="672" />
<p class="caption">
Figure 14.7: Perfect Positive and Perfect negative dependence plots
</p>
</div>
<h5 style="text-align: center;">
<a id="display.plot.1" href="javascript:togglecode
('display.plot.2','display.plot.1');"><i><strong>R Code for Frechet-Hoeffding Bounds for Two Random Variables</strong></i></a>
</h5>
<div id="display.plot.2" style="display: none">
<pre><code>library(copula)
n&lt;-100
set.seed(1980)
U&lt;-runif(n)
par(mfrow=c(1, 2))
plot(cbind(U,1-U), xlab=quote(U[1]), ylab=quote(U[2]),main=&quot;Perfect Negative Dependency&quot;) # W for p=2
plot (cbind(U,U), xlab=quote(U[1]),ylab=quote(U[2]),main=&quot;Perfect Positive Dependency&quot;)  #M for p=2</code></pre>
</div>
</div>
<div id="measures-of-association" class="section level4">
<h4><span class="header-section-number">14.5.3.2</span> Measures of Association</h4>
<p>Schweizer and Wolff (1981) established that the copula accounts for all the dependence between two random variables, <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>, in the following sense. Consider m<span class="math inline">\(_1\)</span> and m<span class="math inline">\(_2\)</span>, strictly increasing functions. Thus, the manner in which <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> “move together” is captured by the copula, regardless of the scale in which each variable is measured.</p>
<p>Schweizer and Wolff also showed the two standard nonparametric measures of association could be expressed solely in terms of the copula function. Spearman’s correlation coefficient is given by</p>
<p><span class="math display">\[= 12 \int \int \left\{\mathrm{C}(u,v) - uv \right\} du dv.\]</span></p>
<p>Kendall’s tau is given by</p>
<p><span class="math display">\[= 4 \int \int \mathrm{C}(u,v)d\mathrm{C}(u,v) - 1 .\]</span></p>
<p>For these expressions, we assume that <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> have a jointly continuous distribution function. Further, the definition of Kendall’s tau uses an independent copy of (<span class="math inline">\(Y_1\)</span>, <span class="math inline">\(Y_2\)</span>), labeled (<span class="math inline">\(Y_1^{\ast}\)</span>, <span class="math inline">\(Y_2^{\ast}\)</span>), to define the measure of “concordance.” the widely used Pearson correlation depends on the margins as well as the copula. Because it is affected by non-linear changes of scale.</p>
</div>
<div id="tail-dependency" class="section level4">
<h4><span class="header-section-number">14.5.3.3</span> Tail Dependency</h4>
<p>There are some applications in which it is useful to distinguish by the part of the distribution in which the association is strongest. For example, in insurance it is helpful to understand association among the largest losses, that is, association in the right tails of the data.</p>
<p>To capture this type of dependency, we use the right-tail concentration function. The function is</p>
<p><span class="math display">\[R(z) = \frac{\Pr(U_1 &gt;z, U_2 &gt; z)}{1-z} =\Pr(U_1 &gt; z | U_2 &gt; z) =\frac{1 - 2z + \mathrm{C}(z,z)}{1-z} .\]</span></p>
<p>From this equation , <span class="math inline">\(R(z)\)</span> will equal to <span class="math inline">\(z\)</span> under independence. Joe (1997) uses the term “upper tail dependence parameter” for <span class="math inline">\(R = \lim_{z \rightarrow 1} R(z)\)</span>. Similarly, the left-tail concentration function is</p>
<p><span class="math display">\[L(z) = \frac{\Pr(U_1 \leq z, U_2 \leq z)}{z}=\Pr(U_1 \leq z | U_2 \leq z) =\frac{ \mathrm{C}(z,z)}{1-z}.\]</span></p>
<p>Tail dependency concentration function captures the probability of two random variables both catching up extreme values.</p>
<p>We calculate the left and right tail concentration functions for four different types of copulas; Normal, Frank,Gumbel and t copula. After getting tail concentration functions for each copula, we show concentration function’s values for these four copulas in <a href="#tab:14.7">Table 14.7</a>. As in <span class="citation">Venter (<a href="#ref-venter2002tails">2002</a>)</span>, we show <span class="math inline">\(L(z)\)</span> for <span class="math inline">\(z\leq 0.5\)</span> and <span class="math inline">\(R(z)\)</span> for <span class="math inline">\(z&gt;0.5\)</span> in the tail dependence plot in Figure <a href="C-DependenceModel.html#fig:DepTails">14.8</a>. We interpret the tail dependence plot, to mean that both the Frank and Normal copula exhibit no tail dependence whereas the <span class="math inline">\(t\)</span> and the Gumbel may do so. The <span class="math inline">\(t\)</span> copula is symmetric in its treatment of upper and lower tails.</p>
<p><a id=tab:14.7></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
\text{Copula} &amp; \text{Lower}    &amp; \text{Upper}     \\
\hline
\text{Frank}  &amp; 0  &amp; 0   \\
\text{Gumbel}  &amp; 0   &amp; 0.74    \\
\text{Normal}  &amp; 0   &amp; 0    \\
\text{t}  &amp; 0.10   &amp; 0.10    \\
   \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.7">Table 14.7</a> : Tail concentration function values for different copulas</p>
<h5 style="text-align: center;">
<a id="display.Tail.1" href="javascript:togglecode
('display.Tail.2','display.Tail.1');"><i><strong>R Code for Tail Copula Functions for Different Copulas</strong></i></a>
</h5>
<div id="display.Sim.2" style="display: none">
<pre><code>
library(copula)
U1 = seq(0,0.5, by=0.002)
U2 = seq(0.5,1, by=0.002)
U = rbind(U1, U2)
TailFunction &lt;- function(Tailcop) {
  lowertail &lt;- pCopula(cbind(U1,U1), Tailcop)/U1
  uppertail &lt;- (1-2*U2 +pCopula(cbind(U2,U2), Tailcop))/(1-U2)
  jointtail &lt;- rbind(lowertail,uppertail)
}
Tailcop1 &lt;- archmCopula(family = &quot;frank&quot;, param= c(0.05), dim = 2)
Tailcop2 &lt;- archmCopula(family = &quot;gumbel&quot;,param = 3)
Tailcop3 &lt;- ellipCopula(&quot;normal&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;)
Tailcop4 &lt;- ellipCopula(&quot;t&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;, df=5)
jointtail1 &lt;- TailFunction(Tailcop1)
jointtail2 &lt;- TailFunction(Tailcop2)
jointtail3 &lt;- TailFunction(Tailcop3)
jointtail4 &lt;- TailFunction(Tailcop4)
tailIndex(Tailcop1)
tailIndex(Tailcop2)
tailIndex(Tailcop3)
tailIndex(Tailcop4)</code></pre>
</div>
<div class="figure"><span id="fig:DepTails"></span>
<img src="LossDataAnalytics_files/figure-html/DepTails-1.png" alt="Tail dependence plots" width="672" />
<p class="caption">
Figure 14.8: Tail dependence plots
</p>
</div>
<h5 style="text-align: center;">
<a id="display.Tailplot.1" href="javascript:togglecode
('display.Tailplot.2','display.Tailplot.1');"><i><strong>R Code for Tail Dependence Plots for Different Copulas</strong></i></a>
</h5>
<div id="display.Sim.2" style="display: none">
<pre><code>plot(U,jointtail1, cex=.2, xlim=c(0,1),ylab=&quot;Tail Dependence&quot;, ylim=c(0,1))
lines(U,jointtail2, type=&quot;p&quot;,lty=1, cex=.2)
lines(U,jointtail3, type=&quot;p&quot;,lty=1, cex=.2)
lines(U,jointtail4, type=&quot;p&quot;,lty=1, cex=.2)
text(0.75, 0.1, &quot;Frank&quot;, cex=1.3)        #1
text(0.1, 0.8, &quot;Gumbel&quot;, cex=1.3)        #2
text(0.25, 0.1, &quot;normal&quot;, cex=1.3)       #3
arrows(.17, 0.1, .07, 0.12,code=2, angle=20, length=0.1)
text(0.9, 0.4, &quot;t with 5 df&quot;, cex=1.3)   #4
</code></pre>
</div>
</div>
</div>
</div>
<div id="S:CopImp" class="section level2">
<h2><span class="header-section-number">14.6</span> Why is Dependence Modeling Important?</h2>
<p>Dependence Modeling is important because it enables us to understand the dependence structure by defining the relationship between variables in a dataset. In insurance, ignoring dependence modeling may not impact pricing but could lead to misestimation of required capital to cover losses. For instance, from Section <a href="C-DependenceModel.html#S:CopAppl">14.4</a> , it is seen that there was a positive relationship between <em>Loss</em> and <em>Expense</em>. This means that, if there is a large loss then we expect expenses to be large as well and ignoring this relationship could lead to misestimation of reserves.</p>
<p>To illustrate the importance of dependence modeling, we refer you back to Portfolio Management example in Chapter 6 that assumed that the property and liability risks are independent. Here, we incorporate dependence by allowing the 4 lines of business to depend on one another through a Gaussian copula. In <a href="#tab:14.8">Table 14.8</a>, we show that dependence affects the portfolio quantiles (<span class="math inline">\(VaR_q\)</span>), although not the expect value. For instance , the <span class="math inline">\(VaR_{0.99}\)</span> for total risk which is the amount of capital required to ensure, with a <span class="math inline">\(99\%\)</span> degree of certainty that the firm does not become technically insolvent is higher when we incorporate dependence. This leads to less capital being allocated when dependence is ignored and can cause unexpected solvency problems.</p>
<p><a id=tab:14.8></a></p>
<p><span class="math display">\[
{\small \begin{matrix}
\begin{array}{l|rrrr} 
    \hline
 \text{Independent} &amp;\text{Expected}   &amp; VaR_{0.9}  &amp; VaR_{0.95}  &amp; VaR_{0.99}  \\
                   &amp;\text{Value}      &amp;            &amp;             &amp;             \\
     \hline              
\text{Retained}    &amp; 269              &amp;  300       &amp; 300         &amp; 300         \\
\text{Insurer}     &amp; 2,274            &amp;  4,400     &amp; 6,173       &amp; 11,859      \\
\text{Total}       &amp; 2,543            &amp;  4,675     &amp; 6,464       &amp; 12,159      \\
   \hline
\text{Gaussian Copula}&amp;\text{Expected}&amp; VaR_{0.9}  &amp; VaR_{0.95}  &amp; VaR_{0.99}  \\
                      &amp;\text{Value}    &amp;           &amp;             &amp;              \\
     \hline                      
\text{Retained}       &amp; 269            &amp;  300      &amp; 300         &amp;  300         \\
\text{Insurer}        &amp; 2,340          &amp;  4,988    &amp; 7,339       &amp; 14,905       \\
\text{Total}          &amp; 2,609          &amp;  5,288    &amp; 7,639       &amp; 15,205       \\
   \hline
\end{array}
\end{matrix}}
\]</span></p>
<p><a href="#tab:14.8">Table 14.8</a> : Results for portfolio expected value and quantiles (<span class="math inline">\(VaR_q\)</span>)</p>
<h5 style="text-align: center;">
<a id="display.Sim.1" href="javascript:togglecode
('display.Sim.2','display.Sim.1');"><i><strong>R Code for Simulation Using Gaussian Copula</strong></i></a>
</h5>
<div id="display.Sim.2" style="display: none">
<pre><code>
# For the gamma distributions, use
alpha1 &lt;- 2;      theta1 &lt;- 100
alpha2 &lt;- 2;      theta2 &lt;- 200
# For the Pareto distributions, use
alpha3 &lt;- 2;      theta3 &lt;- 1000
alpha4 &lt;- 3;      theta4 &lt;- 2000
# Deductibles
d1     &lt;- 100
d2     &lt;- 200


# Simulate the risks
nSim &lt;- 10000  #number of simulations
set.seed(2017) #set seed to reproduce work 
X1 &lt;- rgamma(nSim,alpha1,scale = theta1)  
X2 &lt;- rgamma(nSim,alpha2,scale = theta2)  
# For the Pareto Distribution, use
library(VGAM)
X3 &lt;- rparetoII(nSim,scale=theta3,shape=alpha3)
X4 &lt;- rparetoII(nSim,scale=theta4,shape=alpha4)
# Portfolio Risks
S         &lt;- X1 + X2 + X3 + X4
Sretained &lt;- pmin(X1,d1) + pmin(X2,d2)
Sinsurer  &lt;- S - Sretained

# Expected Claim Amounts
ExpVec &lt;- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S))))
colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;)
round(ExpVec,digits=2)

# Quantiles
quantMat &lt;- rbind(
  quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Sinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(S       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;)
round(quantMat,digits=2)

plot(density(S), main=&quot;Density of Total Portfolio Risk S&quot;, xlab=&quot;S&quot;)

### Normal Copula ##
library(VGAM)
library(copula)
library(GB2)
library(statmod)
library(numDeriv)
set.seed(2017)
parm&lt;-c(0.5,0.5,0.5,0.5,0.5,0.5)
nc &lt;- normalCopula(parm, dim = 4, dispstr = &quot;un&quot;)
mcc &lt;- mvdc(nc, margins = c(&quot;gamma&quot;, &quot;gamma&quot;,&quot;paretoII&quot;,&quot;paretoII&quot;), 
            paramMargins = list(list(scale = theta1, shape=alpha1),
                                list(scale = theta2, shape=alpha2),
                                list(scale = theta3, shape=alpha3),
                                list(scale = theta4, shape=alpha4)))
X &lt;- rMvdc(nSim, mvdc = mcc)

X1&lt;-X[,1]
X2&lt;-X[,2]
X3&lt;-X[,3]
X4&lt;-X[,4]

# Portfolio Risks
S         &lt;- X1 + X2 + X3 + X4
Sretained &lt;- pmin(X1,d1) + pmin(X2,d2)
Sinsurer  &lt;- S - Sretained

# Expected Claim Amounts
ExpVec &lt;- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S))))
colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;)
round(ExpVec,digits=2)

# Quantiles
quantMat &lt;- rbind(
  quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Sinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(S       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;)
round(quantMat,digits=2)

plot(density(S), main=&quot;Density of Total Portfolio Risk S&quot;, xlab=&quot;S&quot;)
</code></pre>
</div>
</div>
<div id="Dep:further-reading-and-resources" class="section level2">
<h2><span class="header-section-number">14.7</span> Further Resources and Contributors</h2>
<div id="contributors-6" class="section level4 unnumbered">
<h4>Contributors</h4>
<ul>
<li><strong>Edward W. (Jed) Frees</strong> and <strong>Nii-Armah Okine</strong>, University of Wisconsin-Madison, and <strong>Emine Selin Sarıdaş</strong>, Mimar Sinan University, are the principal authors of the initial version of this chapter. Email: <a href="mailto:jfrees@bus.wisc.edu">jfrees@bus.wisc.edu</a> for chapter comments and suggested improvements.</li>
</ul>
</div>
</div>
<div id="technical-supplement-a.-other-classic-measures-of-scalar-associations" class="section level2 unnumbered">
<h2>Technical Supplement A. Other Classic Measures of Scalar Associations</h2>
<div id="a.1.-blomqvists-beta" class="section level3 unnumbered">
<h3>A.1. Blomqvist’s Beta</h3>
<p><span class="citation">Blomqvist (<a href="#ref-blomqvist1950measure">1950</a>)</span> developed a measure of dependence now known as <em>Blomqvist’s beta</em>, also called the <em>median concordance coefficient</em> and the <em>medial correlation coefficient</em>. Using distribution functions, this parameter can be expressed as</p>
<span class="math display">\[\begin{equation*}
\beta = 4F\left(F^{-1}_X(1/2),F^{-1}_Y(1/2) \right) - 1.
\end{equation*}\]</span>
<p>That is, first evaluate each marginal at its median (<span class="math inline">\(F^{-1}_X(1/2)\)</span> and <span class="math inline">\(F^{-1}_Y(1/2)\)</span>, respectively). Then, evaluate the bivariate distribution function at the two medians. After rescaling (multiplying by 4 and subtracting 1), the coefficient turns out to have a range of <span class="math inline">\([-1,1]\)</span>, where 0 occurs under independence.</p>
Like Spearman’s rho and Kendall’s tau, an estimator based on ranks is easy to provide. First write <span class="math inline">\(\beta = 4C(1/2,1/2)-1 = 2\Pr((U_1-1/2)(U_2-1/2))-1\)</span> where <span class="math inline">\(U_1, U_2\)</span> are uniform random variables. Then, define
<span class="math display">\[\begin{equation*}
\hat{\beta} = \frac{2}{n} \sum_{i=1}^n I\left( (R(X_{i})-\frac{n+1}{2})(R(Y_{i})-\frac{n+1}{2}) \ge 0 \right)-1 .
\end{equation*}\]</span>
<p>See, for example, <span class="citation">(Joe <a href="#ref-joe2014dependence">2014</a>)</span>, page 57 or <span class="citation">(Hougaard <a href="#ref-hougaard2000analysis">2000</a>)</span>, page 135, for more details.</p>
<p>Because Blomqvist’s parameter is based on the center of the distribution, it is particularly useful when data are censored; in this case, information in extreme parts of the distribution are not always reliable. How does this affect a choice of association measures? First, recall that association measures are based on a bivariate distribution function. So, if one has knowledge of a good approximation of the distribution function, then calculation of an association measure is straightforward in principle. Second, for censored data, bivariate extensions of the univariate Kaplan-Meier distribution function estimator are available. For example, the version introduced in <span class="citation">(Dabrowska <a href="#ref-dabrowska1988kaplan">1988</a>)</span> is appealing. However, because of instances when large masses of data appear at the upper range of the data, this and other estimators of the bivariate distribution function are unreliable. This means that, summary measures of the estimated distribution function based on Spearman’s rho or Kendall’s tau can be unreliable. For this situation, Blomqvist’s beta appears to be a better choice as it focuses on the center of the distribution. <span class="citation">(Hougaard <a href="#ref-hougaard2000analysis">2000</a>)</span>, Chapter 14, provides additional discussion.</p>
<p>You can obtain the Blomqvist’s beta, using the <code>betan()</code> function from the <code>copula</code> library in <code>R</code>. From below, <span class="math inline">\(\beta=0.3\)</span> between the <em>Coverage</em> rating variable in millions of dollars and <em>Claim</em> amount variable in dollars.</p>
<h5 style="text-align: center;">
<a id="display.beta.1" href="javascript:togglecode
('display.beta.2','display.beta.1');"><i><strong>R Code for Blomqvist’s Beta</strong></i></a>
</h5>
<div id="display.beta.2" style="display: none">
<pre><code>### Blomqvist&#39;s beta correlation between Claim and Coverage ###
library(copula)
n&lt;-length(Claim)
U&lt;-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(Coverage)))
beta&lt;-betan(U, scaling=FALSE)
round(beta,2)

Output:
[1]  0.3

### Blomqvist&#39;s beta correlation between Claim and log(Coverage) ###
n&lt;-length(Claim)
Fx&lt;-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(log(Coverage))))
beta&lt;-betan(Fx, scaling=FALSE)
round(beta,2)

Output:
[1]  0.3</code></pre>
</div>
<p>In addition,to show that the Blomqvist’s beta is invariate under strictly increasing transformations , <span class="math inline">\(\beta=0.3\)</span> between the <em>Coverage</em> rating variable in logarithmic millions of dollars and <em>Claim</em> amount variable in dollars.</p>
</div>
<div id="a.2.-nonparametric-approach-using-spearman-correlation-with-tied-ranks" class="section level3 unnumbered">
<h3>A.2. Nonparametric Approach Using Spearman Correlation with Tied Ranks</h3>
<p>For the first variable, the average rank of observations in the <span class="math inline">\(s\)</span>th row is</p>
<span class="math display">\[\begin{equation*}
r_{1s} = n_{m_1*}+ \cdots+ n_{s-1,*}+ \frac{1}{2} \left(1+ n_{s*}\right)
\end{equation*}\]</span>
and similarly <span class="math inline">\(r_{2t} = \frac{1}{2} \left[(n_{*m_1}+ \cdots+ n_{*,s-1}+1)+ (n_{*m_1}+ \ldots+ n_{*s})\right]\)</span>. With this, we have Spearman’s rho with tied rank is
<span class="math display">\[\begin{equation*}
\hat{\rho}_S = \frac{\sum_{s=m_1}^{m_2} \sum_{t=m_1}^{m_2} n_{st}(r_{1s} - \bar{r})(r_{2t} - \bar{r})}
{\left[\sum_{s=m_1}^{m_2}n_{s*}(r_{1s} - \bar{r})^2 \sum_{t=m_1}^{m_2} n_{*t}(r_{2t} - \bar{r})^2
\right]^2}
\end{equation*}\]</span>
<p>where the average rank is <span class="math inline">\(\bar{r} = (n+1)/2\)</span>.</p>
<h5 style="text-align: center;">
<a id="display.Thry.1" href="javascript:togglecode
('display.Thry.2','display.Thry.1');"><i><strong>Click to Show Proof for Special Case: Binary Data.</strong></i></a>
</h5>
<div id="display.Thry.2" style="display: none">
Special Case: Binary Data. Here, <span class="math inline">\(m_1=0\)</span> and <span class="math inline">\(m_2=1\)</span>. For the first variable ranks, we have <span class="math inline">\(r_{10} = (1+n_{0+})/2\)</span> and <span class="math inline">\(r_{11} = (n_{0+}+1+n)/2\)</span>. Thus, <span class="math inline">\(r_{10} -\bar{r}= (n_{0+}-n)/2\)</span> and <span class="math inline">\(r_{11}-\bar{r} = n_{0+}/2\)</span>. This means that we have <span class="math inline">\(\sum_{s=0}^{1}n_{s+}(r_{1s} - \bar{r})^2 = n (n-n_{0+})n_{0+}/4\)</span> and similarly for the second variable. For the numerator, we have
<span class="math display">\[\begin{eqnarray*}
\sum_{s=0}^{1}  \sum_{t=0}^{1} &amp;&amp; n_{st}(r_{1s} - \bar{r})(r_{2t} - \bar{r})\\
&amp;=&amp; n_{00} \frac{n_{0+}-n}{2} \frac{n_{+0}-n}{2}
+n_{01} \frac{n_{0+}-n}{2} \frac{n_{+0}}{2}
+n_{10} \frac{n_{0+}}{2} \frac{n_{+0}-n}{2}
+n_{11} \frac{n_{0+}}{2} \frac{n_{+0}}{2} \\
&amp;=&amp; \frac{1}{4}(n_{00} (n_{0+}-n) (n_{+0}-n)
+(n_{0+}-n_{00}) (n_{0+}-n)n_{+0} \\
&amp;&amp;  ~ ~ ~ +(n_{+0}-n_{00})  n_{0+}(n_{+0}-n)
+(n-n_{+0}-n_{0+}+n_{00}) n_{0+}n_{+0} ) \\
&amp;=&amp; \frac{1}{4}(n_{00} n^2
- n_{0+} (n_{0+}-n)n_{+0} \\
&amp;&amp; ~ ~ ~ +n_{+0}  n_{0+}(n_{+0}-n)
+(n-n_{+0}-n_{0+}) n_{0+}n_{+0} ) \\
&amp;=&amp; \frac{1}{4}(n_{00} n^2
- n_{0+}n_{+0} (n_{0+}-n +n_{+0}-n
+n-n_{+0}-n_{0+}) \\
&amp;=&amp; \frac{n}{4}(n n_{00} - n_{0+}n_{+0}) .
\end{eqnarray*}\]</span>
This yields
<span class="math display">\[\begin{eqnarray*}
\hat{\rho}_S &amp;=&amp; \frac{n(n n_{00} - n_{0+}n_{+0})}
{4\sqrt{(n (n-n_{0+})n_{0+}/4)(n (n-n_{+0})n_{+0}/4)}} \\
&amp;=&amp; \frac{n n_{00} - n_{0+}n_{+0}}
{\sqrt{ n_{0+} n_{+0}(n-n_{0+}) (n-n_{+0})}} \\
&amp;=&amp; \frac{n_{00} - n (1-\hat{\pi}_X)(1- \hat{\pi}_Y)}
{\sqrt{\hat{\pi}_X(1-\hat{\pi}_X)\hat{\pi}_Y(1-\hat{\pi}_Y) }}
\end{eqnarray*}\]</span>
<p>where <span class="math inline">\(\hat{\pi}_X = (n-n_{0+})/n\)</span> and similarly for <span class="math inline">\(\hat{\pi}_Y\)</span>. Note that this is same form as the Pearson measure. From this, we see that the joint count <span class="math inline">\(n_{00}\)</span> drives this association measure.</p>
</div>

<p>You can obtain the ties-corrected Spearman correlation statistic <span class="math inline">\(r_S\)</span> using the <code>cor()</code> function in <code>R</code> and selecting the <code>spearman</code> method. From below <span class="math inline">\(\hat{\rho}_S=-0.09\)</span></p>
<h5 style="text-align: center;">
<a id="display.spearT.1" href="javascript:togglecode
('display.spearT.2','display.spearT.1');"><i><strong>R Code for Ties-corrected Spearman Correlation</strong></i></a>
</h5>
<div id="display.spearT.2" style="display: none">
<pre><code>rs_ties&lt;-cor(AlarmCredit,NoClaimCredit, method = c(&quot;spearman&quot;))
round(rs_ties,2)

Output:
[1] -0.09</code></pre>
</div>

</div>
</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references">
<div id="ref-lee1988thirteen">
<p>Lee Rodgers, J, and W. A Nicewander. 1998. “Thirteen Ways to Look at the Correlation Coeffeicient.” <em>The American Statistician</em> 42 (01): 59–66.</p>
</div>
<div id="ref-hettmansperger1984statistical">
<p>Hettmansperger, T. P. 1984. <em>Statistical Inference Based on Ranks</em>. Wiley.</p>
</div>
<div id="ref-spearman1904proof">
<p>Spearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” <em>The American Journal of Psychology</em> 15 (01): 72–101.</p>
</div>
<div id="ref-kendall1938new">
<p>Kendall, Maurice G. 1938. “A New Measure of Rank Correlation.” <em>Biometrika</em>, 81–93.</p>
</div>
<div id="ref-hougaard2000analysis">
<p>Hougaard, P. 2000. <em>Analysis of Multivariate Survival Data</em>. Springer New York.</p>
</div>
<div id="ref-fechnerkollektivmasslehre">
<p>Fechner, G. T. 1897. “Kollektivmasslehre.” <em>Wilhelm Englemann, Leipzig</em>.</p>
</div>
<div id="ref-bishop1975discrete">
<p>Bishop, Yvonne M., Stephen E. Fienberg, and Paul W. Holland. 1975. <em>Discrete Multivariate Analysis: Theory and Practice</em>. Cambridge [etc.]: MIT.</p>
</div>
<div id="ref-yule1900association">
<p>Yule, G. Udny. 1900. “On the Association of Attributes in Statistics: With Illustrations from the Material of the Childhood Society.” <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em>, 257–319.</p>
</div>
<div id="ref-yule1912methods">
<p>Yule, G. 1912. “On the Methods of Measuring Association Between Two Attributes.” <em>Journal of the Royal Statistical Society</em>, 579–652.</p>
</div>
<div id="ref-joe2014dependence">
<p>Joe, Harry. 2014. <em>Dependence Modeling with Copulas</em>. CRC Press.</p>
</div>
<div id="ref-genest2007methods">
<p>Genest, Christian, and Johanna Nešlohva. 2007. “A Primer on Copulas for Count Data.” <em>Journal of the Royal Statistical Society</em>, 475–515.</p>
</div>
<div id="ref-frees1998understanding">
<p>Frees, Edward W., and Emiliano A. Valdez. 1998. “Understanding Relationships Using Copulas.” <em>North American Actuarial Journal</em> 2 (01): 1–25.</p>
</div>
<div id="ref-hofertelements">
<p>Hofert, Marius, Ivan Kojadinovic, Martin Mächler, and Jun Yan. 2018. <em>Elements of Copula Modeling with R</em>. Springer.</p>
</div>
<div id="ref-genest1986bivariate">
<p>Genest, Christian, and Josh Mackay. 1986. “The Joy of Copulas: Bivariate Distributions with Uniform Marginals.” <em>The American Statistician</em> 40: 280–83.</p>
</div>
<div id="ref-nelsen1997introduction">
<p>Nelson, Roger B. 1997. <em>An Introduction to Copulas</em>. Lecture Notes in Statistics 139.</p>
</div>
<div id="ref-venter2002tails">
<p>Venter, Gary G. 2002. “Tails of Copulas.” In <em>Proceedings of the Casualty Actuarial Society</em>, 89:68–113. 171.</p>
</div>
<div id="ref-blomqvist1950measure">
<p>Blomqvist, Nils. 1950. “On a Measure of Dependence Between Two Random Variables.” <em>The Annals of Mathematical Statistic</em>, 593–600.</p>
</div>
<div id="ref-dabrowska1988kaplan">
<p>Dabrowska, Dorota M. 1988. “Kaplan-Meier Estimate on the Plane.” <em>The Annals of Statistics</em>, 1475–89.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C-DataSystems.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C-AppA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["LossDataAnalytics.pdf", "LossDataAnalytics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
